[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability and Random Signals II",
    "section": "",
    "text": "Course Outline",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#general-information",
    "href": "index.html#general-information",
    "title": "Probability and Random Signals II",
    "section": "General Information (Fall 2025)",
    "text": "General Information (Fall 2025)\n\nInstructor\n\n\nAditya Mahajan\nOffice Hours: Tuesday 10:00am–11:00am\n\n\nTeaching Assistants\n\n\nZiqi Huang\n\n\nLectures\n\n\n8:35am–9:55am Monday, Wednesday (ENGTR 2100)\n\n\nTutorials\n\n\n12:35pm–1:25pm Friday, (ENGTR 2110)\n\n\nPrerequisites\n\n\nECSE 205 (Probability and Random Signals I)\nECSE 206 or ECSE 316 (Signals and Systems)\n\n\nCommunication\n\nUse the discussion board on myCourses for all questions related to the course. Only personal emails related to medical exceptions for missing a deliverable will be answered.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Probability and Random Signals II",
    "section": "Course Content",
    "text": "Course Content\n\n\n\n\n\n\n\nWeek\nMaterial Covered\n\n\n\n\n1\nProbability spaces, algebra of events, axioms of probability\n\n\n2\nRandom variables\n\n\n3\nRandom vectors\n\n\n4\nConditional probability and conditional expectation\n\n\n5\nMoment generating functions and sums of random variables\n\n\n6\nProbability inequalities\n\n\n7\nReview and Mid-Term\n\n\n8\nConvergence of random variables and the strong law of large numbers\n\n\n9\nMarkov chains\n\n\n10\nMarkov chains (continued)\n\n\n11\nPoisson processes\n\n\n12\nGaussian processes and minimum mean squared error estimation\n\n\n13\nWide sense stationary processes\n\n\n\nThe material for the lecture notes is taken from various sources including the textbook and the reference book, as well as the lecture notes of Prof. Ioannis Psaromiglikos (McGill) and Prof. Ashutosh Nayyar (USC).",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#course-material",
    "href": "index.html#course-material",
    "title": "Probability and Random Signals II",
    "section": "Course Material",
    "text": "Course Material\n\nTextbook\n\n\nGrimmett and Stirzaker, Probability and Random Processes, 4th edition, Oxford University Press, 2020.\n\n\n“Engineering” Graduate Probability textbooks\n\n\nJ.A. Gubner, Probability and Random Processes for Electrical and Computer Engineers, Cambridge University Press, 2006.\nS.H. Chan, Introduction to Probability for Data Science, Michigan Publishing, 2021.\nH. Hsu, Probability, Random Variables, and Random Processes, McGraw Hill, 1997.\n\n\nExercise books\n\n\nF. Mosteller, Fifty challenging problems in probability with solutions, Courier Corporation, 1987. Available online\nGrimmett and Stirzaker, One Thousand Exercises in Probability, Oxford University Press, 2000.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Probability and Random Signals II",
    "section": "Evaluation",
    "text": "Evaluation\n\nAssignments (25%) Weekly homework assignments. Typically, each assignment will consist of four or five questions, out of which one or two randomly selected questions will be graded.\nMid Term (25%) Closed book in-class exam. Oct 8 (during class time)\nFinal Exam (50%) Closed book, in-person exam. Will be scheduled by the exam office and the dates will be announced later.\nThe final exam will cover all the material seen in the class during the term.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#marking-policy",
    "href": "index.html#marking-policy",
    "title": "Probability and Random Signals II",
    "section": "Marking policy",
    "text": "Marking policy\n\nAssignments must be submitted electronically on myCourses as a PDF. You may write the assignments on paper and then scan them as a PDF (there are several such apps available for all phone platforms), or write on a tablet and convert to PDF, or type using a word processor.\nThere will no make-up examination for students who miss a mid-term.\n\nStudent who miss the exam due to a valid reason (see Faculty of Engineering policy) should notify the instructor within a week of the exam and provide necessary documentation.\nIf, and only if, proper documentation for a missed exam is presented, the marks for the missed exam will be shifted to the final exam.\nStudents who miss the mid-term exam for any other reason (e.g., no medical note, going to the exam at the wrong time, or on the wrong day, etc.) will get zero marks on the exam.\n\nAny request for reevaluation of a mid-term or an assignment must be made in writing within a week of its return. Note that requesting a re-grade will mean that you WHOLE assignment or exam will be re-graded.\n\n\nRight to submit in English or French written work that is to be graded.\n\nIn accord with McGill University’s Charter of Students’ Rights, students in this course have the right to submit in English or in French any written work that is to be graded.\n\nAcademic Integrity\n\nMcGill University values academic integrity. Therefore all students must understand the meaning and consequences of cheating, plagiarism and other academic offences under the Code of Student Conduct and Disciplinary Procedures (see McGill’s guide to academic honesty for more information).\nL’université McGill attache une haute importance à l’honnêteté académique. Il incombe par conséquent à tous les étudiants de comprendre ce que l’on entend par tricherie, plagiat et autres infractions académiques, ainsi que les conséquences que peuvent avoir de telles actions, selon le Code de conduite de l’étudiant et des procédures disciplinaires (pour de plus amples renseignements, veuillez consulter le guide pour l’honnêteté académique de McGill.)",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#course-delivery",
    "href": "index.html#course-delivery",
    "title": "Probability and Random Signals II",
    "section": "Course delivery",
    "text": "Course delivery\nThe course is taught in a “chalk and board” style; there will be no power point presentations. All students are expected to attend lectures and take notes. Partial notes on some of the material will be provided, but are not a substitute for the material covered in class.\n© Instructor-generated course materials (e.g., handouts, notes, summaries, exam questions) are protected by law and may not be copied or distributed in any form or in any medium without explicit permission of the instructor. Note that infringements of copyright can be subject to follow up by the University under the Code of Student Conduct and Disciplinary Procedures.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#additional-notes",
    "href": "index.html#additional-notes",
    "title": "Probability and Random Signals II",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nAs the instructor of this course I endeavor to provide an inclusive learning environment. However, if you experience barriers to learning in this course, do not hesitate to discuss them with me or contact the office of Student Accessibility and Achievement.\nEnd-of-course evaluations are one of the ways that McGill works towards maintaining and improving the quality of courses and the student’s learning experience. You will be notified by e-mail when the evaluations are available. Please note that a minimum number of responses must be received for results to be available to students.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "probability-spaces.html",
    "href": "probability-spaces.html",
    "title": "1  Probability spaces",
    "section": "",
    "text": "1.1 Background\nThis is a graduate course on probability and random signals. I am going to assume that everyone is familiar with the basics of undergraduate probability. For example, you should be able to answer the following questions:\nYou should also be familiar with the following concepts:\nSome of you might have also seen the following concepts\nIn this course, we will revisit these topics with a more formal approach.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#background",
    "href": "probability-spaces.html#background",
    "title": "1  Probability spaces",
    "section": "",
    "text": "A fair 6-sided die is rolled twice. What is the probability that the sum of the rolls equals 7?\nA biased coin with \\(\\PR({\\rm heads}) = 3/4\\) is tossed 10 times. What is the probability of obtaining 3 consecutive heads?\n\n\n\nRandom variables, probability distributions, and expectations\nConditional distributions and independent random variables\n\n\n\nThe law of large numbers\nThe central limit theorem",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#why-do-we-care-about-probability-theory",
    "href": "probability-spaces.html#why-do-we-care-about-probability-theory",
    "title": "1  Probability spaces",
    "section": "1.2 Why do we care about probability theory",
    "text": "1.2 Why do we care about probability theory\nProbability theory is used to model, analyze, and design various engineering systems. Broadly speaking, there are three forms of random phenomenon that arise in systems: noise, uncertainty, and randomness.\n\nNoise. Noise refers to effects beyond our control. For example, in a digital communication system, when a waveform indicating a binary \\(0\\) is transmitted, background radiation may distort the signal, causing the receiver to incorrectly decode it as \\(1\\). This interference can be modeled as noise in the channel. Similar examples arise in computer networks, photonics, and other areas.\nUncertainty. Uncertainty refers to effects that arise due to lack of information. For example, to bid in an electricity market, a wind farm may need to know how much energy it can generate in the next hour, but this depends on the speed of wind which depends on a lot of factors and is difficult to predict precise. This lack of knowledge can be modelled as uncertainty and the system modeler may have a quantified belief on that uncertainty based on historical data. Similar examples arise in circuits (where quality of a chip may depend on the vagaries in the manufacturing process).\nRandomness. Sometimes introducing randomness can improve system performance. For example, power of two choices algorithm in multi-server load balancing randomly selects two servers and assigns the task to the one with lighter load. This simple strategy effectively reduces the peak load in the servers without needing to query all servers. Similar algorithms are used in bandwidth allocation in ad-hoc networks and randomized routing in networks on chips.\n\nIn this course, we will not focus on how such probabilistic models are constructed. Instead we will study the mathematical properties these models should satisfy and explore the implications of those properties. For the modeling and application-specific details, consider the following courses:\n\nECSE 506: Stochastic Control and Decision Theory\nECSE 508: Multi-agent systems\nECSE 510: Filtering and Prediction for Stochastic Systems\nECSE 511: Introduction to Digital Communication\nECSE 515: Optical Fibre Communications\nECSE 518: Telecommunication Network Analysis\nECSE 521: Digital Communication 1\nECSE 541: Design of Multiprocessor Systems-on-Chip\nECSE 551: Machine Learning for Engineers\nECSE 554: Applied Robotics\nECSE 608: Machine Learning\nECSE 610: Wireless Communication\nECSE 620: Information Theory\nECSE 621: Statistical Detection and Estimation\nECSE 623: Digital Communications 2\nECSE 626: Statistical Computer Vision\n\nAs you can see, probability theory is a foundational tool for Electrical Engineering.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#review-of-set-theory",
    "href": "probability-spaces.html#review-of-set-theory",
    "title": "1  Probability spaces",
    "section": "1.3 Review of Set Theory",
    "text": "1.3 Review of Set Theory\n\nA set is a collection of objects. We say that a set \\(B\\) is a subset of set \\(A\\) (written as \\(B \\subseteq A\\)) if all elements of \\(B\\) are also elements of \\(A\\). We say that \\(B\\) is a proper subset (written \\(B \\subsetneq A\\)) if \\(B \\subseteq A\\) and \\(B \\neq A\\).\n\nExercise 1.1 Let \\(A = \\{1, 2, 3\\}\\). Find all subsets of \\(A\\).\n\nThe set of all subsets of \\(A\\) is also called the power set of \\(A\\) (denoted by \\(2^A\\)). The notation \\(2^A\\) represents that the power set of \\(A\\) contains \\(2^{|A|}\\) elements. For example, your answer to Exercise 1.1 must have \\(2^3 = 8\\) elements.\nGiven two sets \\(A\\) and \\(B\\), we define the set difference \\(A\\setminus B\\) to be all elements of \\(A\\) not in \\(B\\). Note that mathematically \\(A \\setminus B\\) is well defined even if \\(B \\not\\subseteq A\\). In particular \\[\nA \\setminus B = A \\setminus (A \\cap B).\n\\]\n\nExercise 1.2 Compute \\(A \\setminus B\\) for the following:\n\n\\(A = \\{1,2,3,4\\}\\) and \\(B = \\{1, 2\\}\\).\n\\(A = \\{1,2,3,4\\}\\) and \\(B = \\{1, 2, 5\\}\\).\n\n\nGiven a collection \\(\\{A_1, A_2, \\dots, A_n\\}\\) of sets, we define two operations:\n\nUnion \\(A_1 \\cup A_2  \\cup \\cdots \\cup A_n\\) as follows \\[\n  \\bigcup_{i=1}^n A_i = \\{ a: a \\in A_i \\text{ for some } i \\}.\n\\] This means that an element belongs to \\(A_1 \\cup A_2  \\cup \\cdots \\cup A_n\\) if it belongs to at least one of \\(A_1\\), \\(A_2\\), \\(\\ldots\\), \\(A_n\\).\nIntersection \\(A_1 \\cap A_2  \\cap \\cdots \\cap A_n\\) as follows \\[\n  \\bigcap_{i=1}^n A_i = \\{ a: a \\in A_i \\text{ for all } i \\}\n\\] This means that an element belongs to \\(A_1 \\cap A_2  \\cap \\cdots \\cap A_n\\) if it belongs to all of \\(A_1\\), \\(A_2\\), \\(\\ldots\\), \\(A_n\\).\n\nA collection \\(\\{A_1, A_2, \\dots, A_n\\}\\) is disjoint if for every \\(i \\neq j\\), \\(A_i \\cap A_j = \\emptyset\\), where \\(\\emptyset\\) denotes the empty set.\nGiven a universal set \\(Ω\\) and a collection \\(\\{B_1, B_2, \\dots, B_m\\}\\) of subsets of \\(Ω\\), we say that \\(\\{B_1, B_2, \\dots, B_m\\}\\) is a partition of \\(Ω\\) if \\(\\{B_1, B_2, \\dots, B_m\\}\\) are pairwise disjoint and their union equals the universal set \\(Ω\\).\n\n\n\n\n\n\nFigure 1.1: Example of a partition\n\n\n\n\nExample 1.1 Let \\(Ω = \\{1,2,3,4\\}\\). The following are partitions of \\(Ω\\):\n\n\\(\\{ \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\} \\}\\).\n\\(\\{ \\{1, 2\\}, \\{3, 4\\} \\}\\).\n\\(\\{ \\{1\\}, \\{2, 3\\}, \\{4\\} \\}\\).\n\nThe follow are not partitions of \\(Ω\\) [Explain why?]\n\n\\(\\{ \\{1\\}, \\{2\\}, \\{3\\}, \\}\\).\n\\(\\{ \\{1, 2, 3\\}, \\{3, 4\\} \\}\\).\n\\(\\{ \\{1\\}, \\{2, 3\\}, \\{4, 5\\} \\}\\).\n\n\nIn most of our discussion, we will work with a pre-specified universal set \\(Ω\\). In this setting we use \\(A^c\\) (read as the complement of \\(A\\)) as a short hand for \\(Ω\\setminus A\\).\n\n\n\n\n\n\nPartitions are useful because they allow breaking up a set into disjoint pieces. In particular, suppose \\(\\{B_1, \\dots, B_m\\}\\) is a partition and \\(A\\) is any subset of \\(Ω\\).\nThen, \\[\nA = (A \\cap B_1) \\cup (A \\cap B_2) \\cup \\cdots \\cup (A \\cap B_m)\n\\] where each of the components is disjoint.\n\n\n\nProperties of set operations\n\nCommutative \\[A \\cup B = B \\cup A\n\\quad\\text{and}\\quad\nA \\cap B = B \\cap A\\]\nAssociative \\[A \\cup (B \\cup C)= (A \\cup B) \\cup C\n\\quad\\text{and}\\quad\nA \\cap (B \\cap C)= (A \\cap B) \\cap C\\]\nDistributive \\[A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cap C)\n\\quad\\text{and}\\quad\nA \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\]\nDe Morgan’s Law \\[(A \\cup B)^c = A^c \\cap B^\\cap\n\\quad\\text{and}\\quad\n(A \\cap B)^c = A^c \\cup B^c\\]\n\n\nExercise 1.3 Use distributive property to simplify:\n\n\\([1,4] \\cap ([0,2] \\cup [3,5])\\).\n\\([2,4] \\cup ([3,5] \\cap [1,4])\\).\n\n\nGiven a universal set \\(Ω\\), a collection \\(\\ALPHABET F = \\{F_1, F_2, \\dots, F_m\\}\\) of subsets of \\(Ω\\) is called an algebra if it satisfies the following properties:\n\n\\(\\emptyset \\in \\ALPHABET F\\) and \\(Ω \\in \\ALPHABET F\\).\nClosed under complements: if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nClosed under finite unions and finite intersections: if \\(A_1, \\dots, A_n \\in \\ALPHABET F\\), then \\[\nA_1 \\cup A_2 \\cup \\cdots \\cup A_n \\in \\ALPHABET F\n\\quad\\text{and}\\quad\nA_1 \\cap A_2 \\cap \\cdots \\cap A_n \\in \\ALPHABET F\n\\]\n\nWe will sometimes use the notation “\\((Ω,\\ALPHABET F)\\) is an algebra of sets” or “\\(\\ALPHABET F\\) is an algebra on \\(Ω\\)”. Some examples of algebras are as follows:\n\nThe smallest algebra associated with \\(Ω\\) is \\(\\{\\emptyset, Ω\\}\\).\nIf \\(A\\) is any subset of \\(Ω\\), then \\(\\{\\emptyset, A, A^c, Ω\\}\\) is an algebra.\nFor any set \\(Ω\\), the power-set \\(2^Ω\\) is an algebra on \\(Ω\\). As an illustration, check that the power set defined in Exercise 1.1 is an algebra.\n\nThese are all examples of a general principle: The power-set of any partition of a set is an algebra.\n\nIf the partition is \\(\\{Ω\\}\\), then the power-set \\(\\{ \\emptyset, Ω \\}\\) is an algebra.\nIf the partition is \\(\\{A, A^c\\}\\), then the power-set \\(\\{ \\emptyset, A, A^c, Ω\\}\\) is an algebra.\nIf the partition is the collection of all singleton elements of a set, then the power-set \\(2^Ω\\) is an algebra.\n\nWe will use the notation \\(σ(\\ALPHABET D)\\) to denote the algebra generated by a partition \\(\\ALPHABET D\\).\nThe reverse is also true. If \\(\\ALPHABET F\\) is an algebra on a finite space \\(Ω\\), then there is a unique partition \\(\\ALPHABET D = \\{D_1, \\dots, D_m\\}\\) such that \\(\\ALPHABET F = 2^{\\ALPHABET D}\\). The elements \\(D_i\\) are called atoms of \\(\\ALPHABET F\\).\nLet \\(\\ALPHABET D_1\\) and \\(\\ALPHABET D_2\\) be two partitions of \\(Ω\\). We say \\(\\ALPHABET D_1\\) is finer than \\(\\ALPHABET D_2\\) if \\(σ(\\ALPHABET D_1) \\subseteq σ(\\ALPHABET D_2)\\). For instance, suppose \\(Ω = \\{1, 2, 3, 4\\}\\) and \\[\n  \\ALPHABET D_1 = \\{ \\{1,2,3\\}, \\{4\\} \\}\n  \\quad\\text{and}\\quad\n  \\ALPHABET D_1 = \\{ \\{1,2\\}, \\{3\\}, \\{4\\} \\}\n\\] then, \\(\\ALPHABET D_1\\) is finer than \\(\\ALPHABET D_2\\).\nIf a partition \\(\\ALPHABET D_1\\) is finer than \\(\\ALPHABET D_2\\), we also say that \\(\\ALPHABET D_2\\) is coarser than \\(\\ALPHABET D_1\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#functions",
    "href": "probability-spaces.html#functions",
    "title": "1  Probability spaces",
    "section": "1.4 Functions",
    "text": "1.4 Functions\n\nA function \\(f\\) is a rule that assign each input from a set \\(\\ALPHABET X\\) (called the domain) to exactly one output in another set \\(\\ALPHABET Y\\) (called the co-domain). Symbolically, this is written as \\[\n  f \\colon \\ALPHABET X \\to \\ALPHABET Y\n\\] and we say \\(f\\) maps \\(\\ALPHABET X\\) to \\(\\ALPHABET Y\\).\nThe set of values \\(\\{ f(x) : x \\in \\ALPHABET X\\}\\) is called the range. By definition, the range is a subset of the co-domain \\(\\ALPHABET Y\\), but the range may or may not be equal to \\(\\ALPHABET Y\\). For example, consider the function \\(f \\colon \\reals \\to \\reals\\) defined by \\(f(x) = x^2\\). The range of the function is \\(\\reals_{\\ge 0}\\) which is a strict subset of the co-domain \\(\\reals\\).\nA function is called onto or surjective if its range is equal to its co-domain.\nA function is called one-to-one or injective if different inputs maps to different outputs.\nA function which is both onto and one-to-one is called bijective. A bijective function is invertible because for every \\(y \\in \\ALPHABET Y\\), there is a unique \\(x\\) such that \\(f(x) = y\\).\nFor a set \\(B \\subset \\ALPHABET Y\\), the preimage or inverse image of \\(B\\) under \\(f\\) is defined as \\[\n  f^{-1}(B) = \\{ x \\in \\ALPHABET X : f(x) \\in  B \\},\n\\] which is a subset of \\(\\ALPHABET X\\).\n\n\nExample 1.2 Consider the following functions:\n\n\\(f_1 \\colon \\reals \\to \\reals\\)\n\\(f_2 \\colon \\reals \\to \\reals_{\\ge 0}\\)\n\\(f_3 \\colon \\reals_{\\ge 0} \\to \\reals\\)\n\\(f_4 \\colon \\reals_{\\ge 0} \\to \\reals_{\\ge 0}\\)\n\nall given by \\(f_i(x) = x^2\\), \\(i \\in \\{1, \\dots, 4\\}\\).\nExplain if each of the function is onto, into, or bijective.\n\n\nExample 1.3 Let \\(f \\colon \\reals \\to \\reals\\) be given by \\(f(x) = 2x + 1\\). Find \\(f^{-1}([3,7])\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe need to solve for \\(2x + 1 \\in [3,7]\\), which is equivalent to \\[\n  3 \\le 2x + 1 \\le 7\n\\] which is equivalent to \\[\n  1 \\le x \\le 3.\n\\]\nThus, \\(f^{-1}([3,7]) = [1,3]\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#mathematical-model-of-probability",
    "href": "probability-spaces.html#mathematical-model-of-probability",
    "title": "1  Probability spaces",
    "section": "1.5 Mathematical model of probability",
    "text": "1.5 Mathematical model of probability\n\nTo mathematically model probability statements, we need to model the sequence of events that may lead to the occurrence of \\(A\\): this is called a random experiment; the result of an experiment is called an outcome.\nIn general, the outcome of an experiment is not certain. We can only talk about the collection of possible outcomes. The collection of possible outcomes of an experiment is called the sample space and denoted by \\(Ω\\).\n\nExercise 1.4 What is the sample space for the toss of a coin?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\(Ω = \\{ H, T \\}\\).\n\n\n\n\nExercise 1.5 What is the sample space for the roll of a (6-sided) die?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\(Ω = \\{ 1,2,3,4,5,6 \\}\\).\n\n\n\nAn event is any subset of the sample space. If the outcome of the random experiment belongs to the event \\(A\\), we say that “the event \\(A\\) has occurred”. Some examples of events are:\n\nHead occurs in Exercise 1.4 (\\(A = \\{H\\}\\))\nBoth head and tail occur in Exercise 1.4 (\\(A = \\emptyset\\); this is an event that cannot happen, sometimes called the impossible event)\nAn even number is thrown in Exercise 1.5 (\\(A = \\{2,4,6\\}\\)).\n\nNote that events are subset of the sample space but not all subsets of a sample space may be events. The reasons are too complicated to explain, but the high-level explanation is that everything is okay for discrete sample spaces, but weird things can happen in continuous sample spaces.\nProbability (denoted by \\(\\PR\\)) is a function which assigns a number between \\(0\\) and \\(1\\) to every event. This number indicates what is the chance that the event occurs. Such a function should satisfy some axioms, which we will explain below.\nFirst, to define a function, we need to define it’s domain and co-domain Let’s denote the domain (i.e., the set of all events to which we can assign a probability) by \\(\\ALPHABET F\\). We expect probability to satisfy certain properties, which imposes constraints on the domain:\n\nProbability of an impossible event (e.g., getting both heads and tails when we toss a coin) should be zero. Thus, \\(\\emptyset \\in \\ALPHABET F\\).\nProbability of something happening (e.g., getting either a head or a tail when we toss a coin) should be one. Thus, \\(Ω \\in \\ALPHABET F\\).\nIf we assign probability to an event \\(A\\) then we should be able to assign probability to “\\(A\\) does not occur” i.e., \\(A^c\\). Thus, if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nIf we can talk about probability of \\(A\\) and \\(B\\), then we should be able to talk about probability that either \\(A\\) or \\(B\\) occurs and both \\(A\\) and \\(B\\) occur. Thus, if \\(A, B \\in \\ALPHABET F\\), then \\(A \\cup B \\in \\ALPHABET F\\) and \\(A \\cap B \\in \\ALPHABET F\\).\n\nThus, the domain of \\(\\PR\\) must be an algebra! However, when we go beyond finite sample spaces, being an algebra is not sufficient. But we first provide some examples of probability for finite sample spaces.\n\nExample 1.4 (Uniform probability) Consider a finite set \\(Ω\\) with \\(\\ALPHABET F = 2^Ω\\). The uniform probability \\(\\PR\\) on \\(Ω\\) is given by \\[\n\\PR(A) = \\frac{\\ABS{A}}{\\ABS{Ω}},\n\\quad \\forall A \\in \\ALPHABET F\n\\]\n\nAn illustration of uniform probability distribution on the outcomes of a fair coin or a fair dice.\nIn general, when \\(Ω = \\{ω_1, \\dots, ω_n\\}\\) is finite and \\(\\ALPHABET F = 2^{Ω}\\), we can think of probability as an assignment of a weight \\(p(ω_i)\\) to each outcome \\(ω_i\\) such that\n\n\\(0 \\le p(ω_i) \\le 1\\)\n\\(p(ω_1) + \\cdots + p(ω_n) = 1\\).\n\nThen, for any event \\(A \\in \\ALPHABET F\\) (i.e., any \\(A \\subseteq Ω\\), we define \\[\n\\PR(A) = \\sum_{i : ω_i \\in A } p(ω_i).\n\\] This is effectively how probability was defined in undergraduate probability.\n\nExample 1.5 Consider a six-sided die where \\(Ω = \\{1, 2, \\dots, 6 \\}\\), \\(\\ALPHABET F = 2^Ω\\) and \\(\\PR\\) is given by \\[\n\\PR(A) = \\sum_{ω \\in A} p(ω),\n\\quad \\forall A \\in \\ALPHABET F\n\\] where \\[\np(1) = p(2) = p(3) = p(4) = p(5) = \\frac 2{15}\n\\quad\\text{and}\\quad\np(6) = \\frac{1}{3}.\n\\]\nVerify that\n\n\\(\\PR(Ω) = 1\\)\n\\(\\PR(\\{2,3,5\\}) = \\frac{6}{15}\\).\n\\(\\PR(\\{1,3,4,5\\}) = \\frac{8}{15}\\).\n\n\nExample 1.5 can be visualized as follows. First we visualize the probability space as in Figure 1.2.\n\n\n\n\n\n\nFigure 1.2: Probability space of Example 1.5\n\n\n\nTo compute \\(\\PR(\\{2, 3, 5\\})\\), we look at the event \\(A = \\{2, 3, 5\\}\\) and count the probability of all the cells inside \\(A\\), as shown in Figure 1.3.\n\n\n\n\n\n\nFigure 1.3: Computing \\(\\PR(A)\\)\n\n\n\nWe now present an example to illustrate that restricting the domain of \\(\\PR\\) to be an algebra is not sufficient.\n\nExample 1.6 A coin is tossed repeatedly until a head turns up. The sample space is \\(Ω = \\{ω_1, ω_2, \\dots\\}\\) where \\(ω_n\\) denotes the event that the first \\(n-1\\) tosses are tails followed by a head.\n\nSuppose we are interested in finding the probability of the event that the coin is tossed an even number of times, i.e., \\(A = \\{ω_2, ω_4, \\dots\\}\\). Note that \\(ω_2, ω_4, \\dots \\in \\ALPHABET F\\). However, \\(A\\) is a set. If we want to assign probability to \\(A\\) in terms of probability of \\(ω_n\\), we require \\(\\ALPHABET F\\) to be closed under countable unions. This motivates the following definition.\nGiven a universal set \\(Ω\\), a collection \\(\\ALPHABET F = \\{F_1, F_2, \\dots\\}\\) of subsets of \\(Ω\\) is called a \\(\\boldsymbol{σ}\\)-algebra if it satisfies the following properties:\n\n\\(\\emptyset \\in \\ALPHABET F\\) and \\(Ω \\in \\ALPHABET F\\).\nClosed under complements: if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nClosed under countable unions: if \\(A_1, A_2, \\dots \\in \\ALPHABET F\\), then \\[\n\\bigcup_{n=1}^∞ A_n \\in \\ALPHABET F\n\\]\n\n\n\n\n\n\n\nThe distinction between algebras and \\(σ\\)-algebras is technical. The reason that we need to consider \\(σ\\)-algebras is to do with the definition of probability on continuous sample spaces. Take \\(Ω = [0,1]\\) and consider a random experiment where “any outcome is equally likely”. Intuitively we capture this feature by assuming that for any interval \\([a,b]\\) with \\(0 \\le a \\le b \\le 1\\), we have \\[\\begin{equation}\\label{eq:uniform}\n\\PR([a,b]) = b - a.\n\\end{equation}\\]\nWe have seen that if we want \\(\\PR\\) to be a meaningful measure, the domain \\(\\ALPHABET F\\) must at least be an algebra. We have also seen that the power-set \\(2^Ω\\) is always an algebra. So, it is tempting to take \\(\\ALPHABET F = 2^{[0,1]}\\). However, it turns out that \\(2^{[0,1]}\\) includes some weird sets (technically, non-measurable sets) due to which we cannot define a function \\(\\PR\\) on \\(2^{[0,1]}\\) that satisfies \\(\\eqref{eq:uniform}\\).\nTo workaround this technical limitation, we revisit the minimum requirements that we need from the domain of \\(\\PR\\). Since we are interested in \\(\\PR([a,b])\\), \\(\\ALPHABET F\\) must contain intervals (and therefore all finite unions and intersections of intervals). Since we are working with continuous sample spaces, we also want \\(\\PR\\) to be continuous, i.e., for any sequence of sets \\(\\{A_n\\}_{n \\ge 1}\\), we want \\(\\PR(\\lim_{n \\to ∞} A_n) = \\lim_{n \\to ∞} \\PR(A_n)\\). It turns out that the additional requirement of continuity implies that \\(\\ALPHABET F\\) must be closed under countable unions as well. Thus, the domain \\(\\ALPHABET F\\) must at least be a \\(σ\\)-algebra.\nSo, we restrict to the simplest choice of the domain \\(\\ALPHABET F\\) needed for \\(\\eqref{eq:uniform}\\) and continuity to hold. For technical reasons, we need another property known as completeness. See Sec. 1.6 of the textbook.\n\n\n\n\n\n\n\n\n\nTip\\(σ\\)-algebra generated by a collection and Borel \\(σ\\)-algebra\n\n\n\nGiven collection \\(\\ALPHABET S\\) of subsets of \\(Ω\\), we have the following:\n\nThe power-set \\(2^Ω\\) contains \\(\\ALPHABET S\\). Therefore, there is at least one \\(σ\\)-algebra containing \\(\\ALPHABET S\\).\nIf \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) are \\(σ\\)-algebras containing \\(\\ALPHABET S\\), then \\(\\ALPHABET F_1 \\cap \\ALPHABET F_2\\) also contains \\(\\ALPHABET S\\).\n\nThus, if we take the intersection of all \\(σ\\)-algebras containing \\(\\ALPHABET S\\), we get the smallest \\(σ\\)-algebra containing \\(\\ALPHABET S\\), which is sometimes denoted by \\(σ(\\ALPHABET S)\\).\nOne commonly used \\(σ\\)-algebra is the Borel \\(σ\\)-algebra, which is defined as follows. Let \\(Ω\\) be a subset of \\(\\reals\\) and \\(\\ALPHABET S\\) be the collection of all open intervals in \\(Ω\\). Then \\(σ(\\ALPHABET S)\\) is called the “Borel \\(σ\\)-algebra on Ω” and often denoted by \\(\\mathscr{B}(Ω)\\).\nNote: Borel \\(σ\\)-algebra is usually defined for any topological space. We restrict our definition to subsets of reals.\n\n\nA pair \\((Ω, \\ALPHABET F)\\) where \\(Ω\\) is a set and \\(\\ALPHABET F\\) is a \\(σ\\)-algebra on \\(Ω\\) is called a measurable space.\n\nDefinition 1.1 (Probability) Given a measurable space \\((Ω, \\ALPHABET F)\\), probability \\(\\PR \\colon \\ALPHABET F \\to [0,1]\\) is a function that satisfies the following axioms of proability\n\nNon-negativity. \\(\\PR(A) \\ge 0\\).\nNormalization. \\(\\PR(Ω) = 1\\).\nCountable additivity. If \\(A_1, A_2, \\dots Ω\\) is a collection of disjoint events in \\(\\ALPHABET F\\), then, \\[\n\\PR\\biggl( \\bigcup_{n=1}^∞ A_n \\biggr) =\n\\sum_{n=1}^∞ \\PR(A_n).\n\\]\n\nThe collection \\((Ω, \\ALPHABET F, \\PR)\\) is called a probability space.\n\nSome immediate implications of the axioms of probability are the following.\n\nLemma 1.1 (Properties of probability measures)  \n\nProbability of complement. \\(\\PR(A^c) = 1 - \\PR(A)\\).\nMonotonicity. If \\(A \\subset B\\), then \\(\\PR(B) = \\PR(A) + \\PR(B \\setminus A) \\ge \\PR(A)\\).\nInclusion-exclusion. Given two events \\(A\\) and \\(B\\), \\[\n\\PR(A \\cup B) = \\PR(A) + \\PR(B) - \\PR(A \\cap B).\n\\]\nContinuity. Let \\(A_1, A_2, \\dots\\) be (weakly) increasing sequence of events, i.e., \\(A_1 \\subseteq A_2 \\subseteq A_3 \\subseteq \\cdots\\). Define \\[\n  A = \\lim_{n \\to ∞} A_n = \\bigcup_{n=1}^∞ A_n.\n\\] Then, \\[\n  \\PR(A) = \\lim_{n \\to ∞} \\PR(A_n).\n\\]\nSimilarly, let \\(B_1, B_2, \\dots\\) be (weakly) decreasing sequence of events, i.e., \\(B_1 \\supseteq B_2 \\supseteq B_3 \\supseteq \\cdots\\). Define \\[\n   B = \\lim_{n \\to ∞} B_n = \\bigcup_{n=1}^∞ B_n.\n\\] Then, \\[\n   \\PR(B) = \\lim_{n \\to ∞} \\PR(B_n).\n\\]\nUnion bound. For any sequence of events \\(\\{A_n\\}_{n \\ge 1}\\), we have \\[\n\\PR\\biggl( \\bigcup_{n=1}^{∞} A_n \\biggr) \\le\n\\sum_{n=1}^{∞} \\PR(A_n).\n\\]\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof of parts (a)–(c) is elementary and left as an exercise. Part (d) is more technical and is essentially equivalent to countable additivity. See the textbook for a proof. Union bound is an immediate consequence of inclusion-exclusion and continuity.\n\n\n\nLet \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr B[0,1]\\), and \\(\\PR\\) be any probability measure on \\((Ω, \\ALPHABET F)\\). Take any \\(a \\in (0,1)\\).\n\nConsider \\(A_n = \\bigl[0, a - \\frac 1n \\bigr)\\). Then \\(A = \\lim_{n \\to ∞} A_n =  [0, a)\\).\nConsider \\(A_n = \\bigl[0, a - \\frac 1n \\bigr]\\). Then \\(A = \\lim_{n \\to ∞} A_n =  [0, a)\\).\nConsider \\(B_n = \\bigl[0, 1 + \\frac 1n \\bigr)\\). Then \\(B = \\lim_{n \\to ∞} B_n = [0,a]\\).\nConsider \\(B_n = \\bigl[0, 1 + \\frac 1n \\bigr]\\). Then \\(B = \\lim_{n \\to ∞} B_n = [0,a]\\).\n\nIn these examples, continuity implies that \\(\\PR(A) = \\lim_{n \\to ∞} \\PR(A_n)\\) and \\(\\PR(B) = \\lim_{n \\to ∞} \\PR(B_n)\\).\n\n\n\n\n\n\nTipSome terminology\n\n\n\n\nAn event \\(A\\) is called null if \\(\\PR(A) = 0\\). Null event should not be confused with impossible event \\(\\emptyset\\).\nWe say that \\(A\\) occurs almost surely (abbreviated to a.s.) if \\(\\PR(A) = 1\\).\n\n\n\nConsider \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr B([0,1])\\), and \\(\\PR\\) to be the uniform probability distribution on \\(Ω\\). Consider the event \\(A\\) that the outcome is a rational number. \\(A\\) is a countable set (because the set of rational numbers is countable). For any \\(x \\in A\\), \\(\\{x\\} \\in \\ALPHABET F\\), and \\(\\PR(\\{x\\}) = 0\\) (we can infer this from the previous exercise by thinking of \\(\\{x\\}\\) as the limit of intervals \\(\\bigl[x, x+ \\frac 1n\\bigr]\\)). Thus, by countable additivity, \\(\\PR(A) = 0\\). Hence, \\(A\\) is null.\nThe above analysis implies that \\(\\PR(A^c) = 1\\), thus the event that the outcome is irrational occurs almost surely.\n\n\nExercise 1.6 Consider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) and events \\(A, B \\in \\ALPHABET F\\). Using axioms of probability, show that:\n\nIf \\(B \\subseteq A\\), then \\(\\PR(A \\setminus B) = \\PR(A) - \\PR(B)\\). Hence, argue that \\(\\PR(A) \\ge \\PR(B)\\).\n\\(\\PR(A \\cup B) = \\PR(A) + \\PR(B) - \\PR(A \\cap B)\\).\n\\(\\PR(A) = \\PR(A \\cap B) + \\PR(A \\cap B^c)\\).\n\n\n\nExercise 1.7 Consider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) and events \\(A, B, C \\in \\ALPHABET F\\). Prove that \\[\n    \\PR(A \\cup B \\cup C) = 1 - \\PR(A^c \\mid B^c \\cap C^c) \\PR(B^c \\mid C^c) \\PR(C^c).\n  \\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#conditional-probability",
    "href": "probability-spaces.html#conditional-probability",
    "title": "1  Probability spaces",
    "section": "1.6 Conditional Probability",
    "text": "1.6 Conditional Probability\n\nConditional probabilities quantify the uncertainty of an event when it is known that another event has occurred\n\nDefinition 1.2 Let \\((Ω,\\ALPHABET F, \\PR)\\) be a probability space and \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(B) &gt; 0\\). Then, the conditional probability that \\(A\\) occurs given that \\(B\\) occurs is defined as \\[\n\\PR(A | B) = \\dfrac{ \\PR(A \\cap B) }{ \\PR(B) }.\n\\]\n\nThe notation \\(\\PR(A | B)\\) is read as “probability of \\(A\\) given \\(B\\)” or “probability of \\(A\\) conditioned on \\(B\\)”.\n\nExercise 1.8 Suppose we roll a fair six-sided die (a fair die means that all outcomes are equally likely). Consider the events \\(A\\) that the outcomes is prime and \\(B\\) that the outcome is a multiple of \\(3\\). Compute \\(\\PR(A | B)\\) and \\(\\PR(B | A)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe have \\(Ω = \\{1, 2, 3, 4, 5, 6\\}\\), \\(A = \\{2, 3, 5\\}\\), and \\(B = \\{3, 6\\}\\). Note that \\(\\PR(A) = \\frac 12\\) and \\(\\PR(B) = \\frac 13\\).\nThus, \\[ \\PR(A | B) = \\frac{ \\PR(A \\cap B) }{ \\PR(B) }\n= \\frac{ \\PR(\\{3\\}) }{ \\PR(\\{3,6\\}) }\n= \\frac{ \\ABS{\\{3\\}} }{ \\ABS{\\{3,6\\}} } = \\frac {1}{2}.\n\\] Similarly, \\[ \\PR(B | A) = \\frac{ \\PR(B \\cap A) }{ \\PR(A) }\n= \\frac{ \\PR(\\{3\\}) }{ \\PR(\\{2,3,5\\}) }\n= \\frac{ \\ABS{\\{3\\}} }{ \\ABS{\\{2,3,5\\}} } = \\frac {1}{3}.\n\\]\n\n\n\n\nExercise 1.9 Suppose we roll two fair six-sided dice. Consider the event \\(A\\) that the maximum of the two rolls is less than or equal to \\(8\\) and the event \\(B\\) that the minimum of the two rolls is greater than or equal to \\(6\\). Compute \\(\\PR(A|B)\\) and \\(\\PR(B|A)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nNote that \\(Ω = \\{ 1, 2,3, 4, 5, 6\\}^2\\) and \\(\\PR\\) is uniform probability on all outcomes. The sets \\(A\\), \\(B\\), and \\(A \\cap B\\) are shown in Figure 1.4. Note that \\(\\PR(A) = \\PR(B) = \\frac{26}{36} = \\frac{13}{18}\\).\n\n\n\n\n\n\nFigure 1.4: The different events in Exercise 1.9\n\n\n\nThus, we have \\[\n\\PR(A|B) = \\frac{ \\PR(A \\cap B) } { \\PR(B) }\n= \\frac{ \\ABS{ A \\cap B} }{ \\ABS{B} }\n= \\frac{16}{26} = \\frac{8}{13}\n\\] and \\[\n\\PR(B|A) = \\frac{ \\PR(B \\cap A) } { \\PR(A) }\n= \\frac{ \\ABS{ B \\cap A} }{ \\ABS{A} }\n= \\frac{16}{26} = \\frac{8}{13}\n\\]\n\n\n\n\n\n\n\n\n\nTipConditional probabilities are probabilities\n\n\n\nConditional probabilities are legitimate probability measures on \\((Ω, \\ALPHABET F)\\). In particular, fix event \\(B\\) with \\(\\PR(B) &gt; 0\\). Then\n\n\\(\\PR(A \\mid B) \\ge 0\\).\n\\(\\PR(Ω \\mid B) = \\dfrac{\\PR(Ω \\cap B)}{\\PR(B)} = 1\\).\nFor disjoint events \\(A_1, A_2 \\in \\ALPHABET F\\), \\[\\PR(A_1 \\cup A_2 \\mid B) =\n\\frac{ \\PR( (A_1 \\cup A_2) \\cap B) }{ \\PR(B) } =\n\\frac{ \\PR( (A_1 \\cap B) \\cup (A_2 \\cap B) ) }{ \\PR(B) } =\n\\frac{ \\PR(A_1 \\cap B) + \\PR (A_2 \\cap B) }{ \\PR(B) } =\n\\PR(A_1 \\mid B) + \\PR(A_2 \\mid B)\\] where we have used the fact that \\((A_1 \\cap B)\\) and \\((A_2 \\cap B)\\) are disjoint.\n\n\n\n\nExercise 1.10 Given an event \\(B\\) with \\(\\PR(B) &gt; 0\\), show that\n\n\\(\\PR(A^c | B) = 1 - P(A|B)\\).\n\\(\\PR(A_1 \\cup A_2 | B) = \\PR(A_1 | B) + \\PR(A_2 | B) - \\PR(A_1 \\cap A_2 | B)\\).\nIf \\(A_1 \\subset A_2\\) then \\(\\PR(A_1 | B) \\le \\PR(A_2 | B)\\).\n\n\nThe definition of conditional probability gives rise to the chain rule.\n\nLemma 1.2 (Chain rule of probability) Let \\(A\\) and \\(B\\) be events in a probability space \\((Ω, \\ALPHABET F, \\PR)\\).\n\nIf \\(\\PR(B) &gt; 0\\), then \\(\\PR(A \\cap B) = \\PR(A | B) \\PR(B)\\).\nIf \\(\\PR(A) &gt; 0\\), then \\(\\PR(A \\cap B) = \\PR(B | A) \\PR(A)\\).\n\n\nCombining the chain rule with the basic properties of partitions, we get the law of total probability.\n\nLemma 1.3 (Law of total probability) Let \\(\\{B_1, B_2, \\dots, B_m\\}\\) be a partition of \\(Ω\\) such that \\(\\PR(B_i) &gt; 0\\) for all \\(i\\). Then, \\[\n\\PR(A) = \\sum_{i=1}^m \\PR(A \\cap B_i)\n= \\sum_{i=1}^m \\PR(A | B_i) \\PR(B_i).\n\\]\n\n\n\n\n\n\nFigure 1.5: Illustration of Law of total probability\n\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nConsider \\(m=2\\), in which case the result can be simplified as \\[\\PR(A) = \\PR(A|B)\\PR(B) + \\PR(A|B^c) \\PR(B^c).\\]\nTo prove this observe that \\[\\begin{equation}\\label{eq:two-step}\nA = A \\cap (B \\cup B^c) = (A \\cap B) \\cup (A \\cap B^c).\n\\end{equation}\\] The events \\(A \\cap B\\) and \\(A \\cap B^c\\) are disjoint. Therefore, by additivity, we have \\[\n\\PR(A) = \\PR(A \\cap B) + \\PR(A \\cap B^c).\n\\] Then, by the definition of conditional probability, we have \\(\\PR(A \\cap B) = \\PR(A|B) \\PR(B)\\) and \\(\\PR(A \\cap B^c) = \\PR(A|B^c) \\PR(B^c)\\). Substituting in the above, we get \\(\\eqref{eq:two-step}\\).\nThe argument for the general case is similar.\n\n\n\n\nExercise 1.11 There are two routes for a packet to be transmitted from a source to the destination.\n\nThe packet takes route \\(R_1\\) with probability \\(\\frac 34\\) and takes route \\(R_2\\) with probability \\(\\frac 14\\).\nOn route \\(R_1\\), the packet is dropped with probability \\(\\frac 13\\).\nOn route \\(R_2\\), the packet is dropped with probability \\(\\frac 14\\).\n\nFind the probability that the packet reaches the destination.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe start by defining some events: Let \\(R_1\\) denote the event that the packet took route \\(R_1\\) and \\(R_2\\) denote the event that the packet took route \\(R_2\\). Let \\(D\\) denote the event that the packet was dropped.\nThen, the information given in the question can be written as:\n\n\\(\\PR(R_1) = \\frac 34\\) and \\(\\PR(R_2) = \\frac 14\\).\n\\(\\PR(D | R_1) = \\frac 13\\). Thus, \\(\\PR(D^c | R_1) = 1 - \\PR(D | R_1) = \\frac 23\\).\n\\(\\PR(D | R_2) = \\frac 14\\). Thus, \\(\\PR(D^c | R_2) = 1 - \\PR(D | R_2) = \\frac 34\\).\n\nThen, by the law of total probability, we have \\[\\begin{align*}\n\\PR(D^c) &= \\PR(D^c | R_1) \\PR(R_1) + \\PR(D^c | R_2) \\PR(R_2) \\\\\n&= \\frac 34 \\frac 23 + \\frac 14 \\frac 34\n= \\frac {11}{16}.\n\\end{align*}\\]\n\n\n\n\nLemma 1.4 (Bayes rule) For any events \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(A), \\PR(B) &gt; 0\\), we have \\[\n\\PR(B|A) = \\dfrac{\\PR(A|B)\\PR(B)}{\\PR(A)}.\n\\]\nIn general, if \\(\\{B_1, B_2, \\dots, B_m\\}\\) is a partition of \\(Ω\\) such that \\(\\PR(B_i) &gt; 0\\) for all \\(i\\). Then, \\[\n\\PR(B_i|A) =\n\\dfrac{ \\PR(A|B_i) \\PR(B_i) }\n{\\displaystyle \\sum_{j=1}^m \\PR(A|B_j) \\PR(B_j)}\n\\] where we have used the law of total probability (Lemma 1.3) in the denominator.\n\n\nExercise 1.12 Consider the model of Exercise 1.11. Suppose we know that the packet was dropped. What is the probability that it was transmitted via route \\(R_1\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall the events \\(R_1\\), \\(R_2\\), and \\(D\\) defined in the solution of Exercise 1.11. We were given that \\[\\PR(R_1) = \\frac 34, \\quad \\PR(R_2) = \\frac 14, \\quad\n\\PR(D|R_1) = \\frac 13, \\quad \\PR(D|R_2) = \\frac 14.\\] We had compute that \\[\n\\PR(D) = 1 - \\PR(D^c) = \\frac 5{16}.\n\\]\nThus, by Bayes rule, we have \\[\n\\PR(R_1 | D) = \\frac{ \\PR(D | R_1) \\PR(R_1) }{ \\PR(D) }\n= \\frac{ \\frac 13 \\frac 34 } { \\frac 5{16} } = \\frac 45.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#independence",
    "href": "probability-spaces.html#independence",
    "title": "1  Probability spaces",
    "section": "1.7 Independence",
    "text": "1.7 Independence\n\nIn general, the knowledge that an event \\(B\\) has occurred changes the probability of event \\(A\\), since \\(\\PR(A)\\) is replaced by \\(\\PR(A|B)\\). If the knowledge that \\(B\\) has occurred does not does not change our belief about \\(A\\), i.e., when \\(\\PR(A|B) = \\PR(A)\\), we say “\\(A\\) and \\(B\\) are independent”. This leads to the following definition.\n\nDefinition 1.3 The events \\(A, B \\in \\ALPHABET F\\) are called independent if \\[\n\\PR(A|B) = \\PR(A)\n\\quad\\text{or}\\quad\n\\PR(B|A) = \\PR(B).\n\\] An alternative but equivalent definition is \\[\n\\PR(A \\cap B) = \\PR(A) \\PR(B).\n\\]\nWe will use the notation \\(A \\independent B\\) to denote that the events \\(A\\) and \\(B\\) are independent.\n\n\nExample 1.7 The events \\(A\\) and \\(B\\) defined in Exercise 1.8 are independent.\n\n\nExample 1.8 The events \\(A\\) and \\(B\\) defined in Exercise 1.9 are not independent.\n\n\nExercise 1.13 \\(A \\independent B\\) implies the following:\n\n\\(A \\independent B^c\\).\n\\(A^c \\independent B\\).\n\\(A^c \\independent B^c\\).\n\n\nIndependence is what separates probability theory from the more general measure theory.\n\n\n\n\n\n\nIt is common for students to make the mistake and think that independence means \\(A \\cap B = \\emptyset\\). This is not true!\n\n\n\n\n\n\n\n\n\nTipIndependence of \\(σ\\)-algebras\n\n\n\nIn the discussion below, we assume that the probability space \\((Ω, \\ALPHABET F, \\PR)\\) is fixed.\n\nTwo sub-\\(σ\\)-algebras \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) of \\(\\ALPHABET F\\) are said to be independent if every event \\(A_1 \\in \\ALPHABET F_1\\) is independent of every event \\(A_2 \\in \\ALPHABET F_2\\).\nFor any event \\(A\\), let \\(σ(A)\\) denote the smallest \\(σ\\)-algebra containing \\(A\\), i.e., \\(σ(A) = \\{\\emptyset, A, A^c, Ω\\}\\).\nExercise 1.13 implies that independence of \\(A\\) and \\(B\\) implies the independence of \\(σ(A)\\) and \\(σ(B)\\). The reverse implication is trivially true. Thus, independence of events is equivalent to the independence of the smallest \\(σ\\)-algebra containing those events.\n\n\n\n\nDefinition 1.4 A family of events \\(\\{A_1, A_2, \\dots, A_n\\}\\) is called independent (sometimes mutually independent if for all non-empty subset of indices \\(\\{k_1, \\dots, k_m\\} \\subset \\{1,\\dots,n\\}\\), we have \\[\n\\PR\\bigl( A_{k_1} \\cap A_{k_2} \\cap \\cdots \\cap A_{k_m} \\bigr)\n= \\PR(A_{k_1}) \\PR(A_{k_2}) \\cdots \\PR(A_{k_m}).\n\\]\n\n\nExercise 1.14 Three bits are transmitted over a noisy channel. For each bit, the probability of correct reception is \\(λ\\). The error events for the three transmissions are mutually independent. Find the probability that two bits are received correctly.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFor \\(i \\in \\{1, 2, 3\\}\\), let\n\n\\(E_i\\) denote the event that bit \\(i\\) is received incorrectly\n\\(C_i\\) denote the event that bit \\(i\\) is received correctly\n\nMoreover let \\(S\\) denote the event that two bits are received correctly. Then, \\[\nS = (C_1 \\cap C_2 \\cap E_3) \\cup (C_1 \\cap E_2 \\cap C_3)\n\\cap (E_1 \\cap C_2 \\cap C_3).\n\\] Note that the three events in the right hand side are disjoint. Thus, \\[\\begin{align*}\n\\PR(S) &=\n\\PR(C_1 \\cap C_2 \\cap E_3) +  \\PR(C_1 \\cap E_2 \\cap C_3) +\n\\PR(E_1 \\cap C_2 \\cap C_3) \\\\\n&=\n\\PR(C_1)\\PR(C_2)\\PR(E_3) +  \\PR(C_1)\\PR(E_2)\\PR(C_3) +\n\\PR(E_1)\\PR(C_2)\\PR(C_3) \\\\\n&= 3 (1-λ) λ^2.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nWarningPairwise independence vs independence\n\n\n\nA family of events \\(\\{A_1, A_2, \\dots, A_n\\}\\) is pairwise independent if for every \\(i,j \\in \\{1, \\dots, n\\}\\), \\(i \\neq j\\), we have \\[ \\PR(A_i \\cap A_j) = \\PR(A_i) \\PR(A_j). \\]\nPairwise independence is weaker than Independence. For instance, three events \\(A\\), \\(B\\), and \\(C\\) are pairwise independent if \\[\n\\PR(A \\cap B) = \\PR(A) \\PR(B),\n\\quad\n\\PR(B \\cap C) = \\PR(B) \\PR(C),\n\\quad\\text{and}\\quad\n\\PR(C \\cap A) = \\PR(C) \\PR(A).\n\\] For independence, in addition to the above, we also need \\[ \\PR(A \\cap B \\cap C) = \\PR(A)\\PR(B) \\PR(C). \\]\nThe following example illustrates shows that independence is stronger than pairwise independence. Consider an urn with \\(M\\) red balls and \\(M\\) blue balls. Two balls are drawn at random, one at a time, with replacement. Consider the following events:\n\n\\(A\\) is the event that the first ball is red.\n\\(B\\) is the event that the second ball is blue.\n\\(C\\) is the event that both balls are of the same color.\n\nObserve that\n\n\\(A \\cap B\\) is the event that the first is red and second is blue.\n\\(B \\cap C\\) is the event that both balls are blue.\n\\(C \\cap A\\) is the event that both balls are red.\n\\(A \\cap B \\cap C = \\emptyset\\)\n\nTherefore,\n\n\\(\\PR(A) = \\PR(B) = \\PR(C) = \\frac 12\\).\n\\(\\PR(A \\cap B) = \\PR(B \\cap C) = \\PR(C \\cap A) = \\frac 14\\).\n\\(\\PR(A \\cap B \\cap C) = \\emptyset\\).\n\nThus, \\(A\\), \\(B\\), \\(C\\) are pairwise independent but not independent.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#product-spaces",
    "href": "probability-spaces.html#product-spaces",
    "title": "1  Probability spaces",
    "section": "1.8 Product spaces",
    "text": "1.8 Product spaces\n\nSo far, we have restricted attention to the outcome of one experiment. It is also possible to construct probability models which combine the outcome of two independent experiments, e.g., suppose we toss a coin and also roll a die. Let \\((Ω_1, \\ALPHABET F_1, \\PR_1)\\) and \\((Ω_2, \\ALPHABET F_2, \\PR_2)\\) be the probability spaces associated with the two experiments? What is the probability space \\((Ω, \\ALPHABET F, \\PR)\\) of the joint experiments?\nThe sample space should obviously be \\(Ω = Ω_1 \\times Ω_2\\). When \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) are finite, then we can simply define \\(\\ALPHABET F = \\ALPHABET F_1 \\times \\ALPHABET F_2\\) and for any \\(A = (A_1, A_2) \\in \\ALPHABET F\\), \\(\\PR(A)\\) to be \\(\\PR_1(A_1) \\PR_2(A_2)\\). Note that \\(\\PR(Ω) = \\PR_1(Ω_1)\\PR_2(Ω_2) = 1\\), thus \\((Ω, \\ALPHABET F, \\PR)\\) is a valid probability space. This space is called direct product of probability spaces \\((Ω_1, \\ALPHABET F_1, \\PR_1)\\) and \\((Ω_2, \\ALPHABET F_2, \\PR_2)\\).\nYou would have implicitly constructed such product spaces when dealing with joint experiments (like the coin toss and die roll example above) in your undergrad courses. Such constructions were correct because of the following.\nGiven \\(A_1 \\in \\ALPHABET F_1\\) and \\(A_2 \\in \\ALPHABET F_2\\), define the events \\[\n   B_1 = \\{ ω  = (ω_1, ω_2) \\in Ω : ω_1 \\in A_1 \\}\n   \\quad\\text{and}\\quad\n   B_2 = \\{ ω  = (ω_1, ω_2) \\in Ω : ω_2 \\in A_2 \\}.\n\\] Then, \\[\n   \\PR(B_1 \\cap B_2) = \\PR_1(A_1) \\PR_2(A_2) = \\PR(B_1) \\PR(B_2).\n\\] Thus, events \\(B_1\\) and \\(B_2\\) are independent.\nIt is a bit more complicated when \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) are not finite. The difficulty is that \\(\\ALPHABET F_1 \\times \\ALPHABET F_2\\) is not a \\(σ\\)-algebra. So, take \\(\\ALPHABET F\\) to be \\(σ(\\ALPHABET F_1 \\times \\ALPHABET F_2)\\) (which is the smallest \\(σ\\)-algebra comtaining \\(\\ALPHABET F_1 \\times \\ALPHABET F_2\\)) and define \\(\\PR\\) to be the extension of \\(\\PR_1 \\times \\PR_2\\) from \\(\\ALPHABET F_1 \\times \\ALPHABET F_2\\) to \\(σ(\\ALPHABET F_1 \\times \\ALPHABET F_2)\\) (one can show that such an extension exists). Such product space is often written as \\[\n(Ω, \\ALPHABET F, \\PR) = (Ω_1 \\times Ω_2, \\ALPHABET F_1 \\otimes \\ALPHABET F_2, \\PR_1 \\otimes \\PR_2). \\]\n\nWe will not worry too much about the technical details of such product spaces, but will use the above notation at times in the course.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#notes",
    "href": "probability-spaces.html#notes",
    "title": "1  Probability spaces",
    "section": "Notes",
    "text": "Notes\nThe material for this section is fairly standard and adapted from various sources. Both Grimmit and Stirzaker and Gubner have an excellent coverage of this material. The idea of categorizing random phenomenon as noise, uncertainty, or randomness is borrowed from Maxim Raginsky’s course notes. See the book Against the Gods, which provides a fascinating historical account of the development of probability theory and, why a conceptual leap was needed to recognize that uncertainty could be modeled and measured.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "random-variables.html",
    "href": "random-variables.html",
    "title": "2  Random variables",
    "section": "",
    "text": "2.1 Real-valued random variables",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#real-valued-random-variables",
    "href": "random-variables.html#real-valued-random-variables",
    "title": "2  Random variables",
    "section": "",
    "text": "The standard notation in probability theory is to use uppercase letters such as \\(X\\), \\(Y\\), \\(Z\\), etc. to denote random variables and the corresponding lowercase letters \\(x\\), \\(y\\), \\(z\\), etc. to denote the possible numerical values of these variables.\nIf \\(X\\) is a (real-valued) random variable on \\((Ω, \\ALPHABET F, \\PR)\\), then measurability implies that for every \\(B \\in \\mathscr{B}(\\reals)\\), \\(X^{-1}(B) \\in \\ALPHABET F\\). Rather than explicitly defining the measure \\(\\PR_X\\), we simply use the notation \\[\\PR(X \\in B) \\coloneqq \\PR(\\{ω \\in Ω : X(ω) \\in B \\}).\\]\nIf we take \\(B\\) to be the interval \\((-∞, x]\\), then the event \\(X^{-1}(B) = \\{ ω \\in Ω : X(ω) \\le x \\}\\). Such events are abbreviated as \\(\\{ω : X(ω) \\le x \\}\\) or \\(\\{X \\le x\\}\\). Thus, we have\n\n\\(\\PR(X \\le x) = \\PR(\\{ω \\in Ω : X(ω) \\le x \\})\\).\n\\(\\PR(X = x) = \\PR(\\{ω \\in Ω : X(ω) = x \\})\\).\n\\(\\PR(x &lt; X \\le y) = \\PR(\\{ω \\in Ω : x &lt; X(ω) \\le y \\})\\).\n\nThe cumulative distribution function (CDF) of a (real-valued) random variable \\(X\\) is the function \\(F_X \\colon \\reals \\to [0,1]\\) given by \\[\nF_X(x) \\coloneqq \\PR(X \\le x) = \\PR(\\{ ω \\in Ω : X(ω) \\le x \\}).\n\\]\nFor instance, in Example 2.1, the CDF is given by \\[\nF_X(x) = \\begin{cases}\n0, & \\hbox{if } x &lt; 0,  \\\\\n\\frac 14, & \\hbox{if } 0 \\le x &lt; 1, \\\\\n\\tfrac 34, & \\hbox{if } 1 \\le x &lt; 2,\\\\\n1, &\\hbox{if } 2 \\le x.\n\\end{cases}\\]\n\n\n\n\n\n\nFigure 2.3: CDF of number of heads\n\n\n\nSome examples of CDF of random variables\n\nExample 2.2 (Constant random variables) The simplest random variable takes a constant value on the whole domain \\(Ω\\), i.e., \\[\nX(ω) = c, \\quad \\forall ω \\in Ω\n\\] where \\(c\\) is a constant. The CDF \\(F_X(x) = \\PR(X \\le x)\\) is the step function \\[\nF_X(x) = \\begin{cases}\n0, & x &lt; c \\\\\n1, & x \\ge c.\n\\end{cases}\n\\]\nSlightly more generally, we say that \\(X\\) is almost surely a constant if there exists a \\(c \\in \\reals\\) such that \\(\\PR(X=c) = 1\\).\n\n\n\n\n\n\nFigure 2.4: CDF of a constant random variable\n\n\n\n\n\nExample 2.3 (Indicator functions) Let \\(A\\) be an event. Define the indicator of event \\(A\\), denoted by \\(\\IND_{A} \\colon Ω \\to \\reals\\), as \\[ \\IND_{A}(ω) = \\begin{cases}\n    1, & \\hbox{if } ω \\in A \\\\\n    0, & \\hbox{otherwise }\n\\end{cases}.\\]\n\n\nExample 2.4 (Bernoulli random variable) A Bernoulli random variable takes two possible values: value \\(0\\) with probability \\(1-p\\) and value \\(1\\) with probability \\(p\\). It’s CDF is given by \\[\nF_X(x) = \\begin{cases}\n0, & x &lt; 0 \\\\\n1 - p, & 0 \\le x &lt; 1 \\\\\n1, & x \\ge 1.\n\\end{cases}\n\\] Observe that \\(\\IND_A\\) is a Bernoulli random variable which takes values \\(1\\) and \\(0\\) with probabilities \\(\\PR(A)\\) and \\(1 - \\PR(A)\\).\n\n\n\n\n\n\nFigure 2.5: CDF of a Bernoulli random variable\n\n\n\n\n\nLemma 2.1 (Properties of CDFs)  \n\n\\(\\PR(X &gt; x) = 1 - F_X(x)\\).\n\\(\\PR(x &lt; X \\le y) = F_X(y) - F_X(x)\\).\n\\(\\lim_{x \\to -∞} F_X(x) = 0\\) and \\(\\lim_{x \\to +∞} F_X(x) = 1\\).\nCDFs are non-decreasing, i.e., if \\(x &lt; y\\), then \\(F_X(x) \\le F_X(y)\\).\nCDFs are right continuous, i.e., \\(\\lim_{h \\downarrow 0}F_X(x+h) = F_X(x)\\).\n\\(\\PR(X = x) = F_X(x) - F_X(x^{-})\\), where \\(F_X(x^{-})\\) is defined as \\(\\lim_{h \\downarrow 0} F_X(x - h)\\)\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nBy definition, \\(\\{X &gt; x\\}^c = Ω\\setminus \\{X \\le x\\}\\). Thus, \\[\\PR(X &gt; x) = 1 - \\PR(X \\le x) = 1 - F_X(x).\\]\n\\[\\PR(x &lt; X \\le y) = \\PR(X \\le y) - \\PR(X \\le x) = F_X(y) - F_X(x).\\] [See assignment 1 for the first equality.]\nDefine the increasing sequence of events \\(A_n = \\{ X \\le n\\}\\), \\(n \\in \\naturalnumbers\\). By continuity of probability, we have \\[\\begin{align*}\n  &\\quad & \\PR\\biggl( \\bigcup_{n=1}^{∞} A_n \\biggr)\n  & = \\lim_{n \\to ∞} \\PR(A_n) \\\\\n  \\implies && \\PR(\\{X &lt; ∞\\}) &= \\lim_{n \\to ∞} \\PR(X \\le n) \\\\\n  \\implies && \\PR(Ω) &= \\lim_{n \\to ∞} F_X(n) \\\\\n  \\implies && 1 &= \\lim_{n \\to ∞} F_X(n).\n\\end{align*}\\]\nThe reverse argument is similar where we consider the decreasing sequence of events \\(B_n = \\{ X_n \\le -n \\}\\), \\(n \\in \\naturalnumbers\\). Then, by continuity of probability, we have \\[\\begin{align*}\n     &\\quad & \\PR\\biggl( \\bigcap_{n=1}^{∞} B_n \\biggr)\n     & = \\lim_{n \\to ∞} \\PR(B_n) \\\\\n     \\implies && \\PR(\\{X &lt; -∞\\}) &= \\lim_{n \\to ∞} \\PR(X \\le -n) \\\\\n     \\implies && \\PR(\\emptyset) &= \\lim_{n \\to ∞} F_X(-n) \\\\\n     \\implies && 0 &= \\lim_{n \\to ∞} F_X(-n).\n   \\end{align*}\\]\nRecall that \\[\\begin{align*}\n   F_X(x) &= \\PR(X \\le x) \\\\\n   F_X(y) &= \\PR(X \\le y)\n\\end{align*}\\] Observe that since \\(x &lt; y\\), we have \\(\\{X \\le x\\} \\subseteq \\{X \\le y\\}\\). Hence, by monotonicity of probability, we have \\[\\PR(X \\le x) \\le \\PR(X \\le y),\\] which proves the result.\nConsider the decreasing sequence of sets: \\[ A_n = \\{ X \\le x + \\tfrac 1n \\}, \\quad n \\in \\naturalnumbers. \\] Then, by continuity of probability, we have \\[\\begin{align*}\n  &\\quad & \\PR\\biggl( \\bigcap_{i=1}^{∞} A_n \\biggr)\n  & = \\lim_{n \\to ∞} \\PR(A_n) \\\\\n  \\implies && \\PR(X \\le x) &= \\lim_{n \\to ∞} \\PR(X \\le x + \\tfrac 1n ) \\\\\n  \\implies && F_X(x) &= \\lim_{n \\to ∞} F_X(x + \\tfrac 1n).\n\\end{align*}\\]\nDefine the decreasing sequence of sets \\[ A_n = \\biggl\\{ x - \\frac 1n &lt; X \\le x \\biggr\\},\n\\quad n \\in \\naturalnumbers.\\] Observe that by the previous property \\[\\PR(A_n) = F_X(x) - F_X(x - \\tfrac 1n).\\] Since \\(A_n\\) is a decreasing sequence of sets, we have \\[\\begin{align*}\n& \\quad &\n\\PR\\biggl( \\bigcap_{n=1}^∞ A_n \\biggr)\n&= \\lim_{n \\to ∞} \\PR(A_n) \\\\\n\\implies && \\PR(X = x) &= F_X(x) - \\lim_{n \\to ∞} F_X(x - \\tfrac 1n) \\\\\n&&& = F_X(x) - F_X(x^{-}).\n\\end{align*}\\]\n\n\n\n\n\n\nExample 2.5 For \\(x &lt; y\\), express the following in terms of the CDF:\n\n\\(\\PR(x \\le X \\le y)\\).\n\\(\\PR(x \\le X &lt; y)\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#classification-of-random-variables",
    "href": "random-variables.html#classification-of-random-variables",
    "title": "2  Random variables",
    "section": "2.2 Classification of random variables",
    "text": "2.2 Classification of random variables\nThere are three types of random variables\n\nA random variable \\(X\\) is said to be discrete if it takes values in a finite or countable subset \\(\\ALPHABET X \\coloneqq \\{x_1, x_2, \\dots\\}\\) of \\(\\reals\\). A discrete random variable has a probability mass function (PMF) \\(p \\colon \\reals \\to [0,1]\\) which satisfies the following properties:\n\n\\(p(x) = \\PR(X = x) = F_X(x) - F_X(x^{-})\\).\n\\(F_X(x) = \\sum_{x_n : x_n \\le x} p(x_n).\\)\n\nThus, for a discrete random variable, the CDF is a piecewise constant function\nFigure 2.3, Figure 2.4, Figure 2.5 are all examples of discrete random variables.\nA random variable \\(X\\) is called continuous if there exists an integrable function \\(f \\colon \\reals \\to [0, ∞)\\) called the probability denisity function such that the CDF can be written as \\[\nF_X(x) = \\int_{-∞}^x f_X(x) dx.\n\\]\nThus, for a continuous random variable, the CDF is a continuous function\n\n\n\n\n\n\nFigure 2.6: CDF of a continuous random variable\n\n\n\nA random variable is called mixed if it is neither discrete nor continuous. For a mixed random variable, the CDF has has jumps at a finite or countable infinite number of points and it is continuous over one or many intervals.\n\n\n\n\n\n\nFigure 2.7: CDF of a mixed random variable\n\n\n\nAs an example, consider the following random experiment. A fair coin is tossed: if the outcome is heads, then \\(X \\sim \\text{Bernoulli}(0.5)\\); if the outcome is tails; then \\(X \\sim \\text{Uniform}(0,1)\\). Thus (from the law of total probability), the CDF of \\(X\\) is given by \\[\n   F_X(x) = \\begin{cases}\n   0, & \\hbox{if } x &lt; 0 \\\\\n   \\frac 14 & \\hbox{if } x = 0 \\\\\n   \\frac 14 + \\frac x2 & \\hbox{if } 0 &lt; x &lt; 1 \\\\\n   1 & \\hbox{if } x \\ge 1.\n   \\end{cases}\n   \\] and shown in Figure 2.7.\n\n\nLemma 2.2 (Properties of discrete and continuous random variables)  \n\nProperties of discrete random variables\nFor a discrete random variable \\(X\\), define the probability mass function (PMF) \\(P_X \\colon \\reals \\to \\reals\\) as \\[ P_X(x) = F_X(x) - F_X(x^{-}).\\] By construction, \\(P_X(x) \\ge 0\\) and \\(\\sum_{x \\in \\ALPHABET X} P_X(x) = 1\\).\nThen, for any event \\(A \\in \\mathscr{B}(\\reals)\\), \\[\\PR(X \\in A) = \\sum_{x \\in \\ALPHABET X \\cap A} P_X(x).\\]\nProperties of continuous random variables\nFor a discrete random variable \\(X\\), define the probability density function (PDF) \\(f_X \\colon \\reals \\to \\reals\\) as \\[\n   f_X(x) = \\frac{d}{dx} F_X(x).\n\\] By construction, \\(f_X(x) \\ge 0\\) and \\(\\int_{-∞}^{∞} f_X(x)\\, dx = 1\\).\nThen, for any event \\(A \\in \\mathscr{B}(\\reals)\\), \\[\\PR(X \\in A) = \\int_{x \\in A} f_X(x)\\,dx.\\]\n\n\n\n2.2.1 Examples of discrete random variables\nWe now consider some other examples of discrete random variables, which are typically described by specifying their PMF.\n\nExample 2.6 (Binomial random variable) A Binomial random variable is the sum of intendant and identically Bernoulli random variables (we will prove this fact later). For example, if a biased coin (with \\(\\PR(H) = p\\)) is tossed \\(n\\) times, then the number of heads is a binomial random variable with parameters \\(n\\) and \\(p\\), which is denoted by \\(\\text{Binomial}(n,p)\\). For such a random variable, \\[\nP_X(k) = \\binom n k p^k (1-p)^{n-k}, \\quad 0 \\le k \\le n.\n\\]\n\n\nExample 2.7 (Geometric random variable) A geometric random variable is the number of trails in i.i.d. Bernoulli random variables. For example, if a biased coin (with \\(\\PR(H) = p\\)) is tossed repeated, the number of tosses needed for the first head is a geometric random variable with parameter \\(p \\in (0,1)\\)_, which is denoted by \\(\\text{Geo}(p)\\). For such a random variable, \\[\nP_X(k) = (1-p)^{k-1} p, \\quad k \\in \\integers_{&gt; 0}.\n\\]\n\nA geometric random variables have the property of being memoryless: the distribution of waiting time does not depend on how much time has already elapsed.\n\nExample 2.8 (Poisson random variable) Poisson random variables model many different phenomenon ranging from photoelectric effect in photonics to inter-packet arrival times in computer networks. A random variable is said to Poisson random variable with parameter \\(λ &gt; 0\\), which is denoted by \\(\\text{Poisson}(λ)\\), if \\[\nP_X(k) = \\frac{λ^k}{k!} e^{-λ}, \\quad k \\in \\integers_{\\ge 0}.\n\\]\n\nPoisson random variables model rare events. It describes the limit of a Binomial random variable with success probability \\(p = λ/n\\) as the number \\(n\\) of trials increases. Poisson random variables have the stability property that sum of Poisson random variables is Poisson\n\nExample 2.9 (Uniform random variable) A random variable is said to have a (discrete) uniform distribution over a discrete set \\(\\ALPHABET S\\) if \\[P_X(k) = \\frac 1{\\ABS{\\ALPHABET S}}, \\quad k \\in \\ALPHABET S.\\]\n\n\n\n2.2.2 Some examples of continuous random variables\n\nExample 2.10 (Uniform random variable) A random variable is said to have a (continuous) uniform distribution over an interval \\([a, b]\\), where \\(a &lt; b\\) if \\[f_X(x) = \\frac 1{b - a}, \\quad x \\in [a,b].\\]\n\n\nExample 2.11 (Exponential random variable) A random variable is said to have an exponential distribution with parameter \\(λ &gt; 0\\), which is denoted by \\(\\text{exp}(λ)\\) if \\[f_X(x) = λ e^{-λ x}, \\quad x \\ge 0.\\]\n\nExponential random variables arise in queueing theory, network traffic, and photonics. They have the property of being memoryless: the distribution of the waiting time does not depend on how much time has already elapsed.\n\nExample 2.12 (Gaussian random variable) A random variable is said to have a Gaussian distribution with mean \\(μ\\) and standard deviation \\(σ &gt; 0\\), which is denoted by \\(\\mathcal N(μ, σ^2)\\) if \\[f_X(x) = \\frac 1{\\sqrt{2 π}\\, σ}\n\\exp\\left( -\\frac {(x-μ)^2}{2 σ^2} \\right),\n\\quad x \\in \\reals.\\]\nA Gaussian distribution is also called a Normal distribution. When \\(μ = 0\\) and \\(σ^2 = 1\\), the distribution is called standard Normal.\n\nThe Gaussian distribution is perhaps the most important continuous distribution because of its role in the Central Limit Theorem. Gaussian random variables have the stability property that sum of Gaussian random variables is Gaussian.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#functions-of-random-variables",
    "href": "random-variables.html#functions-of-random-variables",
    "title": "2  Random variables",
    "section": "2.3 Functions of random variables",
    "text": "2.3 Functions of random variables\nWe often encounter situations where we are interested in functions of random variables. Functions of random variables are random variables.\nIn particular, suppose \\[\n  X \\colon (\\ALPHABET Ω, \\ALPHABET F) \\to (\\reals, \\mathscr{B}(\\reals))\n\\] is a random variable and \\(g \\colon \\reals \\to \\reals\\) is a measurable function.\nSince \\(g\\) is measurable, for any (Borel) subset \\(B\\) of \\(\\reals\\), we have that \\(C = g^{-1}(B) \\in \\mathscr B(\\reals)\\). Therefore, \\(X^{-1}(C) \\in \\ALPHABET F\\). Thus, we can think of \\(Y\\) as a random variable.\nSince \\(Y\\) is a random variable, it is possible to compute its CDF and PMF/PDF as appropriate. We discuss the details separately for discrete and continuous random variables.\n\n2.3.1 Functions of discrete random variables\nIn the discrete time, it is trivial to find the PMF of \\(Y\\) in terms of PMF of \\(X\\). For example, consider the random variable \\(X\\) defined in Example 2.1. Let \\(g \\colon \\{0, 1, 2\\} \\to \\{0, 1\\}\\) be given by \\[\n  g(0) = 0, \\quad g(1) = 0, \\quad g(2) = 1.\n\\] Then, \\[\n  P_Y(0) = P_X(0) + P_X(1)\n  \\quad\\text{and}\\quad\n  P_Y(1) = P_X(2).\n\\]\nHowever, it will be useful to visualize this slightly differently. We start with revisiting measurability for discrete random variables. The main point is that\n\nA discrete random variable creates a partition on the sample space (we expand on this point below).\nThe power-set of \\(\\{A_1, A_2, \\dots\\}\\) is called the \\(σ\\)-algebra generated by \\(X\\) and denoted by \\(σ(X)\\). This \\(σ\\)-algebra captures the crux of measurability.\n\nWe now discuss the partition generated by a discrete random variable. Let \\(X\\) be a random variable and \\(\\ALPHABET X = \\{x_1, x_2, \\dots, x_n\\}\\) be the range of \\(X\\). Define \\[A_i = \\{ω \\in Ω : X(ω) = x_i \\} = X^{-1}(x_i).\\] Then, \\(X\\) can be written as \\[\n  X(ω) = \\sum_{i=1}^{n} x_i \\IND_{A_i}(ω).\n\\]\nNote that \\(\\{A_1, \\dots, A_n \\}\\) are disjoint events and their union is the entire sample space \\(Ω\\) (because one of \\(x_i\\)’s must occur). Thus, \\(\\{A_1, \\dots, A_n \\}\\) is a partition of \\(Ω\\).\nAs an illustration, let’s reconsider Example 2.1. In this case, the range of \\(X\\) is \\(\\ALPHABET X = \\{0, 1, 2\\}\\). The partition corresponding to \\(X\\) is shown in Figure 2.8.\n\n\n\n\n\n\nFigure 2.8: Illustration of the partition created by \\(X\\) for Example 2.1\n\n\n\nThe partition corresponding to \\(Y = g(X)\\) is shown in Figure 2.9. Observe that this partition is a coarsening of the partition corresponding to \\(X\\).\n\n\n\n\n\n\nFigure 2.9: Illustration of the partition created by $ = g(X)$ for Example 2.1\n\n\n\nThe PMFs of \\(X\\) and \\(Y\\) can be obtained by summing up the masses in each element of their respective partitions.\n\n\n2.3.2 Functions of continuous random variables\nFor continuous random variables, computing the PDF of a function of a random variable is more involved. We first illustrate the main idea via some examples.\n\nExample 2.13 Suppose \\(X \\sim \\text{Uniform}(0,2)\\). Consider a function \\(g\\) given by \\[\ng(x) = \\begin{cases}\n  x & x \\in (0,1] \\\\\n  2 - x & x \\in (1,2] \\\\\n  0 & \\hbox{otherwise}\n\\end{cases}\n\\] Define \\(Y = g(X)\\). Find \\(F_Y(y)\\) and \\(f_Y(y)\\).\n\ng_exm_1 = [\n  {x: -1, y: 0},\n  {x: 0, y: 0},\n  {x: 1, y: 1},\n  {x: 2, y: 0},\n  {x: 3, y: 0}\n];\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  width: 600,\n  height: 200,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(g_exm_1, {x: \"x\", y: \"y\", stroke: \"blue\"}),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 2.10: The function \\(g(x)\\) for Example 2.13\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFrom the definition of \\(g\\), we know that the rannge of \\(g\\) is \\([0,1]\\). Thus, we know that the support of \\(Y\\) is \\([0,1]\\).\n\nFor any \\(y &lt; 0\\), the event \\(\\{Y \\le y\\} = \\emptyset\\). Therefore, \\(F_Y(y) = 0\\).\nFor any \\(y &gt; 1\\), the event \\(\\{Y \\le y\\} = Ω\\). Therefore, \\(F_Y(y) = 1\\).\nNow consider a \\(y \\in (0,1)\\). We have \\[\n\\{Y \\le y \\} = \\{ X \\le y \\} \\cup \\{X \\ge 2 - y \\}.\n\\] Thus, \\[\nF_Y(y) = F_X(y) + F_X(2-y) = \\frac {y}{2} + 1 - \\frac{2-y}{2} = y.\n\\] Thus, \\[\n   f_Y(y) = \\dfrac{d}{dy} F_Y(y) = 1, \\quad y \\in [0,1].\n\\] Thus, \\(Y\\) is \\(\\text{Uniform}(0,1)\\).\n\n\n\n\n\nExample 2.14 Suppose \\(X \\sim \\text{Uniform}(0,4)\\). Consider a function \\(g\\) given by \\[\ng(x) = \\begin{cases}\n  x & x \\in (0,1] \\\\\n  1 & x \\in (1, 3) \\\\\n  4- x & x \\in (3,4] \\\\\n  0 & \\hbox{otherwise}\n\\end{cases}\n\\] Define \\(Y = g(X)\\). Find \\(F_Y(y)\\) and \\(f_Y(y)\\).\n\ng_exm_2 = [\n  {x: -1, y: 0},\n  {x: 0, y: 0},\n  {x: 1, y: 1},\n  {x: 3, y: 1},\n  {x: 4, y: 0},\n  {x: 5, y: 0}\n];\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  width: 600,\n  height: 200,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(g_exm_2, {x: \"x\", y: \"y\", stroke: \"blue\"}),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 2.11: The function \\(g(x)\\) for Example 2.14\n\n\n\n\n\n\nExample 2.15 Suppose \\(X \\sim \\mathcal{N}(μ,σ^2)\\). Show that \\(Z = (X - μ)/σ\\) is a standard normal random variable, i.e., \\(Z \\sim \\mathcal{N}(0,1)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe can write the CDF \\(F_Z(z)\\) as \\[\\begin{align*}\nF_Z(z) &= \\PR(Z \\le z) = \\PR\\left( \\frac{X - μ}{σ} \\le z \\right)\n= \\PR(X \\le σ z + μ)\n\\\\\n&= \\int_{-∞}^{σ z + μ} \\frac{1}{\\sqrt{2 π}\\, σ} \\exp\\left(\n- \\frac{(x-μ)^2}{2 σ^2}\\, dx \\right)\n\\\\\n&= \\int_{-∞}^{z} \\frac{1}{\\sqrt{2 π}} \\exp\\left(\n- \\frac{y^2}{2}\\, dy \\right)\n\\end{align*}\\] where the last step uses the change of variables \\(y = (x-μ)/σ\\).\nThus, \\[f_Z(z) = \\frac{d F_Z(z)}{dz} = \\frac{1}{\\sqrt{2 π}} e^{-z^2/2}.\\] Thus, \\(Z \\sim \\mathcal{N}(0,1)\\).\n\n\n\n\nExample 2.16 (Generating non-uniform random variables by inverting the CDF) Suppose \\(F \\colon \\reals \\to [0,1]\\) is a function that satisfies the following properties: there exist a pair \\((a,b)\\) with \\(a &lt; b\\) (we allow \\(a\\) to be \\(-∞\\) and \\(b\\) to be \\(∞\\)) such that\n\n\\(F(x) = 0\\) for \\(x \\le a\\)\n\\(F(x) = 1\\) for \\(x \\ge b\\)\n\\(F(x)\\) is strictly increasing in \\((a,b)\\).\n\nThus, \\(F\\) satisfies the properties of the CDF of a continuous random variable and \\(F\\) is invertible in the interval \\((a,b)\\).\nSuppose \\(U \\sim \\text{Uniform}(0,1)\\) and \\(X = F^{-1}(U)\\). Show that \\(F_X(x) = F(x)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe can write the CDF \\(F_X(x)\\) as \\[\nF_X(x) = \\PR(X \\le x) = \\PR(F^{-1}(U) \\le x)\n\\]\nSince \\(F\\) is strictly increasing, \\(F^{-1}(U) \\le x\\) is equivalent to \\(U \\le F(x)\\). Thus, \\[\nF_X(x) = \\PR(U \\le F(x)) = F_U(F(x)) = F(x)\n\\] where the last step uses the fact that \\(U\\) is uniform over \\((0,1)\\).\n\n\n\n\n\n2.3.3 Change of variables formula\nIt is possible to obtain a general change of variables formula to obtain \\(f_Y\\).\n\nSuppose \\(g\\) is a continuous and one-to-one function (from \\(\\text{Range}(X)\\) to \\(\\text{Range}{Y}\\)). Thus, \\(g\\) must be either strictly increasing or strictly decreasing, and in both cases the inverse \\(g^{-}\\) is well defined.\n\nIf \\(g^{-1}\\) is strictly increasing, we have \\[ F_Y(y) = \\PR(Y \\le y) = \\PR(X \\le g^{-1}(y) = F_X(g^{-1}(y))\\] Therefore, \\[ f_Y(y) = \\frac{d F_X(g^{-1}(y))}{dy} = f_X(g^{-1}(y)) \\frac{d g^{-1}(y)}{dy}. \\]\nIf \\(g^{-1}\\) is strictly decreasing, we have \\[ F_Y(y) = \\PR(Y \\le y) = \\PR(X \\ge g^{-1}(y)) = 1 - F_X(g^{-1}(y)).\\] Therefore, \\[ f_Y(y) = - \\frac{d F_X(g^{-1}(y))}{dy} = - f_X(g^{-1}(y)) \\frac{d g^{-1}(y)}{dy}. \\]\nThe above two formulas can be combined as \\[ \\bbox[5pt,border: 1px solid]{f_Y(y) = f_X(g^{-1}(y)) \\left| \\frac{d g^{-1}(y)}{dy} \\right|} \\]\n\nFrom calculus, we know that if \\(h(y) = g^{-1}(y)\\), then \\(h'(y) = 1/g'(h(y))\\). Thus, the above expression can be simplified as \\[ \\bbox[5pt, border: 1px solid]{f_Y(y) = \\frac{f_X(x)}{\\ABS{g'(x)}}, \\quad \\text{where } x = g^{-1}(y).} \\]\n\nExample 2.17 Resolve Example 2.15 using the above formula.\n\nIf the transform \\(g(x)\\) is not one-to-one (as in Example 2.13), we can obtain \\(f_Y(y)\\) as follows. Suppose \\(y = g(x)\\) has finite roots, denoted by \\(\\{x^{(k)}\\}_{k=1}^m\\). Then, \\[ f_Y(y) = \\sum_{k=1}^m \\frac{f_X(x^{(k)})}{\\ABS{g'(x^{(k)})}}. \\]\n\nExample 2.18 Resolve Example 2.13 using the above formula.\n\nThe change of variables formula can be used to verify some common ways of generating non-uniform random variables from uniform random variables. For example:\n\nSuppose \\(U \\sim \\text{Uniform}(0,1)\\) and \\(λ &gt; 0\\). Then, \\(X = - \\ln(1 - U)/λ\\) has \\(\\text{exp}(λ)\\) distribution.\n\nNote that the change of variable formula only works for continuous function. For non-continuous functions, we need to use first principles (or the idea in Example 2.16). For example:\n\nSuppose \\(U \\sim \\text{Uniform}(0,1)\\) and \\(p \\in (0,1)\\). Then \\(X = \\IND_{\\{U \\le p\\}}\\) has \\(\\text{Bernoulli}(p)\\) distribution.\n\nSee Devroye (1986) for a general discussion on generating non-uniform random variables,",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#expectation-of-random-variables",
    "href": "random-variables.html#expectation-of-random-variables",
    "title": "2  Random variables",
    "section": "2.4 Expectation of random variables",
    "text": "2.4 Expectation of random variables\nSuppose we generate \\(N\\) i.i.d. (independent and identically distributed) samples \\(\\{s_1, s_2, \\dots, s_N\\}\\) of a random variable \\(X\\) and compute the average: \\[ m = \\frac 1N \\sum_{n=1}^N s_n. \\] When \\(X\\) is discrete and takes values \\(\\{x_1, x_2, \\dots, x_n\\}\\), we expect that the number of times we obtain a value \\(x_i\\) is approximately \\(NP_X(x_i)\\) when \\(N\\) is large. Thus, \\[ m \\approx \\frac 1N \\sum_{i=1}^n x_i \\, N P_X(x_i)  = \\sum_{i=1}^n x_i P_X(x_i). \\]\nThis quantity is called the expectation or the expected value or the mean value of the random variable \\(X\\) and denoted by \\(\\EXP[X]\\).\n\nDefinition 2.2 The expectation of a random variable \\(X\\) is defined as follows:\n\nwhen \\(X\\) is discrete and takes values \\(\\{x_1, x_2, \\dots, x_n \\}\\), then \\[\\EXP[X] = \\sum_{i=1}^n x_i P_X(x_i).\\]\nwhen \\(X\\) is continuous, then \\[\\EXP[X] = \\int_{-∞}^{∞} x f_X(x)\\, dx. \\]\nThus, we can think of the expected value as the center of mass of the PDF.\n\n\n\n\n\n\n\n\nWarningDoes the summation or integration exist?\n\n\n\nWhen \\(X\\) takes countably or uncountably infinite values, we need to be a bit more precise by what we mean by the summation (or the integration) formula above. In particular, we do not want the answer to depend on the order in which we do the summation or the integration (i.e., we do not want \\(∞ - ∞\\) situation). This means that the sum or the integral should be :absolutely convergent. Such random variables are called integrable random variables.\nFormally, expectation is defined only for integrable random variables.\nTo illustrate why this is important, consider a discrete random variable defined over \\(\\integers\\setminus\\{0\\}\\) where \\[\nP_X(n) = P_X(-n) = \\frac {1}{2C n^2}, \\quad n \\in \\naturalnumbers\n\\] where \\(C\\) is a normalizing constant given by \\[\nC = \\sum_{n=1}^∞ \\frac 1{n^2} = \\frac{π^2}{6}.\n\\] Then, observe that \\[\\begin{align*}\n\\EXP[X] &= \\sum_{n=1}^∞ \\frac{n}{2 C n^2}\n+ \\sum_{n=-∞}^{-1} \\frac{n}{2 C n^2} \\\\\n&= \\frac 1{2C} \\sum_{n=1}^∞ \\frac{1}{n}\n+ \\frac 1{2C} \\sum_{n=-∞}^{-1} \\frac{1}{n} \\\\\n&= \\frac{∞}{2C} - \\frac{∞}{2C}\n\\end{align*}\\] which is undefined.\nThe concern here is that the summation is undefined. Mathematically, we are okay when the summation is infinity. For example, consider another random variable \\(Y\\) defined over \\(\\naturalnumbers\\) for which \\[\nP_X(n) = \\frac {1}{C n^2}, \\quad n \\in \\naturalnumbers\n\\] where \\(C\\) is as defined above. This is called the Zipf distribution. By following an argument same as above, we see that \\[\\EXP[Y] = ∞.\\]\n\n\n\nExample 2.19 Find the mean of \\(X\\) defined in Example 2.1.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\[\n\\EXP[X] = \\frac 14 \\cdot 0 + \\frac 12 \\cdot 1 + \\frac 14 \\cdot 2 = 1.\n\\]\n\n\n\n\nExample 2.20 Find the expected value of the random variables with the following distributions:\n\n\\(\\text{Bernoulli}(p)\\).\n\\(\\text{Binomial}(n,p)\\).\n\\(\\text{Geo}(p)\\).\n\\(\\text{Poisson}(λ)\\).\n\\(\\text{Uniform}(a,b)\\).\n\\(\\text{Exp}(λ)\\).\n\n\n\nLemma 2.3 For any (measurable) function \\(g \\colon \\reals \\to \\reals\\), we have\n\nwhen \\(X\\) is discrete and takes values \\(\\{x_1, x_2, \\dots, x_n \\}\\), then \\[\\EXP[g(X)] = \\sum_{i=1}^n g(x_i) P_X(x_i).\\]\nwhen \\(X\\) is continuous, then \\[\\EXP[g(X)] = \\int_{-∞}^{∞} g(x) f_X(x)\\, dx. \\]\n\nBoth expressions are defined only when the sum/integral is absolutely convergent.\n\n\n\n\n\n\n\nNoteHow to avoid a proof\n\n\n\n\n\nThis result is sometimes called *the law of the unconscious statistician (LOTUS). One typically shows this result by defining a new random variable \\(Y = g(X)\\), computing its PMF/PDF \\(f_Y\\) and then using the definition in Definition 2.2.\nA simpler proof is to define expectation by Lemma 2.3 for any (measurable) function \\(g\\). Then the definition of Definition 2.2 is a special case for \\(g(x) = x\\). No proofs needed!\n\n\n\n\nExample 2.21 Suppose \\(X \\sim \\text{Unif}[-1,1]\\). Compute \\(\\EXP[X^2]\\).\n\n\nLemma 2.4 (Properties of expectation)  \n\nLinearity. For any (measurable) functions \\(g\\) and \\(h\\) \\[\\EXP[g(X) + h(X)] = \\EXP[ g(X)] + \\EXP[ h(X) ]. \\] As a special case, for a constant \\(c\\), \\[\\EXP[X + c] = \\EXP[X] + c.\\]\nScaling. For any constant \\(c\\), \\[\\EXP[cX] = c\\EXP[X].\\]\nBounds. If \\(a \\le X(ω) \\le b\\) for all \\(ω \\in Ω\\), then \\[ a \\le \\EXP[X] \\le b. \\]\nIndicator of events. For any (Borel) subset \\(B\\) of \\(\\reals\\), we have \\[\\EXP[ \\IND_{\\{ X \\in B \\}}] = \\PR(X \\in B). \\]\n\n\n\nA continuous random variable is said to be symmetric if \\(f_X(-x) = f_X(x)\\) for all \\(x \\in \\reals\\). A symmetric random variable has mean \\(0\\).\nA continuous random variable is said to be symmetric around \\(m\\) if \\(f(m - x) = f(m + x)\\), for all \\(x \\in \\reals\\). The mean of such a random variable is \\(m\\).\n\n\n2.4.1 Higher moments\n\nThe \\(m\\)-th moment, \\(m \\ge 1\\) of a random variable \\(X\\) is defined as \\(\\EXP[X^m]\\).\nThe \\(m\\)-th central moment is defined as \\(\\EXP[(X - μ)^m]\\), where \\(μ = \\EXP[X]\\).\nFor second central moment (i.e., \\(m=2\\)) is called variance. The variance satisfies the following: \\[\\VAR(X) = \\EXP[X^2] - (\\EXP[X])^2.\\]\nThe positive square root of variance is called the standard deviation. Variance is often denoted by \\(σ^2\\) and the standard deviation by \\(σ\\).\n\n\nExample 2.22 Find the variance of \\(X\\) defined in Example 2.1.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe first compute \\[\n\\EXP[X^2] = \\frac 14 \\cdot 0^2 + \\frac 12 \\cdot 1^2 + \\frac 14 \\cdot 2^2 = \\frac 32.\n\\] Therefore, \\[\n\\VAR(X) = \\EXP[X^2] - \\EXP[X]^2 = \\frac 32 - 1 = \\frac 12.\n\\]\n\n\n\n\nLemma 2.5 (Properties of variance)  \n\nScaling. For any constant \\(c\\), \\[\\VAR(cX) = c^2 \\VAR(X).\\]\nShift invariance. For any constant \\(c\\), \\[\\VAR(X + c) = \\VAR(X).\\]\n\n\nThe mean and variance of common random variables is show in Table 2.1\n\n\n\nTable 2.1: Mean and variance of common random variables\n\n\n\n\n\n\n\n\n\n\n\nRandom variable\nParameter(s)\nMean\nVariance\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p\\)\n\\(p(1-p)\\)\n\n\nBinomial\n\\((n,p)\\)\n\\(np\\)\n\\(np(1-p)\\)\n\n\nGeometric\n\\(p\\)\n\\(\\dfrac 1p\\)\n\\(\\dfrac{1-p}{p}\\)\n\n\nPoisson\n\\(λ\\)\n\\(λ\\)\n\\(λ\\)\n\n\nUniform\n\\((a,b)\\)\n\\(\\frac 12 (a+b)\\)\n\\(\\frac 1{12}(b-a)^2\\)\n\n\nExponential\n\\(λ\\)\n\\(\\dfrac 1 λ\\)\n\\(\\dfrac 1{λ^2}\\)\n\n\nGaussian\n\\((μ,σ)\\)\n\\(μ\\)\n\\(σ^2\\)\n\n\n\n\n\n\n\nExample 2.23 Let \\(X\\) be a real-valued random variable where \\(μ = \\EXP[X]\\). Let \\(t\\) be any real number. Then show that \\[\n  \\EXP[(X-t)^2] = \\VAR(X) + (t-μ)^2.\n\\] Argue that the above equation implies that \\[\n  \\VAR(X) = \\min_{t \\in \\reals} \\EXP[(X -t)^2].\n\\]\nRemark: The above relationship suggests a single-pass algorithm to compute \\(\\VAR(X)\\) from a set of samples: choose any \\(t\\) and compute \\(μ = \\EXP[X]\\) and \\(\\EXP[(X-t)^2]\\) in a single pass. Then, \\(\\VAR(X)\\) can be computed bs \\(\\VAR(X) = \\EXP[(X-t)^2] + (t-μ)^2\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe know that \\[\\begin{align*}\n  \\EXP[(X-t)^2] &= \\EXP[\\bigl((X-μ) + (μ-t) \\bigr)^2]\n   \\\\\n   &= \\EXP[(X-μ)^2 + (μ-t)^2  + 2 (X - μ)(μ-t) ]\n   \\\\\n   &\\stackrel{(a)}= \\EXP[(X-μ)^2] + (t-μ)^2 + 2(μ-t)\\EXP[(X-μ)]\n   \\\\\n    &\\stackrel{(b)}= \\VAR(X) + (t-μ)^2\n\\end{align*}\\] where \\((a)\\) uses the linearity property of expectations and \\((b)\\) uses the fact that \\(\\EXP[X-μ] = μ -μ = 0\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables</span>"
    ]
  },
  {
    "objectID": "random-vectors.html",
    "href": "random-vectors.html",
    "title": "3  Random vectors",
    "section": "",
    "text": "3.1 Classification of random vectors\nSuppose \\(X\\) and \\(Y\\) are two random variables defined on the same probability space. The CDFs \\(F_X\\) and \\(F_Y\\) provide information about their individual probabilities. However, we are often interested in functions of the two random variables, e.g., \\[\n  X + Y, \\quad XY, \\quad \\max(X,Y), \\quad \\min(X,Y), \\quad \\text{etc.}\n\\] To understand how they behave together, we need to think of the random vector \\((X,Y)\\) taking values in \\(\\reals^2\\). The natural way to do so is to think of the joint CDF \\[\nF_{X,Y}(x,y) = \\PR(\\{ ω \\in Ω : X(ω) \\le x, Y(ω) \\le y \\})\n\\] where we may write the right hand side as \\(\\PR(X \\le x, Y \\le y)\\) for short.\nAs was the case for random variables, we can also classify random vectors as discrete, continuous, and mixed.\nThe above discussion generalizes in the obvious manner to more than two random variables. Thus, we can talk about random vectors \\(X = (X_1, \\dots, X_n) \\in \\reals^n\\). In practice, we often do not make a distinction between random variables and random vectors and refer both of them simply as random variables.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#classification-of-random-vectors",
    "href": "random-vectors.html#classification-of-random-vectors",
    "title": "3  Random vectors",
    "section": "",
    "text": "A random vector \\((X,Y)\\) is called jointly discrete if it takes values in a countable subset of \\(\\reals^2\\) (we denote this subset by \\(\\ALPHABET X \\times \\ALPHABET Y\\)). The jointly discrete random variables have a joint PMF \\(P_{X,Y} \\colon \\reals^2 \\to [0,1]\\) given by \\[ \\PR(X = x, Y = y) = P_{X,Y}(x,y). \\]\nA random vector \\((X, Y)\\) is called jointly continuous (or absolutely continuous) if there exists an integrable function \\(f_{X,Y} \\colon \\reals^2 \\to [0, ∞)\\) such that \\[ F_{X,Y}(x,y) = \\int_{-∞}^x \\int_{-∞}^{y} f_{X,Y}(u,v)\\, du dv, \\quad\nx,y \\in \\reals \\] \\(f_{X,Y}\\) is called the joint PDF and can be computed as \\[\n   f_{X,Y}(x,y) = \\frac{∂^2}{∂x ∂y}F_{X,Y}(x,y).\n\\]\n\n\n\n\n\n\n\nTipCan we always define a density if CDF is continuous?\n\n\n\nContinuity of the CDF is not sufficient for a joint density to exist. Consider the joint CDF \\[\n  F_{X,Y}(x,y) = \\begin{cases}\n    0, & \\min(x,y) &lt; 0 \\\\\n    \\min(x,y), & 0 \\le \\min(x,y) \\le 1 \\\\\n    1, & \\min(x,y) &gt; 1\n  \\end{cases}\n\\] This is a valid CDF. It corresponds to \\[\n  (X,Y) = (U,U), \\quad \\text{where } U \\sim \\text{Unif}[0,1],\n\\] i.e., the distribution is uniform along the diagonal of the unit square.\nThe diagonal is a set a Lebesgue measure zero. That means for any ordinary function \\(f_{X,Y}\\) \\[\n  \\iint_{A} f_{X,Y}(x,y)\\, dx\\,dy = 0\n\\] for all subsets \\(A\\) of the diagonal. Outside the diagonal, the density must be zero. Hence, the density cannot integrate to one over the entire plane. Therefore, mathematically, a density does not exist. Such random variables are said to be singular.\nSometimes in the engineering literature, we sometimes set the PDF to be \\[\n    f_{X,Y}(x,y) = δ(x-y), \\quad 0 \\le x,y \\le 1\n\\] which produces the correct \\(F_{X,Y}\\) but delta functions are generalized functions. Thus, although the CDF is continuous, it does not have a joint density.\n\n\n\nLemma 3.2 (Properties of PMFs and PDFs)  \n\nProperties of PMFs\n\nNormalization. For a jointly discrete random vector \\((X,Y)\\), \\[\\sum_{x,y \\in \\ALPHABET X × \\ALPHABET Y}P_{X,Y}(x,y) = 1.\\]\nFor any event \\(A \\in \\ALPHABET F\\), \\[\\PR((X,Y) \\in A) = \\sum_{(x,y) \\in (\\ALPHABET X \\times \\ALPHABET Y) \\cap A} P_{X,Y}(x,y).\\]\nMarginalization.\n\n\\(\\displaystyle \\sum_{x \\in \\ALPHABET X} P_{X,Y}(x,y) = P_Y(y)\\).\n\\(\\displaystyle \\sum_{y \\in \\ALPHABET Y} P_{X,Y}(x,y) = P_X(x)\\).\n\n\nProperties of PDFs\n\nNormalization. For a jointly continuous random vector \\((X,Y)\\), \\[\\int_{-∞}^{∞} \\int_{-∞}^{∞} f_{X,Y}(x,y)\\, dxdy = 1.\\]\nFor any event \\(A \\in \\ALPHABET F\\), \\[\\PR((X,Y) \\in A) = \\iint_{(x,y) \\in A} f_{X,Y}(x,y)\\,dxdy.\\]\nMarginalization.\n\n\\(\\displaystyle \\int_{-∞}^{∞} f_{X,Y}(x,y) dx = f_Y(y)\\).\n\\(\\displaystyle \\int_{-∞}^{∞} f_{X,Y}(x,y) dy = f_X(x)\\).\n\n\n\n\n\n\nExample 3.3 Consider jointly discrete random variables \\(X \\in \\{1,2,3\\}\\) and \\(Y \\in \\{1, 2, 3\\}\\) with joint PMF \\[\n  P_{X,Y} = \\MATRIX{\n    0.1 & 0.1 & 0.2 \\\\\n    0.2 & 0.1 & 0   \\\\\n    0.3 & 0   & 0   \n  }\n\\]\n\nFind the marginals \\(P_X\\) and \\(P_Y\\).\nFind the probability of the event \\(A = \\{ X + Y = 3 \\}\\).\n\n\n\nExample 3.4 Consider \\(F_{X,Y}\\) given in Example 3.2.\n\nFind the joint density \\(f_{X,Y}\\)\nFind the marginal densities \\(f_X\\) and \\(f_Y\\).\nFind the probability of the event \\(A = \\{ X + Y \\le 1 \\}\\).\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFor \\(x,y &gt; 0\\), we have \\[\n  f_{X,Y}(x,y) = \\frac{∂^2}{∂x ∂y}F_{X,Y}(x,y) = x e^{-x(y+1)}.\n\\] Thus, \\[\n  f_{X,Y}(x,y) = \\begin{cases}\n    x e^{-x(y+1)}, & x,y &gt; 0 \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\]\nThus, \\[\n  f_X(x) = \\int_{-∞}^{∞} f_{X,Y}(x,y)\\,dy =\n  \\begin{cases}\n    e^{-x}, & x &gt; 0 \\\\\n    0, & x \\le 0\n  \\end{cases}\n\\] and \\[\n  f_Y(y) = \\int_{-∞}^{∞} f_{X,Y}(x,y)\\,dx =\n  \\begin{cases}\n    \\dfrac{1}{(1+y)^2}, & y &gt; 0 \\\\\n    0, & y \\le 0\n  \\end{cases}\n\\]\n\n\n\n\nExample 3.5 Consider a joint PDF \\[\n  f_{X,Y}(x,y) = c x y, \\quad 0 \\le y \\le x \\le 1.\n\\] Find the constant \\(c\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe know that the joint PDF must integrate to \\(1\\). Thus, \\[\\begin{align*}\n  1 &= \\int_{-∞}^{∞} f_{X,Y}(x,y)\\, dx\\, dy \\\\\n  &= \\int_{0}^{1} \\int_{0}^x c xy\\, dy\\, dx  \\\\\n  &= \\int_{0}^{1} c x\\frac{x^2}{2} dx \\\\\n  &= c \\frac{x^3}{8}\\biggr|_{0}^1 = \\frac{c}{8}\n\\end{align*}\\] Therefore, \\(c=8\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#independence-of-random-vectors",
    "href": "random-vectors.html#independence-of-random-vectors",
    "title": "3  Random vectors",
    "section": "3.2 Independence of random vectors",
    "text": "3.2 Independence of random vectors\n\nDefinition 3.1 Two random variables \\(X\\) and \\(Y\\) defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\) are said to be independent if the sigma algebras \\(σ(X)\\) and \\(σ(Y)\\) are independent.\n\nThe above definition means that if we take any (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\), then the events \\(\\{X \\in B_1\\}\\) and \\(\\{X \\in B_2\\}\\) are independent, i.e., \\[\n\\PR(X \\in B_1, Y \\in B_2) = \\PR(X \\in B_1) \\PR(Y \\in B_2).\n\\]\nUsing this, we can show that following:\n\n\\(X\\) and \\(Y\\) are independent if and only if \\[\n   F_{X,Y}(x,y) = F_X(x) F_Y(y), \\quad \\forall x, y \\in \\reals.\n\\]\nTwo jointly continuous random variables \\(X\\) and \\(Y\\) are independent if and only if \\[\n   f_{X,Y}(x,y) = f_X(x) f_Y(y), \\quad \\forall x, y \\in \\reals.\n\\]\nTwo jointly discrete random variables \\(X\\) and \\(Y\\) are independent if and only if \\[\n   P_{X,Y}(x,y) = P_X(x) P_Y(y), \\quad \\forall x, y \\in \\reals.\n\\]\n\n\nExample 3.6 Consider the random variables \\(X\\) and \\(Y\\) with the joint PMF \\(P_{X,Y}\\) given in Example 3.3. Are these random variables independent?\n\n\nExample 3.7 Consider the random variables \\(X\\) and \\(Y\\) with the joint CDF \\(F_{X,Y}\\) given in Example 3.2. Are these random variables independent?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nObserve that \\(F_{X,Y}(x,y) \\neq F_X(x) F_Y(y)\\). Hence, the two random variables are not independent.\nWe can also see that because \\(f_{X,Y}(x,y) \\neq f_X(x) f_Y(y)\\).\n\n\n\n\nExample 3.8 Consider random variables \\(X\\) and \\(Y\\) with joint PDF \\(f_{X,Y}\\) which is a uniform distribution on the unit square. Are \\(X\\) and \\(Y\\) independent?\n\n\nExample 3.9 Consider random variables \\(X\\) and \\(Y\\) with joint PDF \\(f_{X,Y}\\) which is a uniform distribution on the unit triangle Are \\(X\\) and \\(Y\\) independent?\n\nThese definitions extend naturally to any number of random variables:\n\nA sequence of random variables \\(X_1, \\dots, X_n\\) are independent if and only if \\[\n  F_{X_1, \\dots, X_n}(x_1, \\dots, x_n) = \\prod_{i=1}^n F_{X_i}(x_i).\n\\]\nA sequence of jointly continuous random variables \\(X_1, \\dots, X_n\\) are independent if and only if \\[\n  f_{X_1, \\dots, X_n}(x_1, \\dots, x_n) = \\prod_{i=1}^n f_{X_i}(x_i).\n\\]\nA sequence of jointly discrete random variables \\(X_1, \\dots, X_n\\) are independent if and only if \\[\n  P_{X_1, \\dots, X_n}(x_1, \\dots, x_n) = \\prod_{i=1}^n P_{X_i}(x_i).\n\\]\n\nUnlike independence of events, we do not need to separately check for independence of subsets of random variables because that is automatically implied due to the marginalization property.\n\nExample 3.10 Suppose (X, Y, Z) are independent with joint PDF: \\[\n  f_{X,Y,Z}(x,y,z) = f_X(x)\\, f_Y(y)\\, f_Z(z).\n\\]\nShow that \\(X\\) and \\(Y\\) are independent.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTo find the joint PDF of \\(X\\) and \\(Y\\), marginalize over \\(Z\\): \\[\nf_{X,Y}(x,y) = \\int_{-\\infty}^{\\infty} f_{X,Y,Z}(x,y,z)\\, dz\n= \\int_{-\\infty}^{\\infty} f_X(x)\\, f_Y(y)\\, f_Z(z)\\, dz.\n\\]\nSince \\(f_X(x)\\) and \\(f_Y(y)\\) do not depend on \\(z\\): \\[\nf_{X,Y}(x,y) = f_X(x)\\, f_Y(y) \\int_{-\\infty}^{\\infty} f_Z(z)\\, dz\n= f_X(x)\\, f_Y(y) \\cdot 1.\n\\]\nThus, \\(X\\) and \\(Y\\) remain independent after marginalization.\n\n\n\nAn immediate implication of the definition of independence is the following.\n\nProposition 3.1 Let \\(X\\) and \\(Y\\) be independent random variables defined on a common probability space. Consider \\(U = g(X)\\) and \\(V = h(Y)\\) for some (measurable) functions \\(g\\) and \\(h\\). Then, \\(U\\) and \\(V\\) are independent.\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nConsider any (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\) and consider the events \\(\\{ U \\in B_1 \\}\\) and \\(\\{ V \\in B_2 \\}\\). Note that\n\n\\(\\{ U \\in B_1 \\} = \\{ X \\in g^{-1}(B_1) \\}\\).\n\\(\\{ V \\in B_2 \\} = \\{ Y \\in h^{-1}(B_2) \\}\\).\n\nSince the random variables \\(X\\) and \\(Y\\) are independent, the events \\(\\{ X \\in g^{-1}(B_1) \\}\\) and \\(\\{ Y \\in h^{-1}(B_2) \\}\\). Which implies that the events \\(\\{ U \\in B_1 \\}\\) and \\(\\{ V \\in B_2 \\}\\) are independent. Consequently, the random variables \\(U\\) and \\(V\\) are independent.\n\n\n\n\nProposition 3.2 Let \\(X\\) and \\(Y\\) be independent random variables defined on a common probability space. Then \\(X\\) and \\(Y\\) are independent if and only if \\[\\begin{equation}\\label{eq:expectation-product}\n  \\EXP[ g(X) h(Y) ] = \\EXP[ g(X) ] \\EXP[ h(Y) ]\n\\end{equation}\\] for all (measurable) functions \\(g\\) and \\(h\\).\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThere are two claims here.\n\nIf \\(X\\) and \\(Y\\) are independent then \\(\\eqref{eq:expectation-product}\\) holds.\nIf \\(\\eqref{eq:expectation-product}\\) holds, then \\(X\\) and \\(Y\\) are independent.\n\nWe will prove the first claim assuming that \\(X\\) and \\(Y\\) are continuous. Similar argument works for the discrete case as well. \\[\\begin{align*}\n  \\EXP[ g(X) h(Y) ]\n  &= \\int_{-∞}^∞ \\int_{-∞}^∞ g(x) h(y) f_{X,Y}(x,y)\\, dx dy \\\\\n  &\\stackrel{(a)}= \\int_{-∞}^∞ \\int_{-∞}^∞ g(x) h(y) f_{X}(x) f_{Y}(y)\\, dy dx \\\\\n  &\\stackrel{(b)}= \\int_{-∞}^∞ \\left[ \\int_{-∞}^∞ g(x)f_{X}(x)\\, dx \\right]h(y) f_{Y}(y)  \\, dy \\\\\n  &\\stackrel{(c)}= \\left[ \\int_{-∞}^∞ g(x)f_{X}(x)\\, dx \\right]\n  \\left[\\int_{-∞}^∞  h(y) f_{Y}(y)  \\, dy \\right] \\\\\n  &= \\EXP[ g(X) ] \\EXP [ h(Y) ]\n\\end{align*}\\] where \\((a)\\) follows from the fact that \\(X \\independent Y\\), \\((b)\\) and \\((c)\\) are simple algebra, and the last step uses the definition of expectation.\nTo prove the second claim, pick any (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\) and consider the functions \\(g(x) = \\IND_{B_1}(x)\\) and \\(h(y) = \\IND_{B_2}(y)\\). Observe that \\[\\begin{align*}\n  \\PR(X \\in B_1, Y \\in B_2)\n  &= \\EXP[\\IND_{ \\{ X \\in B_1, Y \\in B_2 \\}}] \\\\\n  &\\stackrel{(d)}= \\EXP[\\IND_{ \\{ X \\in B_1 \\}} \\IND_{\\{ Y \\in B_2 \\}}] \\\\\n  &\\stackrel{(e)}=\\EXP[\\IND_{ \\{ X \\in B_1 \\}}\\ \\EXP[ \\IND_{\\{ Y \\in B_2 \\}}] \\\\\n  &\\stackrel{(f)}= \\PR(X \\in B_1) \\PR(Y \\in B_2)\n\\end{align*}\\] where \\((d)\\) follows from basic algebra, \\((e)\\) follows from \\(\\eqref{eq:expectation-product}\\), and \\((f)\\) follows from expectation of an indicator.\nThe above equation shows that for any arbitrary (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\), \\(\\PR(X \\in B_1, Y \\in B_2) = \\PR(X \\in B_1) \\PR(Y \\in B_2)\\). Hence, \\(\\{X \\in B_1\\} \\independent \\{Y \\in B_2 \\}\\). Since \\(B_1\\) and \\(B_2\\) were arbitrary, we have \\(X \\independent Y\\).\n\n\n\n\nExample 3.11 Let \\(X\\) and \\(Y\\) be independent random variables defined on a common probability space. Show that\n\n\\(\\EXP[XY] = \\EXP[X] \\EXP[Y]\\).\n\\(\\VAR(X + Y) = \\VAR(X) + \\VAR(Y)\\).\n\n\n\nDefinition 3.2 A collection of random variables \\(X_1, \\dots, X_n\\) is called independent and identically distributed (i.i.d.) if all random variables are independent and have the same distribution, i.e., \\[\n  F_{X_1}(x_1) = F_{X_2}(x_2) = \\cdots = F_{X_n}(x_n).\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#functions-of-random-variables",
    "href": "random-vectors.html#functions-of-random-variables",
    "title": "3  Random vectors",
    "section": "3.3 Functions of random variables",
    "text": "3.3 Functions of random variables\nIn an interconnected systems, the output of one system is used as input to another system. To analyze such systems, it is important to understand how to analyze functions of random variables.\nThe same idea can be used for functions of multiple random variables as we illustrate via the following examples.\n\nExample 3.12 Let \\(X\\) and \\(Y\\) be random variables defined on a common probability space. Define \\[\nU = \\max(X,Y)\n\\quad\nV = \\min(X,Y).\n\\] Find \\(F_U\\) and \\(F_V\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nWe first look at \\(F_U\\). By definition \\[ F_U(u) = \\PR(X \\le u, Y \\le u) = F_{X,Y}(u,u).\\]\nNow consider \\(F_V\\). The event \\(\\{V \\le v\\}\\) can be expressed as \\[ \\{ V \\le v \\} = \\{ X \\le v \\} \\cup \\{Y \\le v \\} \\cap \\{X \\le v \\} \\cap \\{Y \\le v\\}.\\] Thus, \\[F_V(v) = F_X(v) + F_Y(v) - F_{X,Y}(v,v). \\]\n\n\n\n\n\nExample 3.13 Suppose \\(X_1\\) and \\(X_2\\) are continuous random variables and \\(Y = X_1 + X_2\\). Find the PDF \\(f_Y(y)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe can write the CDF \\(F_Y(y)\\) as follows: \\[\nF_Y(y)\n= \\int_{-∞}^∞ \\int_{-∞}^{y - x_1} f_{X_1,X_2}(x_1, x_2)\\, d x_2 d x_1 \\\\\n\\] Therefore, \\[\\begin{align*}\nf_Y(y) &= \\frac{d F_Y(y)}{dy} \\\\\n&= \\int_{-∞}^∞ \\frac{d}{dy} \\int_{-∞}^{y-x_1} f_{X_1, X_2}(x_1, x_2) \\, dx_2\\, dx_1 \\\\\n&= \\int_{-∞}^∞ f_{X_1, X_2}(x_1, y - x_1)\\, dx_1.\n\\end{align*}\\]\n\n\n\n\nExample 3.14 Repeat Example 3.13 when \\(X_1\\) and \\(X_2\\) are independent.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nIn this case, \\(f_{X_1, X_2}(x_1, x_2) = f_{X_1}(x_1) f_{X_2}(x_2)\\). Therefore, we get \\[f_Y(y) = \\int_{-∞}^{∞} f_{X_1}(x_1) f_{X_2}(y - x_2) d x_1 = (f_{X_1} * f_{X_2})(y)\\] where \\(*\\) denotes convolution.\n\n\n\n\nExample 3.15 Repeat Example 3.14 when \\(X_1 \\sim \\text{Poisson}(λ_1)\\) and \\(X_2 \\sim \\text{Poisson}(λ_2)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall that for a Poisson random variable \\(X\\) with parameter \\(λ\\) \\[\nP_X(k) = e^{-λ} \\frac{λ^k}{k!}, \\quad k \\ge 0\n\\]\nThus, \\[\\begin{align*}\nP_Y(n) &= (P_{X_1} * P_{X_2})(n)\n= \\sum_{k=-∞}^{∞} P_{X_1}(k) P_{X_2}(n-k)\n\\\\\n&=\\sum_{k=0}^{n} P_{X_1}(k) P_{X_2}(n-k)  \n\\\\\n&= \\sum_{k=0}^n e^{-λ_1 - λ_2} \\frac{ λ_1^k λ_2^{n-k} }{ k! (n-k)! }\n\\\\\n&= e^{-(λ_1 + λ_2)} \\frac{1}{n!}\n\\sum_{k=0}^n \\frac{n!}{k!(n-k)!} λ_1^k λ_2^{n-k}\n\\\\\n&= e^{-(λ_1 + λ_2)} \\frac{(λ_1 + λ_2)^n}{n!}\n\\end{align*}\\]\nThus, \\(Y \\sim \\text{Poisson}(λ_1 + λ_2)\\).\n\n\n\n\n3.3.1 Change of variables formulas\nFor continuous random variables, it is possible to obtain a general change of variable formula to obtain the PDF of functions of random variable in terms of their joint PDF.\nSuppose \\(\\{X_1, \\dots, X_n\\}\\) are jointly continuous random variables with joint PDF \\(f\\). Consider \\(n\\) random variables: \\[\\begin{align*}\n    Y_1 &= g_1(X_1, \\dots, X_n), \\\\\n    Y_2 &= g_2(X_1, \\dots, X_n), \\\\\n    \\vdots &= \\vdots \\\\\n    Y_n &= g_n(X_1, \\dots, X_n).\n\\end{align*}\\] We can view this as an equation between two \\(n\\)-dimensional vectors \\(Y = \\VEC(Y_1, \\dots, Y_n)\\) and \\(X = \\VEC(X_1, \\dots, X_n)\\) written as \\[ Y = g(X) \\]\nAs was the case for the scalar system, for a given \\(y \\in \\reals^n\\), the vector equation \\(y = g(x)\\) may have zero, one, or multiple solutions.\n\nIf \\(y = g(x)\\), \\(y \\in \\reals^n\\) has no solution, then \\[ f_Y(y) = 0. \\]\nIf \\(y = g(x)\\), \\(y \\in \\reals^n\\) has one solution \\(x \\in \\reals^n\\), then \\[ f_Y(y) = \\frac{f_X(x)}{\\ABS{J(x)}}, \\quad \\text{where } y = g(x)\\] and \\(J(x)\\) denotes the Jacobian on \\(g(x)\\) evaluated at \\(x = (x_1, \\dots, x_n)\\), i.e., \\[\n\\def\\1#1#2{\\dfrac{∂ g_{#1}}{∂ x_{#2}}}\nJ(x_1, \\dots, x_n) =\n\\DET{ \\1 11 & \\cdots & \\1 1n \\\\\n      \\vdots & \\vdots & \\vdots \\\\\n      \\1 n1 & \\cdots & \\1 nn }\n\\]\nIf \\(y = g(x)\\), \\(y \\in \\reals^n\\) has multiple solutions given by \\(\\{x^{(1)}, \\dots, x^{(m)}\\}\\), then \\[ f_Y(y) =\n\\sum_{k=1}^m \\frac{f_X(x^{(k)})}{\\ABS{J(x^{(k)})}}.\\]\n\n\nExample 3.16 Resolve Example 3.12 using the change of variables formula.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nLet \\(g_1(x,y) = \\max\\{x, y\\}\\) and \\(g_2(x,y) = \\min\\{x,y\\}\\). Define \\[ U = g_1(X,Y) \\quad\\text{and}\\quad V = g_2(X,Y).\\]\nDefine \\(g(x,y) = \\VEC(g_1(x,y), g_2(x,y))\\). Note that \\(g\\) is not differentiable at \\(x=y\\).\n\nWhen \\(x &gt; y\\), we have \\(g_1(x,y) = x\\) and \\(g_2(x,y) = y\\). Thus, \\[\nJ(x,y) = \\DET{\\1 11 & \\1 12 \\\\ \\1 21 & \\1 22}\n= \\DET{1 & 0 \\\\ 0 & 1} = 1. \\]\nWhen \\(x &lt; y\\), we have \\(g_1(x,y) = y\\) and \\(g_2(x,y) = x\\). Thus, \\[\nJ(x,y) = \\DET{\\1 11 & \\1 12 \\\\ \\1 21 & \\1 22}\n= \\DET{0 & 1 \\\\ 1 & 0} = -1. \\]\n\nWe now compute \\(f_{U,V}(u,v)\\).\n\nIf \\(u &lt; v\\), then the equation \\((u,v) = g(x,y)\\) has no solution. So we set \\[ f_{U,V}(u,v) = 0. \\]\nIf \\(u &gt; v\\), then the equation \\((u,v) = g(x,y)\\) has two solutions: \\(\\{ (u,v), (v,u) \\}\\). Thus, \\[\nf_{U,V}(u,v) = \\frac{f_{X,Y}(u,v)}{\\ABS{1}} + \\frac{f_{X,Y}(v,u)}{\\ABS{-1}}\n= f_{X,Y}(u,v) + f_{X,Y}(v,u). \\]\nIf \\(u = v\\), then the equation \\((u,u) = g(x,y)\\) has one solution \\((u,u)\\). Thus, \\[ f_{U,V}(u,u) = f_{X,Y}(u,u). \\] Note that \\(u = v\\) is a line in two-dimensional space. (Formally, it is a set of measure zero.) Hence, the choice of \\(f_{U,V}\\) at \\(u = v\\) will not affect any probability computations. So we can also set \\[ f_{U,V}(u,u) = 0. \\]\n\nFrom the joint PDF \\(f_{U,V}\\), we can compute the marginals as follows:\n\nFor \\(U\\), we have \\[\nf_U(u) = \\int_{-∞}^{∞} f_{U,V}(u,v) dv\n= \\int_{-∞}^{u} \\bigl[ f_{X,Y}(u,v) + f_{X,Y}(v,u) \\bigr] dv.\n\\] Therefore, \\[\nF_U(u) = \\int_{-∞}^{u} f_U(\\tilde u) d\\tilde u\n= \\int_{-∞}^u \\int_{-∞}^{\\tilde u} \\bigl[ f_{X,Y}(\\tilde u,v) + f_{X,Y}(v,\\tilde u) \\bigr] dv d\\tilde u.\n\\] Note that \\[ \\int_{-∞}^u \\int_{-∞}^{\\tilde u} f_{X,Y}(\\tilde u, v) dv d\\tilde u\n= \\int_{-∞}^u \\int_{-∞}^{x} f_{X,Y}(x, y) dy dx\n\\] and \\[\\begin{align*}\n\\int_{-∞}^u \\int_{-∞}^{\\tilde u} f_{X,Y}(v, \\tilde u) dv d\\tilde u\n&= \\int_{-∞}^u \\int_{-∞}^y f_{X,Y}(x,y) dx dy\n\\\\\n&= \\int_{-∞}^u \\int_{x}^u f_{X,Y}(x,y) dy dx\n\\end{align*}\\] where the last step follows from changing the order of integration.\nSubstituting these back in the expression for \\(F_U(u)\\), we get \\[\nF_U(u)\n= \\int_{-∞}^u \\int_{-∞}^{x} f_{X,Y}(x, y) dy dx\n+ \\int_{-∞}^u \\int_{x}^u f_{X,Y}(x,y) dy dx\n= \\int_{-∞}^u \\int_{-∞}^u f_{X,Y}(x,y) dy dx = F_{X,Y}(u,u). \\]\nFor \\(V\\), we can follow similar algebra as above.\n\n\n\n\n\nExample 3.17 Let \\(X\\) and \\(Y\\) be random variables defined on a common probability space. Define \\[\n  U = X^2 \\quad\\text{and}\\quad V = X + Y.\n\\] Find \\(F_{U,V}\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nLet’s consider the system of equations \\[\n  u = x^2 \\quad\\text{and}\\quad v = x + y\n\\] for a given value of \\((u,v)\\). First observe that \\[\n  J(x,y) = \\DET{ 2x & 0 \\\\ 1 & 1 } = 2x.\n\\]\n\nIf \\(u &lt; 0\\), then the system of equations has no solutions. Therefore, \\[\n   F_{U,V}(u,v) = 0, \\quad u &lt; 0.\n\\]\nIf \\(u = 0\\), then the system of equations has one solution: \\[\n   x^{(1)} = 0 \\quad\\text{and}\\quad y^{(1)} = v.\n\\] However, \\(J(0,v) = 0\\). So, \\[\n   f_{U,V}(0,v) = \\frac{f_{X,Y}(0,v)}{J(0,v)}\n\\] is undefined. However, since \\(u = 0\\) is a line in two-dimensions (i.e., a set of measure zero), the choice of \\(f_{U,V}\\) at \\(u = 0\\) will not affect any probability computations. So, we set \\[\n   f_{U,V}(0,v) = 0.\n\\]\nIf \\(u &gt; 0\\), then the system of equations has two solutions: \\[\n   (x^{(1)}, y^{(1)}) = (+\\sqrt{u}, v - \\sqrt{u})\n   \\quad\\text{and}\\quad\n   (x^{(2)}, y^{(2)}) = (-\\sqrt{u}, v + \\sqrt{u})\n\\] Therefore, \\[\n   f_{U,V}(u,v) = \\frac{f_{X,Y}(\\sqrt{u}, v - \\sqrt{u})}{2 \\sqrt{u}}\n   + \\frac{f_{X,Y}(-\\sqrt{u}, v + \\sqrt{u})}{2 \\sqrt{u}}.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#covariance-of-random-vectors",
    "href": "random-vectors.html#covariance-of-random-vectors",
    "title": "3  Random vectors",
    "section": "3.4 Covariance of random vectors",
    "text": "3.4 Covariance of random vectors\nWe start with the definition of covariance of two real-valued random variables. We will then generalize the notion to random vectors.\n\n3.4.1 Real-valued random variables\nLet \\(X\\) and \\(Y\\) be real-valued random variables defined on the same probability space.\n\nCovariance measures how two random variables vary together. In particular, let \\(X\\) and \\(Y\\) be jointly distributed random variables and let \\(μ_X\\) and \\(μ_Y\\) denote their means. Then, \\[ \\COV(X,Y) = \\EXP[(X - μ_X) (Y - μ_Y)].\\] Properties of expectation imply that \\[\n\\COV(X,Y) = \\EXP[XY] - \\EXP[X] \\EXP[Y].\n\\]\nCorrelation coefficient between \\(X\\) and \\(Y\\) is defined as \\[ρ_{XY} = \\frac{\\COV(X,Y)}{\\sqrt{\\VAR(X) \\VAR(Y)}}.\\]\nThe correlation coefficeint satisfies \\(\\ABS{ρ_{XY}} \\le 1\\) with equality if and only if \\(\\PR(aX + bY = c) = 1\\) for some \\(a,b,c \\in \\reals\\). [The proof follows from Cauchy-Schwartz inequality, which we will study later]\n\\(X\\) and \\(Y\\) are said to be uncorrelated if \\(ρ_{XY} = 0\\), which is equivalent to \\(\\COV(X,Y) = 0\\) or \\(\\EXP[XY] = \\EXP[X] \\EXP[Y]\\).\nNote that \\[\\begin{align*}\n  \\VAR(X + Y) &= \\EXP[ ((X - \\EXP[X]) + (Y - \\EXP[Y]) )^2 ]\n  \\\\\n  &= \\VAR(X) + \\VAR(Y) + 2\\COV(X,Y).\n\\end{align*}\\] Thus, when \\(X\\) and \\(Y\\) are uncorrelated, we have \\[ \\VAR(X + Y) = \\VAR(X) + \\VAR(Y). \\]\nIndepdent random variables are uncorrelated but the reverse in not true.\n\n\nExample 3.18 Consider the probability space \\((Ω, \\ALPHABET F, \\PR)\\) where \\(Ω = [0, 2 π)\\), \\(\\ALPHABET F\\) is the Borel \\(σ\\)-algebra on \\([0, 2 π)\\) and \\(\\PR\\) is the uniform distribution on \\(Ω\\). Define \\(X(ω) = \\cos ω\\) and \\(Y(ω) = \\sin ω\\). Show that \\(X\\) and \\(Y\\) are uncorrelated but not independent.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe event \\(\\{X = 1\\}\\) corresponds to \\(ω = 0\\) and therefore \\(\\{Y = 0\\}\\). Thus, \\(X\\) and \\(Y\\) are not independent.\nObserve that\n\n\\(\\displaystyle \\EXP[X] = \\int_{0}^{2 π} \\cos ω \\frac{1}{2 π}\\, d ω = 0\\).\n\\(\\displaystyle \\EXP[Y] = \\int_{0}^{2 π} \\sin ω \\frac{1}{2 π}\\, d ω = 0\\).\n\\(\\displaystyle \\EXP[XY] = \\int_{0}^{2 π} \\cos ω \\sin ω \\frac{1}{2 π}\\, d ω =\n\\frac{1}{4{π}} \\int_0^{2 π} \\cos 2 ω\\, d ω = 0\\).\n\nThus, \\[\\EXP[XY] = \\EXP[X]\\EXP[Y].\\]\n\n\n\n\n\n3.4.2 Random vectors\nThe idea of covariance generalizes to random vectors. First we recall the definition of expectation for random vectors and random matrices.\n\nFor a random vector \\(X = [X_1, \\dots, X_n] \\in \\reals^n\\), we have \\[ \\EXP[X] = \\MATRIX{ \\EXP[X_1] & \\cdots & \\EXP[X_n] }. \\]\nFor a random matrix \\(X = \\MATRIX{ X_{1,1} & \\cdots & X_{1,n} \\\\ X_{2,1} & \\cdots & X_{2,n} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nX_{m,1} & \\cdots & X_{m,n} } \\in \\reals^{m \\times n}\\), we have \\[ \\EXP[X] = \\MATRIX{ \\EXP[X_{1,1}] & \\cdots & \\EXP[X_{1,n}] \\\\ \\EXP[X_{2,1}] & \\cdots & \\EXP[X_{2,n}] \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\EXP[X_{m,1}] & \\cdots & \\EXP[X_{m,n}] }.\n\\]\n\nWith the above notation, we have the following.\n\nThe covariance matrix of a random vector \\(X \\in \\reals^n\\) is defined as \\[Σ_X = \\COV(X) = \\EXP[ (X - μ_X) (X - μ_X)^\\TRANS].\\]\nWhen \\(X = [X_1, \\dots, X_n]^\\TRANS\\), the covariance can be written as \\[\n  Σ_X = \\MATRIX{\n      \\VAR(X_1) & \\COV(X_1, X_2) & \\cdots & \\COV(X_1, X_n) \\\\\n      \\COV(X_2, X_1) & \\VAR(X_2) & \\cdots & \\COV(X_1, X_n) \\\\\n      \\vdots & \\ddots & \\ddots & \\vdots \\\\\n      \\COV(X_n, X_1) & \\COV(X_n, X_2) & \\cdots & \\VAR(X_n) }\n\\]\nIt is easy to see that \\(Σ_X = Σ_X^\\TRANS\\). Thus, the covariance matrix is symmetric.\nIf the components \\(X_1, \\dots, X_n\\) are independent, then \\(Σ_X\\) is a diagonal matrix.\n\\(Σ_X\\) is positive semidefinite, i.e., its eigenvalues are real and non-negative. To see this, observe that for any (deterministic) vector \\(v\\), we have \\[\\begin{align*}\n  v^\\TRANS Σ_X v\n  &= v^\\TRANS \\EXP[ (X - μ_X) (X - μ_X)^\\TRANS ] v \\\\\n  &= \\EXP[ v^\\TRANS (X - μ_X) (X - μ_X)^\\TRANS v ]  \\\\\n  &= \\EXP[ w^\\TRANS w ] = \\EXP[ \\NORM{w}^2 ] \\ge 0.\n\\end{align*}\\] where \\(w = (X - μ_X)^\\TRANS v\\).\nThe cross covariance matrix of random vectors \\(X \\in \\reals^n\\) and \\(Y \\in \\reals^m\\) is a \\(n × p\\) matrix given by \\[\nΣ_{XY} = \\COV(X,Y) = \\EXP[ (X - μ_X) (Y - μ_Y)^\\TRANS ].\n  \\]\nTwo random vectors \\(X\\) and \\(Y\\) are called uncorrelated if \\[ Σ_{XY} = 0 \\implies\n\\EXP[XY^\\TRANS] = \\EXP[X] \\EXP[Y]^\\TRANS. \\]\nTwo random vectors \\(X\\) and \\(Y\\) are called orthogonal if \\[\\EXP[X Y^\\TRANS] = 0 \\]\n\n\n\n3.4.3 Linear transformation of mean and covariance\nConsider a random vector (not necessarily Gaussian) \\(X = [X_1, \\dots, X_n]^\\TRANS\\) with mean \\(μ_X\\) and covariance \\(Σ_X\\).\nLet \\(A \\in \\reals^{n × n}\\) be a deterministic matrix and \\(b \\in \\reals^n\\) be a deterministic vector.\nDefine \\(Y = A X + b\\) and let \\(μ_Y\\) and \\(Σ_Y\\) denote the mean and covariance of \\(Y\\). Then, \\[\n  μ_Y = A μ_X + b\n  \\quad\\text{and}\\quad\n  Σ_Y = A Σ_X A^\\TRANS\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe property of the mean follows immediately from properties of expectation. For the covariance, note that \\[\\begin{align*}\n  Σ_Y &= \\EXP[ (Y - μ_Y) (Y - μ_Y)^\\TRANS ] \\\\\n  &= \\EXP[ (A X - A μ_X) (AX - A μ_X)^\\TRANS ] \\\\\n  &= \\EXP[ A (X - μ_X) (X - μ_X)^\\TRANS A^\\TRANS ] \\\\\n  &= A Σ_X A^\\TRANS\n\\end{align*}\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#multi-dimensional-gaussian",
    "href": "random-vectors.html#multi-dimensional-gaussian",
    "title": "3  Random vectors",
    "section": "3.5 Multi-dimensional Gaussian",
    "text": "3.5 Multi-dimensional Gaussian\nA \\(n\\)-dimensional Gaussian with mean \\(μ\\) and covariance \\(Σ\\), written as \\(\\mathcal{N}(μ, Σ)\\), has the PDF \\[\n  f_X(x) = \\frac{1}{\\sqrt{(2 π)^n \\det(Σ)}}\n  \\exp\\left( - \\tfrac{1}{2} (x - μ)^\\TRANS Σ^{-1} (x-μ) \\right),\n  \\quad x \\in \\reals^n.\n\\]\nWe consider a few special cases to understand the definition.\n\nConsider the special case when all components are independent, let \\[\n   μ = \\MATRIX{μ_1 \\\\ \\vdots μ_n},\n  \\quad\\text{and}\\quad\n  Σ = \\MATRIX{σ_1^2 & \\cdots & 0 \\\\\n              \\vdots & \\ddots & \\vdots \\\\\n              0 & \\cdots & σ_n^2 }\n\\] where \\(μ_i = \\EXP[X_i]\\) and \\(σ_i^2 = \\VAR(X_i)\\). Observe that \\[\n  \\det(Σ) = \\prod_{i=1}^n σ_i^2\n\\] and \\[\n  (x - μ)^\\TRANS Σ^{-1} (x - μ) = \\sum_{i=1}^n \\frac{(x_i - μ_i)^2}{σ_i^2}.\n\\] Thus, \\[\n  f_X(x) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 π σ_i^2}}\n  \\exp\\left(-\\frac{(x_i-μ_i)^2}{2 σ_i^2} \\right)\n\\] which is the product of the marginal distributions.\nConsider the two-dimensional Gaussian vector and let \\(σ_i^2 = \\VAR(X_i)\\) and \\(ρ = \\COV(X_1,X_2)/σ_1 σ_2\\). Then, \\[\n   Σ = \\MATRIX{ σ_1^2 & ρ σ_1 σ_2 \\\\ ρ σ_1 σ_2 & σ_2^2 }.\n\\] When \\(\\ABS{ρ} = 1\\), the distribution is singular. When \\(\\ABS{ρ} &lt; 1\\), \\(\\det(Σ) = (1 - ρ^2)σ_1^2 σ_2^2\\) and \\[\n   Σ^{-1} = \\frac{1}{(1 - ρ^2)σ_1^2 σ_2^2}\n   \\MATRIX{ σ_2^2 & -ρ σ_1^2 σ_2^2 \\\\ -ρ σ_1^2 σ_2^2 & σ_1^2}.\n\\] Thus, \\[\n(x-μ)^\\TRANS Σ^{-1} (x - μ)\n= \\frac{1}{1 - ρ^2} \\biggl[\n   \\frac{(x_1 - μ_1)^2}{2 σ_1^2 }\n   - 2 ρ \\frac{(x_1 - μ_1)(x_2 - μ_2)}{σ_1 σ_2}\n   + \\frac{(x_2 - μ_2)^2}{2 σ_2^2 }\n\\biggr].\n\\] Thus, the level set \\[(x - μ)^\\TRANS Σ^{-1} (x - μ) = k^2\\] is an ellipse centered at \\(μ\\).\n\nWhen \\(ρ = 0\\), the axes of the ellipse are aligned with the coordinates.\nWhen \\(ρ &gt; 0\\), the ellipse tilts towards the \\(x_1 = x_2\\) diagonal\nWhen \\(ρ &lt; 0\\), the ellipse tilts towards the anti-diagonal.\n\n\n\n3.5.1 Linear transformation of Gaussian vectors\n\nIf real-valued random variables \\(X\\) and \\(Y\\) are jointly Gaussian, then \\(X + Y\\) is Gaussian. We will prove this later using moment generating functions.\nIf \\(X\\) and \\(Y\\) are jointly Gaussian, then they are independent if and only if they are uncorrelated.\nLet \\(X\\) let a Gaussian random vector with mean \\(μ_X\\) and covariance \\(Σ_X\\) and \\(Y = AX + b\\) for a constant matrix \\(A\\) and vector \\(b\\). Then \\(Y\\) is also a Gaussian vector with \\[μ_Y = A μ_X  + b\n\\quad\\text{and}\\quad\n  Σ_Y = A Σ_X A^\\TRANS.\n\\]\nLet \\(X \\sim \\mathcal{N}(μ, Σ)\\) and define \\(Z = Σ^{-\\frac 12}(X - μ)\\). Then, \\[\n   μ_Z = Σ^{-1}(μ - μ) = 0\n   \\quad\\text{and}\\quad\n   Σ_Z = Σ^{-\\frac 12} Σ Σ^{-\\frac 12} = I.\n\\] Thus, \\(Z \\sim \\mathcal{N}(0, I)\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html",
    "href": "conditional-expectation.html",
    "title": "4  Conditional probability and conditional expectation",
    "section": "",
    "text": "4.1 Conditioning on events\nConditional probability is perhaps the most important aspect of probability theory as it explains how to incorporate new information in a probability model. However, formally defining conditional probability is a bit intricate. In the notes, I will first provide an intuitive high-level explanation of conditional probability. We will then do a deeper dive, trying to develop a bit more intuition about what is actually going on.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html#conditioning-on-events",
    "href": "conditional-expectation.html#conditioning-on-events",
    "title": "4  Conditional probability and conditional expectation",
    "section": "",
    "text": "Recall that conditional probability for events is defined as follows: given a probability space \\((Ω, \\ALPHABET F, \\PR)\\) and events \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(B) &gt; 0\\), we have \\[\n\\PR(A \\mid B) = \\frac{\\PR(A \\cap B)}{\\PR(B)}.\n\\]\nBuilding on this definition, we can define the conditional CDF of a random variable \\(X\\) conditioned on an event \\(C\\) (such that \\(\\PR(C) &gt; 0\\)) as follows: \\[\nF_{X|C}(x \\mid C) = \\PR(X \\le x \\mid C) = \\frac{\\PR( \\{ X \\le x \\} \\cap C)}{\\PR(C)}.\n\\]\nAs we pointed out, conditional probabilities are probabilities, the conditional CDF defined above satisfies the properties of regular CDFs. In particular\n\n\\(0 \\le F_{X|C}(x\\mid C) \\le 1\\)\n\\(\\displaystyle \\lim_{x \\to -∞} F_{X|C}(x \\mid C) = 0\\)\n\\(\\displaystyle \\lim_{x \\to +∞} F_{X|C}(x \\mid C) = 1\\)\n\\(F_{X|C}(x \\mid C)\\) is non-decreasing function.\n\\(F_{X|C}(x \\mid C)\\) is right-continuous function.\n\nSince \\(F_{X|C}\\) is CDF, we can classify random variables conditioned on an event as discrete or continuous in the usual way. In particular\n\nIf \\(F_{X|C}\\) is piecewise constant, then \\(X\\) conditioned on \\(C\\) is a discrete random variable which takes values in a finite or countable subset \\(\\text{range}(X\\mid C) = \\{x_1, x_2, \\dots\\}\\) of \\(\\reals\\). Furthermore, \\(X\\) conditioned on \\(C\\) has a conditional PMF \\(P_{X|C} \\colon \\reals \\to [0,1]\\) defined as \\[\n  P_{X|C}(x\\mid C) = F_{X|C}(x\\mid C) - F_{X|C}(x^{-} \\mid C).\n\\]\nIf \\(F_{X|C}\\) is continuous, then \\(X\\) conditioned on \\(C\\) is a continuous random variable which has a conditional PDF \\(f_{X|C}\\) given by \\[\nf_{X|C}(x\\mid C) = \\frac{d}{dx} F_X(x \\mid C).\n\\]\nIf \\(F_{X|C}\\) is neither piecewise constant nor continuous, then \\(X\\) conditioned on \\(C\\) is a mixed random variable.\n\nTherefore, a random variable conditioned on an event behaves exactly like a regular random variable. We can define conditional expectation \\(\\EXP[X \\mid C]\\), conditional variance \\(\\VAR(X \\mid C)\\) in the obvious manner.\nIf \\(X\\) is a random variable with \\(\\EXP[\\ABS{X}] &lt; ∞\\) and \\(C\\) is an event with \\(\\PR(C) &gt; 0\\), then \\(\\EXP[X | C]\\) is defined as the expectation with respect with the conditional CDF \\(F_{X|C}\\). In particular, \\[\n   \\EXP[ X \\mid C] = \\frac{\\EXP[ X \\IND_{C}]}{\\PR(C)}.\n\\]\nAn immediate implication of the law of total probability is the following.\nIf \\(C_1, C_2, \\dots, C_n\\) is a partition of \\(Ω\\), then \\[\n  F_X(x) = \\sum_{i=1}^n F_{X|C_i}(x \\mid C_i) \\PR(C_i).\n\\] Furthermore, if \\(X\\) and \\(X\\) conditioned on \\(C\\) are both discrete, we have \\[\nP_X(x) = \\sum_{i=1}^n P_{X|C_i}(x \\mid C_i) \\PR(C_i)\n\\] and if \\(X\\) and \\(X\\) conditioned on \\(C\\) are both continuous, we have \\[\n  f_X(x) = \\sum_{i=1}^n f_{X|C_i}(x \\mid C_i) \\PR(C_i).\n\\]\nIf \\(C_1, C_2, \\dots, C_n\\) is a partition of \\(Ω\\), then we can write \\[\n    X = \\sum_{i=1}^n X \\IND_{C_i}.\n\\] This gives us the law of total expectation \\[\n    \\EXP[X] = \\sum_{i=1}^n \\EXP[ X \\mid C_i ] \\PR(C_i).\n\\]\n\n\nExample 4.1 Consider the following experiment. A fair coin is tossed. If the outcome is heads, \\(X\\) is a uniform \\([0,1]\\) random variable. If the outcome is tails, \\(X\\) is \\(\\text{Bernoulli}(p)\\) random variable. Find \\(F_X(x)\\).\n\n\nExample 4.2 (Memoryless property of geometric random variable) Let \\(X \\sim \\text{geometric}(p)\\) and \\(m,n\\) be positive integers. Compute \\[ \\PR(X &gt; n + m \\mid X &gt; m). \\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall that the PMF of a geometric random variable is \\[\nP_X(k) = p (1-p)^{k-1}, \\quad k \\in \\naturalnumbers.\n\\] Therefore, \\[\n\\PR(X &gt; m) = \\sum_{k = m + 1}^∞ P_X(k)\n= \\sum_{k=m+1}^{∞} p (1-p)^{k-1} = (1-p)^m.\n\\]\nNow consider \\[\\begin{align*}\n\\PR(X &gt; m + n \\mid X &gt; m) &=\n\\frac{ \\PR(\\{ X &gt; m + n \\} \\cap \\{X &gt; m \\}) } {\\PR(X &gt; m) }\n\\\\\n&=\n\\frac{ \\PR(X &gt; m + n ) } {\\PR(X &gt; m) } \\\\\n&= \\frac{(1-p)^{m+n}}{(1-p)^m} = (1-p)^n = \\PR(X &gt; n).\n\\end{align*}\\]\nThis is called the memoryless property of a geometric random variable.\n\n\n\n\nExample 4.3 (Memoryless property of exponential random variable) Let \\(X \\sim \\text{Exponential}(λ)\\) and \\(t,s\\) be positive reals. Compute \\[ \\PR(X &gt; t + s \\mid X &gt; t). \\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall that the PDF of an exponential random variable is \\[\nf_X(x) = λ e^{-λ x}, \\quad x \\ge 0.\n\\] Therefore, \\[\n\\PR(X &gt; t) = \\int_{t}^{∞} f_X(x)\\, dx = e^{-λ t}.\n\\]\nNow consider \\[\\begin{align*}\n\\PR(X &gt; t + s \\mid X &gt; t) &=\n\\frac{ \\PR(\\{ X &gt; t + s \\} \\cap \\{X &gt; t \\}) } {\\PR(X &gt; t) }\n\\\\\n&=\n\\frac{ \\PR(X &gt; t + s ) } {\\PR(X &gt; t) } \\\\\n&= \\frac{e^{-λ(t+s)}}{e^{-λt}} = e^{-λs} = \\PR(X &gt; s).\n\\end{align*}\\]\nThis is called the memoryless property of a exponential random variable.\n\n\n\n\nExample 4.4 Suppose \\(X\\) and \\(Y\\) are random variables that are uniformly distributed in the shaded region shown in Figure 4.1. Compute \\[\n  \\PR( X - Y &lt; 1 \\mid Y &lt; 2).\n\\]\n\nf_X_Y_2025_mid_term_1 = [\n  {x: 0, y: 0},\n  {x: 4, y: 0},\n  {x: 4, y: 4},\n  {x: 0, y: 0},\n];\n\nf_X_Y_2025_mid_term_1_A = [\n  {x: 0, y: 0},\n  {x: 4, y: 4},\n];\n\nf_X_Y_2025_mid_term_1_A_2 = [\n  {x: 1, y: 0},\n  {x: 4, y: 3},\n];\n\nf_X_Y_2025_mid_term_1_B = [\n  {x: 0, y: 0},\n  {x: 4, y: 0},\n  {x: 4, y: 2},\n  {x: 2, y: 2},\n  {x: 0, y: 0},\n];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  width: 300,\n  height: 300,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.areaY(f_X_Y_2025_mid_term_1, {x: \"x\", y: \"y\", fill: \"lightblue\"}),\n    Plot.line(f_X_Y_2025_mid_term_1, {x: \"x\", y: \"y\", stroke: \"black\"}),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 4.1: The joint PDF \\(f_{X,Y}\\) is uniform in the shaded region and zero outside.\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nDefine the events \\[\n  A = \\{ ω : X(ω) - Y(ω) &lt; 1 \\}\n  \\quad\\text{and}\\quad\n  B = \\{ ω : Y(ω) &lt; 2 \\}.\n\\] These are shown in Figure 4.2\nPlot.plot({\n  grid: true,\n  width: 300,\n  height: 300,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.areaY(f_X_Y_2025_mid_term_1_A, {x: \"x\", y: \"y\", fill: \"blue\"}),\n    Plot.areaY(f_X_Y_2025_mid_term_1_A_2, {x: \"x\", y: \"y\", fill: \"lightblue\"}),\n    Plot.line(f_X_Y_2025_mid_term_1, {x: \"x\", y: \"y\", stroke: \"black\"}),\n  ]\n})\nPlot.plot({\n  grid: true,\n  width: 300,\n  height: 300,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.areaY(f_X_Y_2025_mid_term_1, {x: \"x\", y: \"y\", fill: \"lightblue\"}),\n    Plot.areaY(f_X_Y_2025_mid_term_1_B, {x: \"x\", y: \"y\", fill: \"blue\"}),\n    Plot.line(f_X_Y_2025_mid_term_1, {x: \"x\", y: \"y\", stroke: \"black\"}),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(A = \\{ X - Y &lt; 1 \\}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(B = \\{ Y &lt; 2 \\}\\).\n\n\n\n\n\n\n\nFigure 4.2: The events \\(A\\) and \\(B\\)\n\n\n\nObserve that \\[\n  f_{X,Y}(x,y) = \\frac 18, \\quad 0 &lt; y &lt; x &lt; 4.\n\\] We are interested in computing \\[\n    \\PR(A \\mid B) = \\frac{\\PR(A \\cap B)}{\\PR(B)}.\n\\] We will compute the numerator and denominator separately. Note that since the joint distribution is a uniform distribution on the light red region, the probability of the dark blue regions is equal to their area, which can be either computed geometrically or analytically. We follow the latter approach here.\nObserve that \\[\n    \\PR(A\\cap B) = \\int_{0}^2 \\int_{y}^{y+1} \\frac 18 dx dy\n    = \\int_{0}^{2} \\frac 18 dy = \\frac 14.\n\\]\nSimilarly \\[\n    \\PR(B) =\n    \\int_{0}^2 \\int_{y}^{4} \\frac 18 dx dy\n    = \\int_{0}^2 \\frac{1}{8}(4 - y) dy\n    = \\frac 34.\n\\] Thus, \\[\n    \\PR(A \\mid B) = \\frac{\\PR(A \\cap B)}{\\PR(B)} = \\frac{1}{3}.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html#conditioning-on-random-variables",
    "href": "conditional-expectation.html#conditioning-on-random-variables",
    "title": "4  Conditional probability and conditional expectation",
    "section": "4.2 Conditioning on random variables",
    "text": "4.2 Conditioning on random variables\nWe first start with the case where we are conditioning on discrete random variables.\n\nIf \\(X\\) and \\(Y\\) are random variables defined on a common probability space and \\(Y\\) is discrete, then \\[F_{X|Y}(x \\mid y) = \\PR(X \\le x \\mid Y = y)\\] for any \\(y\\) such that \\(\\PR(Y = y) &gt; 0\\).\nIf \\(X\\) is also discrete, the conditional PMF \\(P_{X| Y}\\) is defined as \\[P_{X|Y}(x|y) = \\PR(X = x \\mid Y = y) = \\frac{P_{X,Y}(x,y)}{P_Y(y)}\\] for any \\(y\\) such that \\(\\PR(Y = y) &gt; 0\\).\nMoreover, we have that \\[ P_{X|Y}(x\\mid y) = F_{X|Y}(x\\mid y) - F_{X|Y}(x^{-}\\mid y). \\]\nThe above expression can be written differently to give the chain rule for random variables: \\[\n   P_{X,Y}(x,y) = P_{Y}(y) P_{X|Y}(x|y).\n\\]\nFor any event \\(B\\) in \\(\\mathscr B(\\reals)\\), the law of total probability may be written as \\[\\PR(X \\in B) = \\sum_{y \\in \\ALPHABET Y} \\PR(X \\in B \\mid Y = y) P_Y(y). \\]\nIf \\(X\\) is independent of \\(Y\\), we have \\[\n   P_{X|Y}(x\\mid y) = P_X(x), \\quad \\forall x,y \\in \\reals.\n\\]\n\nWe now consider the case when we are conditioning on a continuous random variable.\n\nIf \\(Y\\) is continuous, \\(\\PR(Y = y) = 0\\) for all \\(y\\). We may think of \\[ F_{X|Y}(x\\mid y) = \\lim_{δ \\downarrow 0} \\PR(X \\le x \\mid y \\le Y \\le y + δ).\\]\nWhen \\(X\\) and \\(Y\\) are jointly continuous, we define the conditional PDF \\[ f_{X|Y}(x \\mid y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}. \\]\nNote that the conditional PDF cannot be interpreted in the same manner as the conditional PMF because it gives the impression that we are conditioning on a zero-probability event. However, we can view it as a limit as follows:\n\\[\\begin{align*}\n   F_{X|Y}(x\\mid y) &= \\lim_{Δy \\downarrow 0} \\PR(X \\le x \\mid y \\le Y \\le y + Δy) \\\\\n   &= \\lim_{Δy \\downarrow 0} \\dfrac{\\PR(X \\le x, y \\le Y \\le y + Δy)}{\\PR(y \\le Y \\le y + Δy)} \\\\\n   &= \\lim_{Δy \\downarrow 0} \\dfrac{\\int_{-∞}^x \\int_{y}^{y + Δy} f_{X,Y}(u,v)\\, dv\\, du }{ \\int_{y}^{y+Δy} f_Y(v)\\, dv }\n\\end{align*}\\] If \\(Δy\\) is small, we can approximate \\[\n\\int_{y}^{y + Δy} f_Y(v)\\, dv \\approx f_Y(y) \\cdot Δy\n\\] and \\[\n\\int_{y}^{y + Δy} f_{X,Y}(u,v) dv \\approx f_{X,Y}(u,y) \\cdot Δy.\n\\] Substituting in the above equation, we get \\[\\begin{align*}\n   F_{X|Y}(x \\mid y) &\\approx \\lim_{Δy \\downarrow 0}\n   \\dfrac{ \\int_{-∞}^x f_{X,Y}(u,y) Δy\\, du }{ f_Y(y) Δy }\n   \\\\\n   &= \\int_{-∞}^x \\left[ \\frac{f_{X,Y}(u,y)}{f_Y(y)} \\right] du\n\\end{align*}\\]. Thus, when \\(X\\) and \\(Y\\) are jointly continuous, we have \\[\nf_{X|Y}(x\\mid y) = \\frac{d}{dx} F_{X|Y}(x \\mid y) =\n  \\dfrac{f_{X,Y}(x,y)}{f_Y(y)}.\n\\]\nThe formal definition of conditional densities requires some ideas from advanced probability theory, which we will not cover in this course. Nonetheless, I will try to explain the intuition behind the formal definitions in the next section.\nThe above expression may be written differently to give the chain rule for random variables: \\[ f_{X,Y}(x,y) = f_Y(y) f_{X|Y}(x \\mid y). \\]\nFor any event \\(B \\in \\mathscr B(\\reals)\\), the law of total probability may be written as \\[\n\\PR(X \\in B) = \\int_{-∞}^∞ \\PR(X \\in B \\mid Y = y) f_Y(y)\\, dy\n\\] An immediate implication of this is \\[\nF_X(x) = \\PR(X \\le x) = \\int_{-∞}^{∞} \\PR(X \\le x \\mid Y = y) f_Y(y)\\, dy\n= \\int_{-∞}^{∞} F_{X|Y}(x|y) f_Y(y)\\, dy.\n\\]\nIf \\(X\\) is independent of \\(Y\\), we have \\[\nf_{X|Y}(x\\mid y) = f_X(x), \\quad \\forall x, y \\in \\reals.\n\\]\nWe can show that conditional PMF and conditional PDF satisfy all the properties of PMFs and PDFs. Therefore, we can define conditional expectation \\(\\EXP[ g(X) \\mid Y = y ]\\) in terms of \\(P_{X|Y}\\) or \\(f_{X|Y}\\). We can similarly define conditional variance\n\n\nExample 4.5 Suppose \\(X\\) and \\(Y\\) are jointly continuous random variables with the joint PDF \\[\nf_{X,Y}(x,y) = \\frac{e^{-x/y} e^{-y}}{y}, \\quad 0 &lt; x &lt; ∞, 0 &lt; y &lt; ∞.\n\\] Find \\(f_{X|Y}\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe first compute the marginal \\(f_Y(y)\\).\n\\[\\begin{align*}\nf_Y(y) &= \\int_{-∞}^{∞} f_{X,Y}(x,y) \\, dx \\\\\n&= \\int_{0}^{∞} \\frac{e^{-x/y} e^{-y}}{y} dx \\\\\n&= \\frac{e^{-y}}{y} \\int_{0}^∞ e^{-x/y} dx \\\\\n&= e^{-y}.\n\\end{align*}\\] Thus, \\[ f_{X|Y}(x \\mid y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\n= \\frac{e^{-x/y}}{y}, \\quad 0 &lt; x &lt; ∞, 0 &lt; y &lt; ∞.\n\\]\n\n\n\n\nExample 4.6 Suppose \\(X \\sim \\text{Uniform}[0,1]\\) and given \\(X = x\\), \\(Y\\) is uniformly distributed on \\((0,x)\\). Find the PDF of \\(Y\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe will use the law of total probability. \\[\nF_Y(y) = \\int_{-∞}^{∞} F_{Y|X}(y \\mid x) f_X(x) \\, dx\n= \\int_{0}^1 F_{Y|X}(y \\mid x) \\, dx\n\\] where we have used the fact that \\(f_X(x) = 1\\) for \\(x \\in [0,1]\\). Now, we know that given \\(X = x\\), \\(Y \\sim \\text{uniform}[0,x]\\). Therefore, \\[\nf_{Y|X}(y\\mid x) = \\frac 1x, \\quad 0 &lt; y &lt; x.\n\\] Therefore, \\[\nF_{Y|X}(y \\mid x) = \\begin{cases}\n0 & y &lt; 0 \\\\\n\\dfrac{y}{x} & 0 &lt; y &lt; x \\\\\n1 & y \\ge x\n\\end{cases}\n\\]\nWe will compute \\(F_Y(y)\\) for the three cases separately.\n\nFor \\(y &lt; 0\\), \\[ F_Y(y) = \\int_{0}^1 F_{Y|X}(y|x) dx = 0.\\]\nFor \\(0 &lt; y &lt; 1\\), \\[ F_Y(y) = \\int_{0}^y 1\\, dx + \\int_{y}^1 \\frac{y}{x} \\, dx\n= y - y \\ln y. \\]\nFor \\(y &gt; 1\\), \\[ F_Y(y) = \\int_{0}^1 1 \\, dx = 1. \\]\n\nThus, \\[\nF_Y(y) = \\begin{cases}\n0 & y &lt; 0 \\\\\ny - y \\ln y & 0 \\le y &lt; 1 \\\\\n1 & y &gt; 1.\n\\end{cases}\n\\]\nHence, \\[\nf_Y(y) = \\frac{d F_{Y}(y)}{dy} = - \\ln y, \\quad 0 &lt; y &lt; 1.\n\\]\n\n\n\n\nExample 4.7 Let \\(X = [X_1, X_2]\\) be a bivariate Gaussian random variable with \\(μ_X = 0\\) and \\[\nΣ_X = \\MATRIX{ σ_1^2 & ρ σ_1 σ_2 \\\\ ρ σ_1 σ_2 & σ_2^2 }\n\\] where \\(\\ABS{ρ} &lt; 1\\). Compute the conditional PDF \\(f_{X_1 | X_2}\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall that \\[\n  f_{X_1,X_2}(x_1, x_2) =\n  \\frac{1}{(2 π) σ_1 σ_2 \\sqrt{(1 - ρ^2)}}\n  \\exp\\left( \\frac{1}{1 - ρ^2}\n  \\left[\n    \\frac{x_1^2}{2 σ_1^2} - 2 ρ \\frac{x_1 x_2}{σ_1 σ_2}\n    + \\frac{x_2^2}{2 σ_2^2} \\right]\n  \\right).\n\\] Moreover, \\[\n  f_{X_2}(x_2) = \\frac{1}{\\sqrt{2 π} σ_2} \\exp\\left(\n  - \\frac{x_2^2}{2 σ_2^2 } \\right).\n\\] Thus, \\[\\begin{align*}\n  f_{X_1 | X_2}(x_1 | x_2) &= \\frac{f_{X_1, X_2}(x_1, x_2)}{f_{X_2}(x_2) }\n  \\\\\n  &= \\frac{1}{\\sqrt{2 π σ_2 (1 - ρ^2)}}\n  \\exp\\left(\n    -\\frac{(x_1 - ρ \\frac{σ_1}{σ_2} x_2)^2}{ 2 σ_1^2 (1 - ρ^2) }\n  \\right).\n\\end{align*}\\] Thus, \\(X_1 \\mid X_2 = x_2\\) is \\(\\mathcal{N}(ρ \\frac{σ_1}{σ_2} x_2, σ_1^2(1 - ρ^2))\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html#conditional-expectation",
    "href": "conditional-expectation.html#conditional-expectation",
    "title": "4  Conditional probability and conditional expectation",
    "section": "4.3 Conditional expectation",
    "text": "4.3 Conditional expectation\n\nWhen \\(X\\) and \\(Y\\) are jointly discrete with joint PMF \\(P_{X,Y}\\), then we can define conditional expectation as \\[\nE[X \\mid Y = y] = \\sum_{x \\in \\ALPHABET X} x P_{X|Y}(x|y)\n\\] and, more generally, for any function \\(g\\), \\[\nE[g(X) \\mid Y = y] = \\sum_{x \\in \\ALPHABET X} g(x) P_{X|Y}(x|y).\n\\]\nAnalogously, \\(X\\) and \\(Y\\) are jointly continuous with joint PDF \\(f_{X,Y}\\), we can define conditional expectation as \\[\nE[X|Y = y] = \\int_{-∞}^{∞} x f_{X|Y}(x|y)\\, dx\n\\] and, more generally, for any function \\(g\\), \\[\nE[g(X)|Y = y] = \\int_{-∞}^{∞} g(x) f_{X|Y}(x|y)\\, dx\n\\]\nNow consider a function \\(g(X,Y)\\). Then, \\[\nE[g(X,Y) \\mid Y = y] = \\int_{-∞}^{∞} g(x,y) f_{X|Y}(x|y)\\, dx\n= E[g(X,y) \\mid Y = y].\n\\]\nObserve that \\[\\begin{align*}\n\\int_{-∞}^{∞} E[g(X,Y) \\mid Y = y ] f_Y(y) dy\n&=\n\\int_{-∞}^{∞} \\int_{-∞}^{∞} g(x,y) f_{X|Y}(x|y) f_Y(y)\\, dx\\, dy \\\\\n&=\n\\int_{-∞}^{∞} \\int_{-∞}^{∞} g(x,y) f_{X,Y}(x,y)\\, dx\\, dy \\\\\n&= \\EXP[g(X,Y)].\n\\end{align*}\\] This is known as the smoothing property of conditional expectation.\nMore precisely, we may think of \\(\\EXP[g(X,Y) | Y = y]\\) as a function of \\(y\\), say \\(h(y)\\). Then \\(h(Y) = \\EXP[g(X,Y) | Y]\\) is a random variable! The smoothing property of conditional expectation says that \\[ \\EXP[ \\EXP[ g(X,Y) | Y ] ] = \\EXP[ g(X,Y) ]. \\] This is a subtle point, and we will spend some time to develop an intuition of what this means.\n\n\n4.3.1 Conditioning on a \\(σ\\)-algebra\nThe key idea is conditioning on a \\(σ\\)-algebra. To avoid technical subtleties, we restrict to the discrete case.\n\nConsider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) where \\(\\ALPHABET F\\) is a finite \\(σ\\)-algebra. Let \\(\\ALPHABET G\\) be a sub-\\(σ\\)-algebra of \\(\\ALPHABET F\\). In particular, we assume that there is a partition \\(\\{D_1, D_2, \\dots, D_m\\}\\) of \\(Ω\\) such that \\(\\ALPHABET G = 2^{\\{D_1, \\dots, D_m\\}}\\). The elements \\(D_1, \\dots, D_m\\) are called the atoms of the \\(σ\\)-algebra \\(\\ALPHABET G\\).\nTODO: Add example. 4x4 grid. partition for \\(\\ALPHABET G\\).\nWe define \\(\\PR(A \\mid \\ALPHABET G)\\) (which we will write as \\(\\EXP[\\IND_{A} \\mid \\ALPHABET G]\\) as \\[\n\\EXP[\\IND_{A} \\mid \\ALPHABET G](ω)\n= \\sum_{i=1}^m \\EXP[ \\IND_{A} \\mid D_i ] \\IND_{D_i}(ω).\n\\] Thus, on each \\(ω \\in D_i\\), the value of \\(\\EXP[I_{A} \\mid \\ALPHABET G]\\) is equal to \\(\\EXP[I_{A} \\mid D_i]\\).\nThis idea can be extended to any random variable instead of \\(\\IND_{A}\\), that is, for any random variable \\(X\\) \\[\n\\EXP[X \\mid \\ALPHABET G](ω)\n= \\sum_{i=1}^m \\EXP[ X \\mid D_i ] \\IND_{D_i}(ω).\n\\] Thus, on each \\(ω \\in D_i\\), the value of \\(\\EXP[X \\mid \\ALPHABET G]\\) is equal to \\(\\EXP[X \\mid D_i]\\).\nWhen \\(\\ALPHABET G = \\{\\emptyset, Ω\\}\\) is the trivial \\(σ\\)-algebra, \\[\\EXP[X \\mid \\{\\emptyset, Ω\\}] = \\EXP[X].\\]\nWhen \\(X = \\IND_{A}\\), \\(\\EXP[ \\IND_{A} \\mid \\ALPHABET G] = \\PR(A \\mid \\ALPHABET G)\\).\nIf \\(X_1\\) and \\(X_2\\) are joint random variables and \\(a_1\\) and \\(a_2\\) are constants, then \\[ \\EXP[ a_1 X_1 + a_2 X_2 \\mid \\ALPHABET G]\n= a_1 \\EXP[X_1 \\mid \\ALPHABET G] + a_2 \\EXP[X_2 \\mid \\ALPHABET G]. \\]\nIf \\(Y\\) is another random variable which is \\(\\ALPHABET G\\) measurable (i.e., \\(Y\\) takes constant values on the atoms of \\(\\ALPHABET G\\)), then \\[\n\\EXP[X Y \\mid \\ALPHABET G] = Y \\EXP[X \\mid \\ALPHABET G].\n\\]\n[The result can be proved pictorially.]\n\n\n\n4.3.2 Smoothing property of conditional expectation\nLet \\(\\ALPHABET H ⊂ \\ALPHABET G ⊂ \\ALPHABET F\\), where \\(⊂\\) means sub-\\(σ\\)-algebra. Let \\(\\{E_1, \\dots, E_k\\}\\) denote the partition corresponding to \\(\\ALPHABET H\\) and \\(\\{D_1, \\dots, D_m\\}\\) denote the partition corresponding to \\(\\ALPHABET G\\). The fact that \\(\\ALPHABET H\\) is a sub-\\(σ\\)-algebra of \\(\\ALPHABET G\\) means that each atom \\(E_i\\) of \\(\\ALPHABET H\\) is a union of atoms of \\(\\ALPHABET G\\) (i.e., \\(\\{D_1, \\dots, D_m\\}\\) is a refinement of the partition \\(\\{E_1, \\dots, E_k\\}\\)). Therefore, \\[\\begin{equation}\\tag{smoothing-property}\n\\bbox[5pt,border: 1px solid]\n{\\EXP[ \\EXP[ X \\mid \\ALPHABET G ] \\mid \\ALPHABET H ] =\n\\EXP[ X \\mid \\ALPHABET H].}\n\\end{equation}\\] This is known as the smoothing property of conditional expectation.\nA special case of the above property is that \\[\n\\EXP[ \\EXP[ X \\mid \\ALPHABET G] ] =\n\\EXP[ X ].\n\\] where we have taken \\(\\ALPHABET H = \\{\\emptyset, Ω\\}\\) as the trivial \\(σ\\)-algebra. Observe that the above definition is equivalent to the law of total probability!\n\n\n4.3.3 Conditioning on random variable\n\nNow suppose \\(Y\\) is a discrete random variable, then \\(\\PR(A \\mid Y)\\) and \\(\\EXP[X \\mid Y]\\) may be viewed as a short-hand notation for \\(\\PR(A \\mid σ(Y))\\) and \\(\\EXP[X \\mid σ(Y)]\\). Similar interpretations hold for conditioning on multiple random variables (or, equivalently, conditioning on random vectors).\nThe smoothing property of conditional expectation can then be stated as \\[\n\\EXP[ \\EXP[ X | Y_1, Y_2 ] Y_1 ] = \\EXP[ X | Y_1 ].\n\\]\nAn implication of the smoothing property is the following: for any (measurable) function \\(g \\colon \\reals \\to \\reals\\), \\[ \\EXP[ g(Y) \\EXP[ X \\mid Y] ] = \\EXP[ X g(Y) ]. \\]\n\n\n\nThis previous property is used for generalizing the definition of conditional expectation to continuous random variables. First, we consider conditioning wrt \\(σ\\)-algebra \\(\\ALPHABET G \\subset \\ALPHABET F\\), which is not necessarily finite (or countable).\nThen, for any non-negative1 random variable \\(X\\), \\(\\EXP[X \\mid \\ALPHABET G]\\) is defined as a \\(\\ALPHABET G\\)-measurable random variable that satisfies \\[ \\EXP[ \\IND_{A} \\EXP[ X \\mid \\ALPHABET G] ] = \\EXP[ X \\IND_{A} ] \\] for every \\(A \\in \\ALPHABET G\\).\n\n1 We start with non-negative random variables just to avoid the concerns with existence of expectation due to \\(∞ - ∞\\) nature. A similar definition works in general as long as we can rule out \\(∞ - ∞\\) case.\nIt can be shown that \\(\\EXP[X \\mid \\ALPHABET G]\\) exists and is unique up to sets of measure zero. Formally, one takes about a “version” of conditional expectation.\nThen \\(\\EXP[X \\mid Y]\\) for \\(Y\\) continuous may be viewed as \\(\\EXP[X \\mid σ(Y)]\\).\nThe formal definition of conditional expectation implies that if we take any Borel subsets \\(B_X\\) of \\(\\reals\\), then \\(\\PR(X \\in B_X \\mid Y)\\) is a (measurable) function \\(m(y)\\) that satisfies \\[\\begin{equation}\\label{eq:defn-cond}\n\\PR(X \\in B_X, Y \\in B_Y) = \\int_{B_Y} m(y) f_Y(y) dy\n\\end{equation}\\] for all Borel subsets \\(B_Y\\) of \\(\\reals\\).\nWe will show that \\[ m(y) = \\int_{B_X} \\frac{ f_{X,Y}(x,y) }{ f_{Y}(y) } \\, dx \\] satisfies \\(\\eqref{eq:defn-cond}\\). In particular, the RHS of \\(\\eqref{eq:defn-cond}\\) is \\[\n\\int_{B_Y} \\left[ \\int_{B_X} \\frac{ f_{X,Y}(x,y) }{ f_{Y}(y) } \\, dx \\right] f_Y(y) \\, dy\n= \\int_{B_Y} \\int_{B_X} f_{X,Y}(x,y) \\, dx \\, dy\n= \\PR(X \\in B_X, Y \\in B_X)\n\\] which equals the LHS of \\(\\eqref{eq:defn-cond}\\). This is why the conditional density is defined the way it is defined!\nFinally, it can be shown that \\(\\PR(A \\mid Y) \\coloneqq \\EXP[\\IND_{A} \\mid σ(Y)]\\), \\(A \\in \\ALPHABET F\\), satisfies the axioms of probability.Therefore, conditional probability satisfies all the properties of probability (and consequently, conditional expectations satisfy all the properties of expectations).\nNote that the definition of conditional expectation generalizes Bayes rule. In particular, for any (measurable) function \\(g \\colon \\reals \\to \\reals\\) we have \\[\\begin{align*}\n\\EXP[ g(X) \\mid Y = y ] &= \\int_{-∞}^∞ g(x) f_{X|Y}(x\\mid y) \\, dx\n\\\\\n&= \\int_{-∞}^∞ g(x) \\frac{f_{X,Y}(x,y)}{f_Y(y)} \\, dx \\\\\n&= \\frac{ \\displaystyle \\int_{-∞}^{∞} g(x) f_{X,Y}(x,y)\\, dx}\n   { f_Y(y)} \\\\\n&= \\frac{ \\displaystyle \\int_{-∞}^{∞} g(x) f_{X,Y}(x,y)\\, dx}\n   {\\displaystyle \\int_{-∞}^{∞} f_{X,Y}(x,y)\\, dx } \\\\\n&= \\frac{ \\displaystyle \\int_{-∞}^{∞} g(x) f_{Y|X}(y|x) f_X(x) \\, dx}\n   {\\displaystyle \\int_{-∞}^{∞} f_{Y|X}(y|x) f_X(x) \\, dx }. \\\\\n  \\end{align*}\\]\n\n\nExample 4.8 Let \\(X\\) and \\(Y\\) be independent and identically distributed \\(\\text{Bernoulli}(p)\\) random variables.\n\nConsider the events \\(A_k = \\{ ω : X(ω) + Y(ω) = k\\}\\), \\(k \\in \\{0,1,2\\}\\). Find \\(\\PR(A_k \\mid X)\\).\nCompute \\(\\EXP[X + Y \\mid X]\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html",
    "href": "moment-generating-functions.html",
    "title": "5  Moment Generating Functions",
    "section": "",
    "text": "5.1 Moment Generating Functions\nThe moment generating function (MGF) of a random variable \\(X\\) is defined as \\[\nM_X(s) = \\EXP[e^{sX}]\n\\] provided the expectation exists.\nThe MGF of common random variables is shown in Table 5.1.\nIf there exist a neighborhood around origin where \\(M_X(s)\\) is well-defined. Then, we can use the MGF to “generate the moments” of \\(X\\) as follows:",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html#moment-generating-functions",
    "href": "moment-generating-functions.html#moment-generating-functions",
    "title": "5  Moment Generating Functions",
    "section": "",
    "text": "When \\(X\\) is discrete, we have \\[ M_X(s) = \\sum_{x \\in \\ALPHABET X} e^{sx} p_X(x). \\]\nWhen \\(X\\) is continuous, we have \\[ M_X(s) = \\int_{-∞}^∞ e^{sx} f_X(x) \\, dx. \\]\n\n\n\n\n\n\n\nImportantRelationship to Laplace transforms\n\n\n\nAlthough most texts (including the textbook) restrict \\(s\\) to be real, my personal view is that one should really interpret \\(s\\) as a complex number. If we do so, then we have the following:\n\n\\(M_X(-s)\\) is the Laplace transform of the PDF.\n\\(M_X(-j ω)\\) is the Fourier transform of the PDF, which is called the characteristic function of \\(X\\).\n\nTherefore, we can recover the PDF by taking the inverse Laplace transform of MGF. Thus, specifying the MGF of a random variable is equivalent to specifying the PDF.\nHistorically, MGF is defined for \\(s \\in \\reals\\) and there are distributions (e.g., Cauchy) for which MGF does not exist for any \\(s \\neq 0\\). In avoid such situations, one uses the characteristic function because the characteristic function always exists. However, if we view the domain of MGF to be \\(\\mathbb{C}\\), then there is no need for a distinction between MGF and characteristic function.\n\n\n\nExample 5.1 Suppose \\(X\\) is a random variable which takes values \\(\\{0, 1, 2\\}\\) with probabilities \\(\\{\\frac 12, \\frac 13, \\frac 16\\}\\). Then, \\[\\begin{align*}\nM_X(s) &= \\EXP[e^{sX}] \\\\\n&= \\frac 12 e^{s 0} + \\frac 13 e^{s 1} + \\frac 16 e^{s 2} \\\\\n&= \\frac 12 + \\frac 13 e^{s} + \\frac 16 e^{2s}.\n\\end{align*}\\]\n\n\nExample 5.2 Find the MGF of a Poisson random variable with parameter \\(λ\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\[\\begin{align*}\nM_X(s) &= \\EXP[e^{sX}] = \\sum_{k=0}^{∞}e^{ks} \\frac{λ^k e^{-λ}}{k!} \\\\\n&= e^{-λ} \\sum_{k=0}^{∞} \\frac{(λe^s)^k}{k!} \\\\\n&= e^{-λ} e^{λ e^{s}}.\n\\end{align*}\\]\n\n\n\n\nExample 5.3 Find the MGF of an exponential random variable with parameter \\(λ\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\[\\begin{align*}\nM_X(s) &= \\EXP[e^{sX}] \\\\\n&= \\int_{0}^∞ e^{sx} λ e^{-λx} \\, dx \\\\\n&= λ \\int_{0}^∞ e^{(s-λ)x} \\, dx \\\\\n&= \\frac{λ}{λ-s}.\n\\end{align*}\\]\nNote that we could have looked up this result from the Laplace transform tables which show that \\[\ne^{at} \\xleftrightarrow{\\hskip 0.5em \\mathcal{L}\\hskip 0.5em } \\frac{1}{s-a}\n\\]\n\n\n\n\n\n\n\nTable 5.1: MGF of common random vables\n\n\n\n\n\n\n\n\n\n\nRandom variable\nParameter(s)\nMGF\n\n\n\n\nBernoulli\n\\(p\\)\n\\(1 - p + p e^s\\)\n\n\nBinomial\n\\((n,p)\\)\n\\((1-p + p e^s)^n\\)\n\n\nGeometric\n\\(p\\)\n\\(\\dfrac{p e^s}{1 - (1-p)e^s}\\)\n\n\nPoisson\n\\(λ\\)\n\\(\\exp(λ (e^s - 1))\\)\n\n\nUniform\n\\((a,b)\\)\n\\(\\dfrac{e^{sb} - e^{sa}}{s(b-a)}\\)\n\n\nExponential\n\\(λ\\)\n\\(\\dfrac{λ}{λ-s}\\)\n\n\nGaussian\n\\((μ,σ)\\)\n\\(\\exp\\bigl(μ s + \\frac 12 σ^2 s^2 \\bigr)\\)\n\n\n\n\n\n\n\n\n\\(M_X(0) = 1\\)\n\\(\\dfrac{d}{ds} M_X(s) \\biggr|_{s=0} = \\EXP[X]\\).\n\\(\\dfrac{d^2}{ds^2} M_X(s) \\biggr|_{s=0} = \\EXP[X^2]\\).\nand in general \\(\\dfrac{d^k}{ds^k} M_X(s) \\biggr|_{s=0} = \\EXP[X^k]\\).\nHence, by Taylor series expansion of \\(M_X(s)\\) within the radius of convergence, we get \\[\nM_X(s) = \\sum_{k=0}^∞ \\frac{\\EXP[X^k]}{k!} s^k.\n\\] Thus, when if the MGF is well defined in a neighborhood around origin, knowing all the moments of a distribution is sufficient to construct the MGF. We already saw that the MGF is sufficient to construct the PDF/PMF of the distribution. Thus, the distribution of a random variable is completely characterized by its moments.\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe first property follows from definition: \\[\nM_X(0) = \\EXP[e^{0 X}] = \\EXP[1] = 1.\n\\]\nFor the general derivative, we have \\[\\begin{align*}\n\\frac{d^k}{ds^k} M_X(s)\n&= \\int_{-∞}^∞ \\frac{d^k}{ds^k} e^{sx} f_X(x) \\, dx \\\\\n&= \\int_{-∞}^∞ x^k e^{sx} f_X(x) \\, dx.\n\\end{align*}\\]\nTherefore \\[\n\\frac{d^k}{ds^k} M_X(s) \\biggr|_{s=0} = \\int_{-∞}^∞ x^k f_X(x) \\, dx.\n\\]\n\n\n\n\nExample 5.4 Use the MGF of Bernoulli to find its first all the moments of \\(X\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFrom Table 5.1, we see that \\[ M_X(s) = 1 - p + p e^{s}. \\] Therefore,\n\n\\(\\dfrac{d}{ds} M_X(s) = p e^s\\).\n\\(\\dfrac{d^2}{ds^2} M_X(s) = p e^s\\).\nand in general \\(\\dfrac{d^k}{ds^k} M_X(s) = p e^s\\).\n\nThus,\n\n\\(\\EXP[X] = \\dfrac{d}{ds} M_X(s) \\biggr|_{s=0} = p\\).\n\\(\\EXP[X^2] = \\dfrac{d^2}{ds^2} M_X(s) \\biggr|_{s=0} = p\\).\nand in general \\(\\EXP[X^k] = \\dfrac{d^k}{ds^k} M_X(s) \\biggr|_{s=0} = p\\).\n\n\n\n\n\n5.1.1 Moment generating functions and sums of independent random variables\n\nTheorem 5.1 Suppose \\(X_1, X_2, \\dots, X_n\\) are independent random variables defined on the same probability space. Let \\[\n  Z = X_1 + X_2 + \\dots + X_n\n\\] Then, \\[\n  M_Z(s) = M_{X_1}(s) M_{X_2}(s) \\cdots M_{X_n}(s).\n\\]\nFurthermore, if the variables are identically distributed, then \\[\n  M_Z(s) = (M_{X_1}(s))^n.\n\\]\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof follows immediately from properties of independent random variables. We prove the result for \\(n=2\\). \\[\nM_Z(s) = \\EXP[e^{sX_1} e^{sX_2}]\n= \\EXP[e^{sX_1} e^{sX_2}] = M_{X_1}(s) M_{X_2}(s).\n\\]\n\n\n\nTheorem 5.1 is a very useful result. An immediate implication of the result is that following:\n\nSum of i.i.d. Bernoulli random variables is a Binomial random variable\nLet \\(X_i \\sim \\text{Ber}(p)\\). Then \\(M_{X_i}(s) = (1 - p + pe^s)\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = (1 - p + pe^s)^n\\).\nSum of independent Binomial random variables with the same \\(p\\) is a Binomial random variable.\nLet \\(X_i \\sim \\text{Binom}(m_i,p)\\). Then \\(M_{X_i}(s) = (1 - p + p e^s)^{m_i}\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = (1 - p + p e^s)^M\\), where \\(M = \\sum_{i=1}^n m_i\\).\nSum of independent Poisson random variables is Poisson.\nLet \\(X_i \\sim \\text{Pois}(λ_i)\\). Then \\(M_{X_i}(s) = e^{λ_i(e^s - 1)}\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = e^{λ(e^s - 1)}\\), where \\(λ = \\sum_{i=1}^n λ_i\\).\nSum of independent Gaussian random variables is Gaussian.\nLet \\(X_i \\sim \\mathcal N(μ_i, σ_i)\\). Then, \\(M_{X_i}(s) = \\exp( μ_i s + \\frac 12 σ_i^2 s^2)\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = \\exp(μ s + \\frac 12 σ^2 s^2)\\), where \\[\n   μ = \\sum_{i=1}^n μ_i\n   \\quad\\text{and}\\quad\n   σ^2 = \\sum_{i=1}^n σ_i^2.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html#the-central-limit-theorem",
    "href": "moment-generating-functions.html#the-central-limit-theorem",
    "title": "5  Moment Generating Functions",
    "section": "5.2 The Central Limit Theorem",
    "text": "5.2 The Central Limit Theorem\nOne of the ways in which MGFs are useful is that they allow us to understand the limiting behavior of sum of i.i.d. random variables.\n\n\n\n\n\n\nTipConvergence in distribution\n\n\n\nA sequence of random variables \\(\\{X_n\\}_{n \\ge 1}\\) is said to converge in distribution to a random variable \\(X\\) (denoted by \\(X_n \\xrightarrow{D} X\\)) if \\[\n\\lim_{n \\to \\infty} F_{X_n}(x) = F_X(x),\n\\] for all \\(x\\) where \\(F_X\\) is continuous.\n\n\n\nExample 5.5 Consider a sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables where \\(X_n \\sim \\mathcal{N}(0,1/n)\\). Show that \\(X_n \\xrightarrow{D} 0\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nLet \\(F\\) denote the CDF of the constant random variable \\(X=0\\), i.e., \\[\nF(x) = \\begin{cases}\n0, & x &lt; 0 \\\\\n1, & x \\ge 0\n\\end{cases}\n\\]\nLet \\(Z\\) denote the standard Gaussian random variable. Then \\[\nF_{X_n}(x) = \\PR(X_n \\le x) = \\PR(Z \\le \\sqrt{n} x)\n\\to \\begin{cases}\n1, & x &gt; 0 \\\\\n0, & x &lt; 0\n\\end{cases}\n\\] Thus, \\(F_{X_n}(x)\\) converges to \\(F(x)\\) for all \\(x \\neq 0\\). Recall that the definition of convergence in distribution, does not require convergence of \\(F_{X_n}(x)\\) at points of discontinuity of \\(F\\). So, \\(X_n \\xrightarrow{D} 0\\).\n\n\n\nAn important implication of convergence in distribution is that for any continuous bounded function \\(g\\) \\[\n\\EXP[g(X_n)] \\to \\EXP[g(X)].\n\\] For this reason, convergence in distribution is sometimes called weak convergence.\nThe relationship between PDFs and MGFs implies the following continuity theorem:\n\n\n\n\n\n\nImportantContinuity Theorem\n\n\n\nConsider a sequence of random variables \\(\\{X_n\\}_{n \\ge 1}\\). For ease of notation, we use \\((F_n, M_n)\\) to denote the CDF and MGF of \\(X_n\\).\n\nIf \\(F_n \\to F\\) for some CDF \\(F\\) with MGF \\(M\\), then \\(M_n(s) \\to M(s)\\) for all \\(s\\).\nConversely, if \\(M(s) = \\lim_{n\\to ∞} M_n(s)\\) exists and is continuous at \\(s = 0\\), then \\(M\\) is the MGF of some CDF \\(F\\) and \\(F_n \\to F\\).\n\n\n\nThe proof is a bit involved and technical but makes sense from a signal processing point of view: we know that if a sequence of signals \\(x_n \\to x\\), then \\(\\mathcal L(x_n) \\to \\mathcal L(x)\\).\n\n5.2.1 Two limit theorems\nWe now prove two limit theorems using the moment generating function.1\n1 Traditionally, these results are proved via the characteristic function and not the MGF. However, as discussed above, my personal view is that we can do everything with MGFs if we define them over complex numbers (as is done in Signals and Systems) rather than real numbers (as is done in probability theory).\nTheorem 5.2 (A law of large numbers) Let \\(\\{X_n\\}_{n \\ge 1}\\) be a sequence of i.i.d. random variables with finite mean \\(μ\\). Then, their partial sums \\(S_n = X_1 + \\cdots + X_n\\) satisfy \\[\n\\frac{1}{n} S_n \\xrightarrow{D} μ\n\\]\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe theorem asserts that as \\(n \\to ∞\\), \\[\n\\PR(n^{-1}S_n \\le x) =\n\\begin{cases}\n  0 & \\hbox{if } x &lt; μ, \\\\\n  1 & \\hbox{if } x \\ge μ.\n\\end{cases}\n\\]\nLet \\(M_X\\) denote the MGF of \\(X_n\\). From Theorem 5.1, we know that \\[\nM_{n^{-1} S_n}(s) = M_X(s/n)^n.\n\\]\nFrom the Taylor series expansion of \\(M_X(s)\\), we know that \\[\nM_X(s/n) = 1 + μ s + o(s).\n\\] Therefore, \\[M_{n^{-1} S_n}(s) = \\left(1 + \\frac{μs}{n} + o\\left(\\frac{s}{n}\\right) \\right)^n \\to \\exp(μ s)\n\\quad \\text{as} \\quad n \\to ∞,\n\\] which is the MGF of a constant random variable.\n\n\n\nWe will show later that convergence in distribution to a constant implies converge in probability. Therefore, the above implies the weak law of large numbers.\nThe above result shows that when \\(n\\) is large, the sum \\(S_n\\) is approximately the same as \\(n μ\\). The answer to this is provided by the Central Limit Theorem, which asserts that when \\(X_n\\) has finite variance \\(σ\\):\n\n\\(S_n - n μ\\) is about as big as \\(\\sqrt{n}\\).\nIrrespective of the distribution of \\(X_n\\), \\((S_n - n μ)/\\sqrt{n}\\) converges in distribution to a normal distribution with variance \\(σ\\).\n\nWe prove the result below.\n\nTheorem 5.3 (Central Limit Theorem) Let \\(\\{X_n\\}_{n \\ge 1}\\) be i.i.d. random variables with mean \\(μ\\) and finite non-zero variance \\(σ^2\\). Then, their partial sums \\(S_n = X_1 + \\cdots + X_n\\) satisfy \\[\n\\frac{S_n - n μ}{\\sqrt{n}\\, σ}\n\\xrightarrow{D}\n{\\cal N}(0,1).\n\\]\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof idea is similar to that of Theorem 5.2. Consider the “normalized” sequence of random variables \\(Y_n = (X_n - μ)/σ\\), which have mean zero and unit variance. Let \\(M_Y\\) denote their MGF. Then, the Taylor series expansion of \\(M_Y\\) gives \\[ M_Y(s) = 1 + \\frac{1}{2} s^2 + o(s^2). \\] Moreover, observe that \\[\nZ_n \\coloneqq \\frac{S_n - n μ }{\\sqrt{n}\\, σ}\n= \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n Y_i.\n\\] Therefore, by Theorem 5.1, we have \\[\nM_{Z_n}(s) = M_Y(s/\\sqrt{n})^n\n= \\left(1 + \\frac {s^2}{2n} + o\\left( \\frac {s^2}{n} \\right) \\right)^n\n\\to \\exp(-\\tfrac 12 s^2),\n\\] which is the MGF of \\({\\cal N}(0,1)\\).\n\n\n\nThe central limit theorem is one of the cornerstones of probability theory. The earliest statement of the result goes back to de Moivre (1773), but there was little follow up on that until Laplace Théorie analytique des probabilités, (1812). The term “central limit theorem” is due to Poyla (1920) who called is such because the limit theorem plays a “central role in probability theory”.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html#moment-generating-function-of-random-vectors",
    "href": "moment-generating-functions.html#moment-generating-function-of-random-vectors",
    "title": "5  Moment Generating Functions",
    "section": "5.3 Moment generating function of random vectors",
    "text": "5.3 Moment generating function of random vectors\nFor random vectors, \\(X \\in \\reals^n\\), the MGF is a function \\(M_X \\colon \\mathbb{C}^n \\to \\mathbb{C}\\) and for any \\(s \\in \\mathbb{C}^n\\) is given by\n\\[M_X(s) = \\EXP[e^{ \\langle s, X \\rangle }]. \\]\nTODO: Multivariate Gaussian. Relationship with correlation and covariance.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "inequalities.html",
    "href": "inequalities.html",
    "title": "6  Probability inequalities",
    "section": "",
    "text": "6.1 Union Bound\nWe already proved this when talking about probability spaces.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#union-bound",
    "href": "inequalities.html#union-bound",
    "title": "6  Probability inequalities",
    "section": "",
    "text": "TipUnion bound\n\n\n\nLet \\(A_1, \\dots, A_n\\) be a collection of events. Then, \\[\n\\PR\\biggl( \\bigcup_{i=1}^n A_i \\biggr)\n\\le\n\\sum_{i=1}^n \\PR(A_n).\n\\]\n\n\n\n\nExercise 6.1 Let \\(X_1, \\dots, X_n\\) be i.i.d. random variables with CDF \\(F_X(x)\\). Find an upper bound on the CDF of \\[\nZ_n = \\min(X_1, \\dots, X_n).\n\\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe know that \\[Z_n = \\min(X_1, \\dots, X_n) \\le z\\] if and only if \\[ X_i \\le Z \\quad \\forall i \\in \\{1, \\dots, n\\}.\\] Therefore, \\[\\PR(Z_n \\le z) = \\PR\\biggl( \\bigcup_{i=1}^n \\{X_i \\le z \\} \\biggr)\n\\le \\sum_{i=1}^n \\PR(X_i \\le z) = n F_X(z).\n\\]\n\n\n\n\nExercise 6.2 Let \\(\\{A_i\\}_{i = 1}^{∞}\\) be a countably infinite sequence of events. Show that\n\nIf \\(\\PR(A_i) = 0\\) for all \\(i \\in \\naturalnumbers\\), then \\[ \\PR\\Bigl( \\bigcup_{i=1}^{∞} A_i \\Bigr) = 0. \\]\nIf \\(\\PR(A_i) = 1\\) for all \\(i \\in \\naturalnumbers\\), then \\[ \\PR\\Bigl( \\bigcap_{i=1}^{∞} A_i \\Bigr) = 1. \\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#cauchy-schwartz-inequality",
    "href": "inequalities.html#cauchy-schwartz-inequality",
    "title": "6  Probability inequalities",
    "section": "6.2 Cauchy-Schwartz inequality",
    "text": "6.2 Cauchy-Schwartz inequality\nThis is a generalization of :Cauchy-Schwartz inequality on inner products to random variables.\n\n\n\n\n\n\nTipCauchy-Schwartz inequality\n\n\n\nLet \\(X\\) and \\(Y\\) be real-valued random variables. Then, \\[\n\\EXP[XY]^2 \\le \\EXP[X^2] \\EXP[Y^2].\n\\] or equivalently, \\[\n\\COV(X,Y)^2 \\le \\VAR(X) \\VAR(Y).\n\\]\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nDefine \\(f(s) = \\EXP[ (sX + Y)^2\\) for any \\(s \\in \\reals\\). Then, by linearity of expectation, we have \\[\nf(s) = \\EXP[ (sX + Y)^2 ] = s^2 \\EXP[X^2] + 2s \\EXP[XY] + \\EXP[Y^2].\n\\] We know that \\(f(s) \\ge 0\\). Recall that a quadratic form \\[\nA s^2 + B s + C \\ge 0\n\\] for all values of \\(s\\) if and only it has repeated or complex roots. Thus, the determinant must be non-positive, that is \\[\nΔ = B^2 - 4AC \\le 0.\n\\] The result follows by taking \\(A = \\EXP[X^2]\\), \\(B = 2 \\EXP[XY]\\), and \\(C = \\EXP[Y^2]\\).\n\n\n\n\nExample 6.1 Let \\(X \\ge 0\\) be a random variable with mean \\(μ\\) and finite variance \\(σ^2\\). Given \\(λ \\in (0,1)\\), define \\[A = \\{ ω : X(ω) \\ge λ μ \\}.\\]\n\nShow that \\[\\EXP[ X \\IND_{A} ] \\ge (1 -λ) μ. \\]\nHint: Note that \\(X = X (\\IND_A + \\IND_{A^c})\\). What do we know about \\(X\\) when \\(ω \\in A^c\\)?\nUsing the result of part (a), show that \\[\n  \\PR(X \\ge λ μ) \\ge (1-λ)^2 \\frac{ \\EXP[X]^2 }{\\EXP[X^2]}\n  = (1-λ)^2 \\frac{μ^2}{μ^2 + σ^2}.   \n\\]\nHint: Use Cauchy Schwartz to bound \\(\\EXP[X \\IND_A]\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#jensens-inequality",
    "href": "inequalities.html#jensens-inequality",
    "title": "6  Probability inequalities",
    "section": "6.3 Jensen’s inequality",
    "text": "6.3 Jensen’s inequality\nIf we take \\(Y = 1\\) in Cauchy-Schwartz inequality, we get that \\[\n\\EXP[X^2] \\ge \\EXP[X]^2.\n\\] This also follows from the fact that \\(\\VAR(X) \\ge 0\\). Jensen’s inequality may be thought as a generalization of this to convex functions.\n\n\n\n\n\n\nTipJensen’s inequality\n\n\n\nSuppose \\(X\\) is a real-valued random variable. Then for any convex \\(g \\colon \\reals \\to \\reals\\), we have \\[\n\\EXP[ g(X) ] \\ge g(\\EXP[X]).\n\\]\nMoreover, for any concave \\(g \\colon \\reals \\to \\reals\\), we have \\[\n\\EXP[ g(X) ] \\le g(\\EXP[X]).\n\\]\n\n\nFor example, since \\(g(x) = 1/x\\) is convex, we have \\[\n\\EXP\\left[\\frac 1x\\right] \\ge \\frac{1}{\\EXP[X]}.\n\\] Moreover, since \\(g(x) = \\log(x)\\) is concave, we have \\[\n\\EXP[ \\log(X)] \\le \\log(\\EXP[X]).\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.1: Tangent at the mean of \\(X\\)\n\n\n\nLet \\(μ = \\EXP[X]\\). Consider the function \\(L\\) defined in Figure 6.1, which is the tangent at \\(μ\\) on \\(g(x)\\). By construction, \\(g(x) \\ge L(x)\\). Therefore, \\[\n  \\EXP[g(X)] \\ge \\EXP[L(X)]\n  = \\EXP[aX + b] = a μ + b = L(μ) = g(μ).\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#markov-inequality",
    "href": "inequalities.html#markov-inequality",
    "title": "6  Probability inequalities",
    "section": "6.4 Markov inequality",
    "text": "6.4 Markov inequality\n\n\n\n\n\n\nTipMarkov inequality\n\n\n\nFor any non-negative random variable \\(X\\) and a number \\(a &gt; 0\\), \\[\n\\PR(X \\ge a) \\le \\frac{\\EXP[X]}{a}.\n\\]\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nDefine \\(Z = \\IND\\{X \\ge a\\} a\\), which moves all the “mass” of \\(X\\) to the left.\n\nThus, \\[\n\\EXP[X] \\ge \\EXP[Z] = a \\PR(X \\ge a).\n\\]\n\n\n\n\nExercise 6.3 Suppose \\(X \\sim \\text{Unif}(0,1)\\). Verify Markov inequality for \\(\\PR(X \\ge 0.2)\\), \\(\\PR(X \\ge 0.5)\\), and \\(\\PR(X \\ge 0.8)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe table below compares the actual tail probability with the bound obtained from Markov inequality.\n\n\n\n\\(a\\)\n\\(\\PR(X \\ge a)\\)\n\\(\\EXP[X]/a\\)\n\n\n\n\n\\(0.2\\)\n\\(0.8\\)\n\\(0.5/0.2 = 2.5\\)\n\n\n\\(0.5\\)\n\\(0.5\\)\n\\(0.5/0.5 = 1\\)\n\n\n\\(0.8\\)\n\\(0.2\\)\n\\(0.5/0.8 = .625\\)\n\n\n\n\n\n\nExercise 6.3 shows that the Markov inequality is not tight. Moreover, it gives a vaccous bound for \\(a &lt; μ\\). Thus, it only makes sense to apply the Markov inequality of \\(a \\ge μ\\).\n\n6.4.1 Union bound as a special case of Markov inequality\nNote that it is possible to derive the union bound as a corrollary of Markov inequality. Given events \\(A_1, \\dots, A_n\\), define the random variables \\(X_1, \\dots, X_n\\) as \\[\nX_i(ω) = \\IND_{A_i}(ω) = \\begin{cases}\n1, & ω \\in A_i \\\\\n0, & ω \\not\\in A_i\n\\end{cases}.\n\\] Let \\[ X = X_1 + \\dots + X_n. \\] \\(X_i\\) takes the value \\(1\\) when event \\(A_i\\) occurs. Therefore, the event \\(\\bigcup_{i=1}^n A_i\\) is equal to the event \\(X \\ge 1\\), i.e., \\[\n\\PR\\left(\\bigcup_{i=1}^n A_i \\right)\n= \\PR(X \\ge 1).\n\\] Now, by Markov inequality, we have \\[\n\\PR(X \\ge 1) \\le \\EXP[X] = \\PR(A_1) + \\cdots + \\PR(A_n).\n\\] The union bound follows by combining the above equations.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#chebyshev-inequality",
    "href": "inequalities.html#chebyshev-inequality",
    "title": "6  Probability inequalities",
    "section": "6.5 Chebyshev inequality",
    "text": "6.5 Chebyshev inequality\n\n\n\n\n\n\n\nTipChebyshev inequality\n\n\n\nLet \\(X\\) be a real-valued random variable with mean \\(μ\\). Then for any \\(a &gt; 0\\), we have \\[\n\\PR( \\ABS{X - μ} \\ge a) \\le \\frac{ \\VAR(X) }{a^2}.\n\\]\nAn alternative form of is as follows. Let \\(a = k σ\\). Then, \\[\n\\PR(\\ABS{X - μ} \\ge k σ) \\le \\frac{1}{k^2}.\n\\]\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nObserve that \\[\\begin{align*}\n\\PR( \\ABS{X - μ} \\ge a) &= \\PR((X - μ)^2 \\ge a^2)\n\\\\\n&\\stackrel{(a)}\\le \\frac{\\EXP[ (X-μ)^2 ]}{a^2} = \\frac{\\VAR(X)}{a^2}.\n\\end{align*}\\] where \\((a)\\) follows from Markov inequality.\n\n\n\nIn the following example, we compare the strength of Chebyshev inequality compared to that of Markov inequality.\n\nExercise 6.4 Let \\(X \\sim \\text{Bin}(n,p)\\) and consider any \\(q \\in (p,1)\\). Use both Markov and Chebyshev inequalities to bound \\(\\PR(X \\ge nq)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall that \\(\\EXP[X] = np\\) and \\(\\VAR(X) = n p(1-p)\\). Therefore, from Markov inequality we have \\[\n\\PR(X \\ge nq) \\le \\frac{np}{nq} = \\frac{p}{q}.\n\\] Therefore, Markov inequality does not suggest any form of concentration with \\(n\\).\nWe now consider Chebyshev inequality. To do so, we first massage the expression a bit \\[\\begin{align*}\n\\PR(X \\ge nq) &= \\PR( X - np \\ge n(q-p) ) \\\\\n&\\le \\PR( \\ABS{X - np} \\ge n(q-p) )\n\\le \\frac{np(1-p)}{n^2(q-p)^2} = \\frac{1}{n} \\cdot \\frac{p(1-p)}{(q-p)^2}\n\\end{align*}\\] which shows that \\(\\PR(X \\ge nq)\\) gets smaller as \\(n\\) increases.\n\n\n\nWe can use Chebyshev inequality to prove the weak law of large numbers.\n\n\n\n\n\n\nTipWeak law of large numbers\n\n\n\nLet \\(X_1, X_2, \\dots\\) be independent random variables with \\(\\EXP[X_n] = μ\\) and \\(\\VAR(X_n) = σ^2\\). Let \\[\n\\bar X_n = \\frac 1n \\sum_{i=1}^n X_i\n\\] be the sample mean. Then, \\[\\PR(\\ABS{X_n - μ} &gt; ε) \\le \\frac{σ^2}{n ε^2}. \\]\nTherefore, \\(X_n \\xrightarrow{p} μ\\) (which reads as \\(X_n\\) converges in probability to \\(μ\\); we will study convergence in probability in next lecture).\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nObserve that \\[\\begin{align*}\n\\EXP[\\bar X_n] &= \\frac{1}{n} \\sum_{i=1}^n \\EXP[X_i] = μ \\\\\n\\VAR(\\bar X_n) &= \\frac{1}{n^2} \\sum_{i=1}^n \\VAR(X_i) = \\frac{σ^2}{n}.\n\\end{align*}\\]\nThen, by Chebyshev inequality, we have \\[\n\\PR(\\ABS{X_n - μ} &gt; ε) \\le \\frac{\\VAR(\\bar X_n)}{ε^2} = \\frac{σ^2}{n ε^2}.\n\\]\n\n\n\nIf we do not know \\(\\VAR(X)\\), we can still use Chebyshev inequality with an upper bound on \\(\\VAR(X)\\). For example, if \\(X \\in [a,b]\\), then we can show that \\[\n\\VAR(X) \\le \\frac{(b-a)^2}{4}.\n\\] So, for any random variable \\(X \\in [a,b]\\), we have that \\[\n\\PR(\\ABS{X - μ} &gt; ε) \\le \\frac{(b-a)^2}{4 ε^2}.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#chernoff-bound",
    "href": "inequalities.html#chernoff-bound",
    "title": "6  Probability inequalities",
    "section": "6.6 Chernoff bound",
    "text": "6.6 Chernoff bound\n\n\n\n\n\n\nTipChernoff Bound\n\n\n\nLet \\(X\\) be a real-valued random variable. Then, for any \\(a &gt; 0\\), we have \\[\n\\PR(X \\ge a) \\le e^{-φ(a)}\n\\] where \\[\nφ(a) = \\max_{s &gt; 0} \\{ s a - \\log M_X(s) \\}\n\\] is the Lengendre-Fenchel transform of \\(\\log M_X(s)\\).\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof relies on two observations. First, for any \\(s &gt; 0\\), \\(e^s x\\) is an increasing function of \\(x\\). Therefore, \\[\n\\{ ω : X(ω) &gt; a \\} = \\{ ω : e^{sX(ω)} &gt; e^{sa} \\}.\n\\] Hence, \\[\n\\PR(X &gt; a) = \\PR(e^{sX} \\ge e^{sa})\n\\le \\frac{\\EXP[e^{sX}}{e^{sa}}\n= e^{-( sa - \\log M_X(s) ) }\n\\] where the inequality follows from Markov inequality\nSecond, observe that the above inequality holds for every \\(s &gt; 0\\). So, to get the tightest bound, we minimize the RHS over all \\(s &gt; 0\\), i.e., \\[\n\\PR(X &gt; a) = \\PR(e^{sX} \\ge e^{sa})\n\\le \\min_{s &gt; 0} e^{-( sa - \\log M_X(s) ) }\n\\] Since \\(e^x\\) is increasing in \\(x\\), the minimizer above is the same as the maximizer for \\[\nφ(a) = \\max_{s &gt; 0} \\{ sa - \\log M_X(s) \\}.\n\\]\n\n\n\nChernoff bound is much stronger than Markov and Chebyshev inequalities. To see that, let’s revisit the bound of Exercise 6.4. For a Binomial random variable, \\[\nM_X(s) = (1 - p + p e^s)^n.\n\\] Then, \\[\\PR(X \\ge nq) \\le \\exp\\left( - \\max_{s \\ge 0} \\Big[ sn q - n \\log(1-p + p e^s) \\Big] \\right)\n= \\exp\\left( - n \\max_{s \\ge 0} \\Big[ s q - \\log(1 - p + p^s) \\Big] \\right).\n\\] Even before we go ahead and compute the value in maximum of the term in square brackets, we observe that the result here is qualitatively different from that in Exercise 6.4. As for Chebyshev inequality, we get that the probability is going to zero, but Chernoff bound shows that the probability is going to zero exponentially fast. A bit of algebra shows that the exact bound is \\[\n\\PR(X \\ge nq) \\le \\exp\\bigl( - n D(p \\| q) \\bigr),\n\\] where \\[\nD(p \\| q) = q \\log \\left( \\frac{q}{p} \\right) + (1-q) \\log \\left( \\frac{1-q}{1 - p} \\right).\n\\]\nWe now present another example to show the tightness of the Chernoff bound.\n\nExercise 6.5 Use the Chernoff bound to compute a tail bound on Gaussian random variable, i.e., for any \\(ε &gt; 0\\), bound \\(\\PR(X \\ge 0)\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#azuma-hoeffding-inequality",
    "href": "inequalities.html#azuma-hoeffding-inequality",
    "title": "6  Probability inequalities",
    "section": "6.7 Azuma-Hoeffding inequality",
    "text": "6.7 Azuma-Hoeffding inequality\nAlthough Chernoff bound is fairly tight, one of the drawbacks is that it requires the knowledge of the MGF of \\(X\\). This is in contrast to Markov and Chebyshev inequalities which only require knowledge of the mean and variance. For sums of i.i.d. random variables, it is possible to get a tight bound that only depends on the mean (and a proxy for variance).\n\n\n\n\n\n\nTipAzuma-Hoeffding inequality\n\n\n\nLet \\(X_1, \\dots, X_n\\) be i.i.d. random variables such that \\(X_i \\in [a,b]\\) and \\(\\EXP[X_i] = μ\\). Define \\[\n\\bar X_n = \\frac 1n \\sum_{i=1}^n X_i.\n\\] Then, \\[ \\PR( \\bar X_n - μ  &gt; ε ) \\le e ^ {- 2 ε^2 n } \\] and \\[ \\PR( \\ABS{\\bar X_n - μ}  &gt; ε ) \\le 2 e ^ {- 2 ε^2 n } \\]\n\n\nWe will not provide a proof of this inequality. The Azuma-Hoeffding inequality can also be interpreted as follows. For any \\(δ &gt; 0\\), we have that the true mean lies in the interval \\[\n\\left[ \\bar X_n - \\sqrt{ \\frac{1}{2n} \\log \\frac 2 {δ} },\n       \\bar X_n + \\sqrt{ \\frac{1}{2n} \\log \\frac 2 {δ} } \\right]\n\\]\nWe can revisit Exercise 6.4 using Hoeffding inequality. Recall that a Binomial random variable is the sum of i.i.d. Bernoulli random variables. Therefore, we have \\[\n\\PR( X - np \\ge n (q-p) ) = \\PR( \\bar X_n - p \\ge (q - p) )\n\\le e^{-2 (q - p)^2 n}.\n\\]\nThe bound is weaker than what we obtained via Chernoff bound, but it only requires the first moment and the fact that the random variables are bounded.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html",
    "href": "convergence-of-random-variables.html",
    "title": "7  Convergence of random variables",
    "section": "",
    "text": "7.1 Different modes of convergence\nSuppose we have an infinite sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\). Thus, for every \\(ω \\in Ω\\), there is an infinite sequence \\[\n  X_1(ω), X_2(ω), X_3(ω), \\dots\n\\] A sequence of random variables, also called a stochastic process, can be thought of a generalization of random vectors. When does this sequence converge?",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#different-modes-of-convergence",
    "href": "convergence-of-random-variables.html#different-modes-of-convergence",
    "title": "7  Convergence of random variables",
    "section": "",
    "text": "Recall that a sequence \\(\\{x_n\\}_{n \\ge 1}\\) of real numbers converges to a limit \\(x\\) if for every \\(ε &gt; 0\\), there exists a \\(N\\) such that for all \\(n \\ge N\\), we have that \\[ \\ABS{ x_n - x } &lt; ε. \\]\nThe simplest way to define convergence of a sequence of random variables as follows: a sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to a limit \\(X\\) surely if for every \\(ω \\in Ω\\), the sequence \\(\\{X_n(ω)\\}_{n \\ge 1}\\) of real numbers converges to \\(X(ω)\\).\nSure convergence can be too strong, as is illustrated by the following example.\n\n\nExample 7.1 Consider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) where \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr{B}(0,1)\\), and \\(\\PR\\) is the uniform distribution on \\(Ω\\). Define \\(X_n(ω) = \\IND_{A_n}(ω)\\) where \\(A_n = [0, \\frac 1n]\\), i.e., \\[\nX_n(ω) = \\begin{cases}\n1, & ω \\in [0, \\frac{1}{n}] \\\\\n0, & ω \\in (\\frac{1}{n}, 1]\n\\end{cases}\n\\]\n\nviewof ω_convergence_1 = Inputs.range([0.0, 1], {label: \"ω\", value:0.05, step: 0.01})\n\npoints_convergence_1 = {\n    const N = 120\n    var points = new Array(N)\n\n    for(var n=1; n &lt;= N; n++) {\n      var val = (ω_convergence_1*n &lt;= 1) ? 1 : 0 ;\n      points[n-1] = { n: n, X_n: val }\n    }\n    return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  y : {ticks: 1 },\n  height: 100,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.ruleX(points_convergence_1, {x: \"n\", y: \"X_n\", strokeWidth: 1}),\n    Plot.dot(points_convergence_1, {x: \"n\", y: \"X_n\", fill: \"currentColor\", r: 2}),\n  ]\n})\n\n\n\n\n\n\nShow that \\(\\{X_n\\}_{n \\ge 1}\\) does not converge in the sure sense.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFor any \\(ω \\in (0,1]\\), \\(X_n(ω) = 1\\) for \\(n &lt; \\lfloor 1/n \\rfloor\\), and \\(0\\) afterwards. Thus, \\(X_n(ω) \\to 0\\).\nHowever, for \\(ω = 0\\), \\(X_n(ω) = 1\\) for all \\(n\\). Thus, \\(X_n(ω) \\to 1\\).\n\n\n\n\nFrom a practical point of view, we do not care about not converging at \\(ω = 0\\), because that is an event of zero probability. Stated differently, the set \\[\n  \\{ ω : X_n(ω) \\to 0 \\}\n\\] has probability 1.\nBased on the previous discussion, we can relax the notion of sure convergence to almost sure convergence. A sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to a random variable \\(X\\) almost surely if \\[\n  \\PR\\left( \\left\\{ ω : \\lim_{n \\to ∞}\n   X_n(ω) = X(ω) \\right\\} \\right) = 1\n\\] Or, equivalently, for any \\(ε &gt; 0\\), \\[\n  \\PR\\left( \\limsup_{n \\to ∞} \\{ ω : | X_n(ω) - X(ω) | &gt; ε \\}  \\right) = 0\n\\] We denote such convergence as \\(X_n \\xrightarrow{a.s.} X\\).\nAnother way to generalize the definition of convergence of a sequence of real numbers to that of random variables is called convergence in probability. A sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to a random variable \\(X\\) in probability if \\[\n  \\lim_{n \\to ∞} \\PR( \\ABS{ X_n - X } &gt; ε ) = 0.\n\\] Or, equivalently, for any \\(ε &gt; 0\\) and \\(δ &gt; 0\\), there exists a \\(N\\) such that for all \\(n \\ge N\\), we have \\[\n  \\PR( \\ABS{ X_n - X} &gt; ε ) \\le δ.\n\\] We denote such convergence as \\(X_n \\xrightarrow{p} X\\).\nAlmost sure convergence implies convergence in probability, i.e., \\[\n  [X_n \\xrightarrow{a.s.} X]\n  \\implies\n  [X_n \\xrightarrow{p} X]\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nFix \\(ε &gt; 0\\). Define \\[\nA_n = \\{ ω : \\exists m \\ge n, \\ABS{X_m(ω) - X} \\ge ε \\}.\n\\] Then, \\(\\{A_n\\}_{n \\ge 1}\\) is a decreasing sequence of events. If \\(ω \\in \\bigcap_{n \\ge 1} A_n\\), then \\(X_n(ω) \\not\\xrightarrow{a.s} X(ω)\\). This implies \\[\\PR\\Bigl( \\bigcap_{n \\ge 1} A_n \\Bigr) \\le\n  \\PR\\Bigl( \\Bigl\\{ ω : \\lim_{n \\to ∞}X_n(ω) \\neq X(ω) \\Bigr\\}\\Bigr)\n= 0. \\] By continuity of probability, \\[\n\\lim_{n \\to ∞} \\PR(A_n) = \\PR\\Bigl( \\lim_{n \\to ∞} A_n \\Bigr) = 0.\n\\]\n\n\n\nHowever, the converse is not true. Convergence in probability does not imply almost sure convergence, as is illustrated by the following example. An easier to understand example is presented in Example 7.5, part (b).\n\n\nExample 7.2 Consider a probability space as in Example 7.1. Define \\(X_n(ω) = \\IND_{A_n}(ω)\\), where where \\(A_n = \\{ ω : n ω - \\lfloor n ω \\rfloor \\in [0, \\frac 1n] \\}\\).\n\nviewof ω_convergence_2 = Inputs.range([0.0, 1], {label: \"ω\", value:0.05, step: 0.01})\n\npoints_convergence_2 = {\n    const N = 120\n    var points = new Array(N)\n\n    for(var n=1; n &lt;= N; n++) {\n      var val = ((ω_convergence_2*n % 1)*n &lt;= 1) ? 1 : 0 ;\n      points[n-1] = { n: n, X_n: val }\n    }\n    return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  y : {ticks: 1 },\n  height: 100,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.ruleX(points_convergence_2, {x: \"n\", y: \"X_n\", strokeWidth: 1}),\n    Plot.dot(points_convergence_2, {x: \"n\", y: \"X_n\", fill: \"currentColor\", r: 2}),\n  ]\n})\n\n\n\n\n\n\nShow that \\(\\{X_n\\}_{n \\ge 1}\\) converges in probability but not almost surely.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nNote that \\(X_n(ω) \\in \\{0, 1\\}\\). Therefore, the sequence \\(\\{X_n(ω)\\}_{n \\ge 1}\\) of real numbers converges if and only if there exists an \\(N(ω)\\) such that \\(X_n(ω)\\) takes a constant value for all \\(n \\ge N(ω)\\). However, as can be seen from the above figure, for any value of \\(ω \\neq 0\\), \\(X_n(ω)\\) takes both values \\(0\\) and \\(1\\) infinitely often. Therefore, \\(X_n(ω)\\) does not converge to a limit.\nFor \\(ω = 0\\), \\(X_n(ω) = 1\\), and therefore \\(X_n(ω) \\to 1\\). However, \\(\\PR(\\{ω = 0 \\}) = 0\\). Hence, the sequence \\(\\{X_n(ω)\\}_{n \\ge 1}\\) does not converge almost surely.\nNow fix an \\(ε &gt; 0\\) and consider the set \\[\n  E_n = \\{ ω : |X_n(ω) - 0| &gt; ε \\}.\n\\] If \\(ε &gt; 1\\), then \\(E_n = \\emptyset\\), and \\(\\PR(E_n) = 0\\) for all \\(n\\). So, we assume that \\(ε &lt; 1\\). Then, \\[\n  E_n = \\{ ω : X_n(ω) = 1 \\}.\n\\] Note that \\(X_n(ω) = 1\\) for roughly \\(1/n\\) fraction of points (One can make this argument formal by showing the sequence \\(X_n(ω)\\) is equidistributed sequence modulo \\(1\\) by using Weyl’s criterion). So, \\[\n  \\PR(E_n) = \\PR(X_n = 1) = \\frac 1n.\n\\] Thus, \\[\n  \\lim_{n \\to ∞} \\PR(E_n) = 0.\n\\] Hence, \\(\\{X_n\\}\\) converges in probability.\n\n\n\n\nThere is however a partial converse. If \\(\\{X_n\\}_{n \\ge 1}\\) is a strictly decreasing sequence and \\(c\\) is a constant then \\[\n  [X_n \\xrightarrow{p} c]\n  \\implies\n  [X_n \\xrightarrow{a.s.} c].\n  \\]\nIf \\(X_n \\xrightarrow{p} X\\), then there exists a subsequence \\(\\{n_k : k \\in \\naturalnumbers \\}\\) such that \\(\\{X_{n_k}\\}_{k \\ge 1}\\) converges almost surely to \\(X\\).\n\\(X_n \\xrightarrow{p} X\\) if and only if every subsequence \\(\\{n_k : k \\in \\naturalnumbers \\}\\) has a sub-subsequence \\(\\{n_{k_m} : m \\in \\naturalnumbers \\}\\) such that \\(\\{X_{n_{k_m}} \\}_{m \\ge 1}\\) that converges to \\(X\\) almost surely.\nWe have already seen the notion of convergence in distribution (also called weak-convergence). A sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to a random variable \\(X\\) in distribution if \\[\n   \\lim_{n \\to ∞} \\PR(X_n \\le x) = \\PR(X \\le x)\n\\] for every \\(x\\) where \\(F(x) = \\PR(X \\le x)\\) is continuous.\nNote that the limiting object \\(\\lim_{n \\to ∞} \\PR(X_n \\le x)\\) need not be a valid CDF. For example, consider the deterministic sequence, \\(X_n = n\\). The corresponding CDF is \\[\n   F_{X_n}(x) = \\begin{cases}\n     0 & x &lt; n \\\\\n     1 & x \\ge n\n   \\end{cases}\n\\] Thus, for any \\(x\\), we have \\(\\lim_{n \\to ∞} F_{X_n}(x) = 0\\) which is not a valid CDF.\nConvergence in probability implies convergence in distribution. \\[\n  [X_n \\xrightarrow{p} X]\n  \\implies\n  [X_n \\xrightarrow{D} X]\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nLet \\(F_n\\) and \\(F\\) denote the CDFs of \\(X_n\\) and \\(X\\), respectively. Fix \\(ε &gt; 0\\), pick \\(x\\) such that \\(F\\) is continuous at \\(x\\), and consider \\[\\begin{align*}\n  F_n(x) &= \\PR(X_n \\le x) = \\PR(X_n \\le x, X \\le x + ε) + \\PR(X_n \\le x, X &gt; x + ε)\n\\\\\n&\\le \\PR(X \\le x + ε) + \\PR(X - X_n &gt; ε)\n\\\\\n&\\le F(x + ε) + \\PR(\\ABS{X_n - X} &gt; ε).\n\\end{align*}\\] Similarly, \\[\\begin{align*}\n  F(x-ε) &= \\PR(X \\le x-ε) = \\PR(X \\le x-ε, X_n \\le x ) + \\PR(X \\le x - ε, X_n &gt; x )\n\\\\\n&\\le \\PR(X_n \\le x) + \\PR(X_n - X &gt; ε)\n\\\\\n&\\le F_n(x) + \\PR(\\ABS{X_n - X} &gt; ε).\n\\end{align*}\\]\nThus, \\[\nF(x-ε) - \\PR(\\ABS{X_n - X} &gt; ε) \\le F_n(x) \\le F(x+ε) + \\PR(\\ABS{X_n - X} &gt; ε).\n\\] Taking \\(n \\to ∞\\), we have \\[\nF(x-ε) \\le \\liminf_{x \\to ∞} F_n(x) \\le \\limsup_{x \\to ∞} F_n(x) \\le F(x + ε).\n\\] The result is true for all \\(ε &gt; 0\\). Since \\(F\\) is continuous at \\(x\\), when we take \\(ε \\downarrow 0\\), we have \\[\nF(x - ε) \\uparrow F(x)\n\\quad\\text{and}\\quad\nF(x + ε) \\downarrow F(x)\n\\] which implies that \\(F_n(x) \\to F(x)\\).\n\n\n\nThe converse is not true. For example, let \\(X_n\\) be a sequence of i.i.d. \\(\\text{Uniform}(0,1)\\) random variables. Clearly, \\(X_n \\xrightarrow{D} X\\), where \\(X\\) is also \\(\\text{Uniform}(0,1)\\). However, for any \\(ε \\in (0,1)\\), \\[\n  \\PR( \\ABS{X_n - X} &gt; ε ) = (1-ε)^2\n\\]\n[Think of the area of the unit square corresponding to the event \\(|X_n - X| &gt; ε\\)]. Thus, \\[\n  \\lim_{n \\to ∞} \\PR(\\ABS{X_n - X} &gt; ε) \\neq 0.\n\\] So, \\(X_n\\) does not converge in probability to \\(X\\).\nThere is a partial converse. If \\(c\\) is a constant then \\[\n  [X_n \\xrightarrow{D} c]\n  \\implies\n  [X_n \\xrightarrow{p} c].\n\\]\nConvergence in distribution does not imply convergence of moments! For example, consider the sequence of random variables where \\[\n    X_n = \\begin{cases}\n      0 & \\text{w.p. } 1 - \\frac 1n \\\\\n      n & \\text{w.p. } \\frac 1n\n    \\end{cases}\n\\] Then, \\[\n  F_{X_n}(x) = \\begin{cases}\n    0, & x &lt; 0\n    1 - \\frac 1n, & 0 \\le x &lt; n \\\\\n    1 & x \\ge n\n  \\end{cases}\n\\] Thus, we have \\[\n  \\lim_{n \\to ∞} F_{X_n}(x) =\n  \\begin{cases}\n    0 & x &lt; 0 \\\\\n    1 & x \\ge 0\n  \\end{cases}\n\\] which corresponds to a random variable \\(X = 0\\).\nHowever, for any \\(k \\ge 1\\), we have \\(\\EXP[X_n^k] = n^{k-1}\\) but \\(\\EXP[X^k] = 1\\). Thus, the moments do not converge!\nSkorokhod’s representation theorem. If \\(X_n \\xrightarrow{D} X\\), then there exists a sequence \\(\\{Y_n\\}_{n \\ge 1}\\) that is identically distributed to \\(\\{X_n\\}_{n \\ge 1}\\) such that \\(Y_n \\xrightarrow{a.s.} Y\\), where \\(Y\\) is identically distributed to \\(X\\).\nContinuity preserves convergence almost surely, in probability, and in distribution. Thus, if \\(X_n\\) converges to \\(X\\) in any of these modes and \\(g\\) is a continuous function, then \\(g(X_n)\\) converges in the same mode to \\(g(X)\\).\nConvergence of sums and products\n\nIf \\(X_n \\xrightarrow{a.s.} X\\) and \\(Y_n \\xrightarrow{a.s.} Y\\), then \\(X_n + Y_n \\xrightarrow{a.s.} X + Y\\) and \\(X_n Y_n \\xrightarrow{a.s.} XY\\).\nIf \\(X_n \\xrightarrow{p} X\\) and \\(Y_n \\xrightarrow{p} Y\\), then \\(X_n + Y_n \\xrightarrow{p} X + Y\\) and \\(X_n Y_n \\xrightarrow{p} X Y\\).\nIt is not true in general that \\(X_n + Y_n \\xrightarrow{D} X + Y\\) whenever \\(X_n \\xrightarrow{p} X\\) and \\(Y_n \\xrightarrow{p} Y\\). However, the result is true when \\(X\\) or \\(Y\\) is a constant.\n\nThere is another form of convergence: convergence in the \\(r\\)-th mean. We say that a sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to \\(X\\) in \\(r\\)-th mean, \\(r \\ge 1\\), if \\(\\EXP[\\ABS{X_n}^r] &lt; ∞\\) for all \\(n\\), \\(\\EXP[\\ABS{X}^r] &lt; ∞\\), and \\[\n  \\lim_{n \\to ∞} \\EXP{ \\ABS{X_n - X}^r } = 0.\n\\]\nWhen \\(r = 1\\), we say that \\(\\{X_n\\}_{n \\ge 1}\\) converges in the mean. When \\(r=2\\), we say that \\(\\{X_n\\}_{n \\ge 1}\\) converges in the mean-square. We will mainly focus on mean-square convergence and denote it by \\(X_n \\xrightarrow{m.s.} X\\).\nIf \\(\\{X_n\\}_{n \\ge 1}\\) converges to \\(X\\) in the \\(r\\)-th mean, then it also converges in the \\(s\\)-th mean, for all \\(s &lt; r\\).\nConvergence in \\(r\\)-th mean implies convergence in probability.\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nBy Markov inequality, we have (for any \\(r \\ge 1\\)) \\[\n  \\PR(\\ABS{X_n - X} &gt; ε) = \\PR(\\ABS{X_n - X}^r &gt; ε^r)\n  \\le \\frac{ \\EXP[ \\ABS{X_n - X}^r ] }{ε^r}.\n\\] So, if the RHS goes to zero as \\(n \\to ∞\\), then so does the LHS.\n\n\n\nHowever, the converse is not true as is illustrated by Example 7.3, part (c).\nConvergence in mean-square and almost surely are not comparable. Example 7.3, part (c) gives an example that converges almost surely not not in mean square. Example 7.5, part (d) gives an example that converges in mean square but not almost surely.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#some-examples",
    "href": "convergence-of-random-variables.html#some-examples",
    "title": "7  Convergence of random variables",
    "section": "7.2 Some examples",
    "text": "7.2 Some examples\n\nExample 7.3 Consider the probability space \\((Ω, \\ALPHABET F, \\PR)\\) as in Example 7.1, and consider different choices of \\(X_n\\) shown below. For each case, determine whether \\(X_n\\) converges almost surely, in probability, or in mean-square.\n\n\\(X_n(ω) = \\IND_{A_n}(ω)\\), where \\(A_n = [0, \\frac 1n]\\).\n\\(X_n(ω) = \\IND_{A_n}(ω)\\), where \\(A_n = [0, \\frac 1{n^2}]\\).\n\\(X_n(ω) = \\textcolor{red}{n}\\IND_{A_n}(ω)\\), where \\(A_n = [0, \\frac 1n]\\).\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe only prove part (a). The proof of the other two parts is similar.\nFor \\(ω \\neq 0\\), the sequence \\(\\{X_n(ω)\\}_{n \\ge 1}\\) is a finite sequence of ones followed by an infinite sequence of zeros. Then, \\(\\lim_{n \\to ∞} X_n(ω) = 0\\). Thus, \\[\n  \\PR\\Bigl( \\Bigl\\{ ω : \\lim_{n \\to ∞} X_n(ω) = 0 \\Bigr\\} \\Bigr)\n  = \\PR( ω \\in (0,1]) = 1.\n\\] Hence, \\(X_n \\xrightarrow{a.s} 0\\).\nFix an \\(ε &gt; 0\\) and consider the set \\[\n  E_n = \\{ ω : |X_n(ω) - 0| &gt; ε \\}.\n\\] If \\(ε &gt; 1\\), then \\(E_n = \\emptyset\\), and \\(\\PR(E_n) = 0\\) for all \\(n\\). So, we assume that \\(ε &lt; 1\\). Then, \\[\n  E_n = \\{ ω : X_n(ω) = 1 \\}.\n\\] Hence, \\[\n  \\PR(E_n) = \\PR(X_n = 1) = \\frac 1n\n\\] which converges to \\(0\\) as \\(n \\to ∞\\). Therefore, \\(X_n \\xrightarrow{p} 0\\).\nConsider \\[\n  \\EXP[\\ABS{X_n}^2] = \\int_{A_n} 1\\, d ω = \\frac{1}{n}\n\\] which converges to \\(0\\) as \\(n \\to ∞\\). Thus, \\(X_n \\xrightarrow{m.s.} X\\).\nHowever, observe that for part (c) \\[\n  \\EXP[\\ABS{X_n}^2] = \\int_{A_n} n^2\\, d ω = n\n\\] which goes to \\(∞\\) as \\(n \\to ∞\\). Thus, \\(\\{X_n\\}_{n \\ge 1}\\) does not converge in mean square.\n\n\n\n\nExample 7.4 Consider an i.i.d. sequence \\(\\{X_n\\}_{n \\ge 1}\\), where \\(X_n \\sim \\text{Uniform}(0,1)\\). Define \\[\n  Y_n = \\min\\{X_1, \\dots, X_n\\}.\n\\] Determine whether \\(Y_n\\) converges almost surely or in probability.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe first check convergence in probability and then almost surely.\nFix an \\(ε &gt; 0\\) and consider the set \\[\n  E_n = \\{ ω : |Y_n(ω) - 0| &gt; ε \\}.\n\\] As in Example 7.3, if \\(ε &gt; 1\\), then \\(\\PR(E_n) = 0\\). So, we assume that \\(ε &lt; 1\\). Then, \\[\\begin{align*}\n\\PR(E_n) &= \\PR(Y_n &gt; ε) = \\PR\\bigl( \\min\\{ X_1, \\dots, X_n \\} \\ge ε ) \\\\\n&= \\PR( X_1 \\ge ε, X_2 \\ge ε, \\dots, X_n \\ge ε ) \\\\\n&= (1-ε)^n\n\\end{align*}\\] which goes to zero as \\(n \\to ∞\\). Thus, \\(Y_n \\xrightarrow{p} 0\\).\nWe now consider almost sure convergence. Note that for a fixed \\(ω\\), the sequence \\(\\{Y_n(ω)\\}_{n \\ge 1}\\) is a decreasing sequence. Hence, it must have a limit. Denote that limit by \\(Y(ω)\\), i.e., \\[\n  Y(ω) = \\lim_{n \\to ∞} Y_n(ω).\n\\]\nSince \\(\\{Y_n\\}_{n \\ge 1}\\) is a decreasing sequence, we have that \\(Y(ω) \\le Y_n(ω)\\). Hence, for any \\(ε &gt; 0\\), \\[\n  \\PR(Y &gt; ε) \\le \\PR(Y_n &gt; ε) = (1 - ε)^n\n\\] where the last inequality follows from the calculations done above.\nThe above inequality holds for every \\(n\\), so we have \\[\n  \\PR(Y &gt; ε) \\le \\lim_{n \\to ∞} (1-ε)^n = 0.\n\\] Recall that \\(ε &gt; 0\\) was arbitrary. Therefore, we have shown that \\[\n  \\PR\\Bigl( \\lim_{n\\to ∞} Y_n &gt; ε \\Bigr) = 0.\n\\] Thus, the only possibility is that \\[\n  \\PR\\Bigl( \\lim_{n\\to ∞} Y_n = 0 \\Bigr) = 1.\n\\] Hence \\(Y_n \\xrightarrow{a.s.} 0\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#almost-sure-convergence-from-convergence-in-probability.",
    "href": "convergence-of-random-variables.html#almost-sure-convergence-from-convergence-in-probability.",
    "title": "7  Convergence of random variables",
    "section": "7.3 Almost sure convergence from convergence in probability.",
    "text": "7.3 Almost sure convergence from convergence in probability.\nExample 7.4 shows that verifying almost sure convergence can be a bit tricky. In this section, we show that sometimes it is possible to infer almost sure convergence from convergence in probability.\n\n\n\n\n\n\nWarningLim inf and lim sup of sets\n\n\n\n\n\nExplanation adapted from math.stackexchange [1] and [2]\nLimits of sets is easy to describe when we have weakly increasing or weakly decreasing sequence of sets. In particular, if \\(\\{C_n\\}_{n \\ge 1}\\) is a weakly increasing sequence of sets, then \\[\n\\lim_{n \\to ∞} C_n = \\bigcup_{n=1}^{∞} C_n.\n\\] Thus, the limit is the union of all sets. Moreover, if \\(\\{D_n\\}_{n \\ge 1}\\) is weakly decreasing sequence of sets, then \\[\n\\lim_{n \\to ∞} D_n = \\bigcap_{n=1}^{∞} D_n.\n\\] Thus, the limit is the intersection of all sets.\nWhat happens when a sequence \\(\\{A_n\\}_{n \\ge 1}\\) is neither increasing nor decreasing? We can sandwich it between an increasing sequence \\(\\{C_n\\}_{n \\ge 1}\\) and a decreasing sequence \\(\\{D_n\\}\\) as follows:\n\\[\n\\begin{array}{lcl}\nC_1 = A_1 \\cap A_2 \\cap A_3 \\cap \\dotsb\n&\\quad\\subseteq\\qquad\nA_1\n\\qquad\n&\\subseteq \\qquad\nD_1 = A_1 \\cup A_2 \\cup A_3 \\cup \\dotsb\n\\\\\nC_2 = \\phantom{A_1 \\cap{}} A_2 \\cap A_3 \\cap \\dotsb\n&\\quad\\subseteq\\qquad\nA_2\n\\qquad\n&\\subseteq \\qquad\nD_2 = \\phantom{A_1 \\cup{}} A_2 \\cup A_3 \\cup \\dotsb\n\\\\\nC_3 = \\phantom{A_1 \\cap A_2 \\cap{}} A_3 \\cap \\dotsb\n&\\quad\\subseteq\\qquad\nA_3\n\\qquad\n&\\subseteq \\qquad\nD_3 = \\phantom{A_1 \\cup A_2 \\cup{}} A_3 \\cup \\dotsb\n\\\\\n& \\qquad\\vdots &\n\\end{array}\n\\]\nThe limit of \\(\\{C_n\\}_{n \\ge 1}\\) is called the lim inf of \\(\\{A_n\\}_{n \\ge 1}\\), i.e., \\[\n  \\liminf_{n \\to ∞} A_n = \\lim_{n \\to ∞} C_n = \\bigcup_{n=1}^∞ C_n\n  = \\bigcup_{n=1}^{∞} \\bigcap_{i=n}^{∞} A_i.\n\\] Similarly, the limit of \\(\\{D_n\\}_{n \\ge 1}\\) is call the lim sup of \\(\\{A_n\\}_{n \\ge 1}\\), i.e., \\[\n  \\limsup_{n \\to ∞} A_n = \\lim_{n \\to ∞} D_n = \\bigcap_{n=1}^∞ D_n\n  = \\bigcap_{n=1}^{∞} \\bigcup_{i=n}^{∞} A_i.\n\\] When the two limits are equal, we say that the sequence \\(\\{A_n\\}_{n \\ge 1}\\) has a limit.\n\nAnother way to think about these definitions is as follows. \\[\n  ω \\in \\limsup_{n \\to ∞} A_n \\iff\n  \\limsup_{n \\to ∞} \\IND_{A_n}(ω) = 1\n\\] which holds if and only if the binary sequence \\(\\{\\IND_{A_n}(ω)\\}\\) has infinitely many ones, i.e., \\(ω\\) is a member of infinitely many \\(A_n\\).\nSimilarly, \\[\n  ω \\in \\liminf_{n \\to ∞} A_n \\iff\n  \\liminf_{n \\to ∞} \\IND_{A_n}(ω) = 1\n\\] which holds if and only if the binary sequence \\(\\{\\IND_{A_n}(ω)\\}\\) eventually becomes \\(1\\) forever, i.e., \\(ω\\) is eventually a member of \\(A_n\\) forever.\nFor example, suppose we toss a coin infinite number of coins. Let \\((Ω, \\ALPHABET F, \\PR)\\) denote the corresponding probability space, and let \\(A_n\\) denote the event that the \\(n\\)-th toss is a heads. Then,\n\n\\(\\limsup_{n \\to ∞} A_n\\) is the event that there are infinitely many heads.\n\\(\\liminf_{n \\to ∞} A_n\\) is the event that there are all but a finite number of the coins were head, i.e., there were only finitely many tails.\n\n\n\n\nWe now state two fundamental results. The proofs are not difficult but are omitted due to time.\n\nLemma 7.1 (Borel Cantelli Lemma) Let \\(\\{A_n\\}_{n \\ge 1}\\) be a sequence of events defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\). If the sum of the probability of the events is finite, i.e., \\[\n\\sum_{n=1}^∞ \\PR(A_n) &lt; ∞,\n\\] then the probability that infinitely many of them occur is zero, i.e., \\[\n  \\PR\\Bigl(\\limsup_{n \\to ∞} A_n \\Bigr) = 0.\n\\]\n\nThere is a partial converse of Borel-Cantelli lemma.\n\nLemma 7.2 (Second Borel Cantelli Lemma) Let \\(\\{A_n\\}_{n \\ge 1}\\) be a sequence of independent events defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\). If the sum of the probability of the events is infinite, i.e., \\[\n\\sum_{n=1}^∞ \\PR(A_n) = ∞,\n\\] then the probability that infinitely many of them occur is one, i.e., \\[\n  \\PR\\Bigl(\\limsup_{n \\to ∞} A_n \\Bigr) = 1.\n\\]\n\nAn immediate implication of Borel-Cantelli lemma is the following:\n\nLemma 7.3 Suppose \\(X_n \\xrightarrow{p} X\\) and for any \\(ε &gt; 0\\), we have \\[\n\\sum_{n=1}^{∞} \\PR(\\ABS{X_n - X} &gt; ε) &lt; ∞\n\\] then \\(X_n \\xrightarrow{a.s.} X\\).\n\nIn light of the above result, we revisit some variations of the examples of the previous section.\n\nConsider Example 7.3, part a, we have \\[\n    \\PR(E_n) = \\frac 1n.\n\\] Since \\(\\sum_{n \\ge 1} \\PR(E_n) = ∞\\), we cannot use the above theorem to infer that \\(X_n \\xrightarrow{a.s.} 0\\). This is not a contraction because Lemma 7.3 is a sufficient condition, not a necessary condition.\nConsider Example 7.3, part b, we have \\[\n    \\PR(E_n) = \\frac 1{n^2}.\n\\] Since \\(\\sum_{n \\ge 1} \\PR(E_n) &lt; ∞\\), we can use Lemma 7.3 to infer that \\(X_n \\xrightarrow{a.s.} 0\\).\nIn Example 7.4, we have argued that \\(\\PR(Y_n &gt; ε) = (1-ε)^n\\). Therefore, \\(\\sum_{n \\ge 1} \\PR(Y_n &gt; ε) = \\frac{1}{ε} &lt; ∞\\). Hence, by Lemma 7.3, \\(Y_n \\xrightarrow{a.s.} 0\\).\n\nWe can also consider a variation of the above.\n\nExample 7.5 Consider a variation of Example 7.3, where we no longer specify \\(X_n\\) as a function of \\(ω\\) but simply assume that \\[\n   X_n = \\begin{cases}\n     0 & \\text{with probability } 1 - p_n \\\\\n     1 & \\text{with probability } p_n\n   \\end{cases}\n\\] and \\(\\{X_n\\}_{n \\ge 1}\\) are independent.\nDetermine whether \\(\\{X_n\\}_{n \\ge 1}\\) converges almost surely, in probability, or in mean square\n\n\\(p_n = \\frac 1n\\)\n\\(p_n = \\frac 1{n^2}\\).\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nConvergence in probability: Fix \\(ε \\in (0,1)\\). As before define \\(E_n = \\{ ω : \\ABS{X_n(ω)} &gt; ε \\} = \\{ ω : X_n(ω) = 1 \\}\\). Thus, \\(\\PR(E_n) = p_n\\) and for both cases, \\(p_n \\to 0\\) as \\(n \\to ∞\\). Therefore, \\(X_n \\xrightarrow{p} 0\\).\nAlmost sure convergence: For part (a), \\(\\sum_{n \\ge 1} p_n = ∞\\); hence by the Second Borel-Cantelli lemma, \\[ \\PR(\\limsup_{n \\to ∞} \\{ \\ABS{X_n} &gt; ε \\}) = 1. \\] So, \\(X_n\\) does not converge almost surely!\nFor part (b), \\(\\sum_{n \\ge 1} p_n &lt; ∞\\); hence by Lemma 7.3, \\(X_n \\xrightarrow{a.s.} 0\\).\nMean square convergence: We have \\(\\EXP[\\ABS{X_n}^2] = p_n\\). For both cases \\(p_n \\to 0\\). Hence, \\(X_n \\xrightarrow{m.s.} 0\\).\n\nNote that part (a) converges in probability and mean square but not almost surely.\n\n\n\nA variation of Lemma 7.4 is the following:\n\nLemma 7.4 Let \\(\\{X_n\\}_{n \\ge 1}\\) be a sequence of random variables with finite expectations and let \\(X\\) be another random variable. If \\[\n\\sum_{n=1}^{∞} \\EXP[ \\ABS{X_n - X} ]  &lt; ∞\n\\] then \\[\nX_n \\xrightarrow{a.s.} X.\n\\]\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nTo simplify the notation, we assume that \\(X = 0\\).\nPick any \\(ε &gt; 0\\) and define the sequence of events \\[\nA_n = \\bigl\\{ ω : \\ABS{X_i} &gt; ε \\bigr\\},\n\\quad n \\in \\naturalnumbers.\n\\]\nFrom Markov inequality, we have \\[\\PR(A_n) = \\PR\\bigl( \\ABS{X_i} &gt; ε \\bigr)\n\\le \\dfrac{\\EXP[\\ABS{X_i}]}{ε}. \\] Therefore, \\[ \\sum_{n=1}^{∞} \\PR(A_n)\n\\le \\frac{1}{ε} \\sum_{n=1}^{∞} \\EXP[\\ABS{X_i}]\n&lt; ∞\n\\] by the hypothesis of the result. Therefore, by Borel-Cantelli lemma, we have \\[\n\\PR\\Bigl( \\limsup_{n \\to ∞} A_n \\Bigr) = 0.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#strong-law-of-large-numbers",
    "href": "convergence-of-random-variables.html#strong-law-of-large-numbers",
    "title": "7  Convergence of random variables",
    "section": "7.4 Strong law of large numbers",
    "text": "7.4 Strong law of large numbers\n\nTheorem 7.1 Let \\(\\{X_n\\}_{n \\ge 1}\\) be an i.i.d. sequence of random variables with mean \\(μ\\) and variance \\(σ^2\\). Let \\[\n  \\bar X_n = \\frac 1n \\sum_{i=1}^n X_i\n\\] be the sameple average. Then, \\(\\bar X_n \\xrightarrow{a.s} μ\\), i.e., \\[\n\\PR\\Bigl( ω : \\lim_{n \\to ∞} X_n(ω) = μ \\Bigr) = 1.\n\\]\n\nWe provide a proof under the assumption that the fouth moment’s exist.\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nWe assume that \\(μ = 0\\) (this is just for notational simplicity) and \\(\\EXP[X^4] = γ &lt; ∞\\) (this is a strong assumption).\nSince we know that the forth moment exist, we can use a forth moment version of Chebyshev inequality: \\[ \\PR \\ABS{\\bar X_n} \\ge ε) \\le \\frac{ \\EXP[ \\bar X_n^4]}{ε^2}. \\]\nThen, by the multinomial theorem, we have \\[ \\EXP{\\bar X_n^4} = \\frac{1}{4} \\EXP\\biggl[\n\\sum_{i} X_i^4 + \\binom{4}{1,3} \\sum_{i \\neq j} X_i X_j^3 + \\binom{4}{2,2} \\sum_{i \\neq j} X_i^2 X_j^2\n+ \\binom{4}{1,1,2} \\sum_{i \\neq j \\neq k} X_i X_j X_k^2 + \\sum_{i \\neq j \\neq k \\neq \\ell} X_i X_j X_k X_{\\ell} \\biggr].\n\\]\nSince the \\(\\{X_i\\}_{i \\ge 1}\\) are independent and zero mean, we have\n\n\\(\\EXP[X_i X_j^3] = \\EXP[X_i] \\EXP[X_j^3] = 0\\).\n= = 0$.\n\nTherefore, \\[\n\\EXP[\\bar X_n^4= \\frac{1}{n^4} \\bigl[ n \\EXP[X_i^4] + 3 n(n-1) \\EXP[ X_i^2 X_j^2] \\bigr]\n\\le \\frac{M_4}{n_3} + \\frac{3 σ^4}{n^2}\n\\] where \\(M_4\\) is the forth moment. Now, from the forth moment version of Chebyshev inequality, we have \\[ \\PR \\ABS{\\bar X_n} \\ge ε) \\le \\frac{ \\frac{M_4}{n_3} + \\frac{3 σ^4}{n^2} }{ε^2}. \\] This implies that \\[\n\\sum_{n=1}^{∞} \\PR(\\ABS{X_n} \\ge ε) &lt; ∞.\n\\] Thus, from Lemma 7.3, we have that \\(X_n \\xrightarrow{a.s.} 0\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#further-reading",
    "href": "convergence-of-random-variables.html#further-reading",
    "title": "7  Convergence of random variables",
    "section": "7.5 Further Reading",
    "text": "7.5 Further Reading\n\nGubner: Sec. 13.1, 14.1, 14,2, 14,3 (and corresponding problems at the end of the chapters)\nGrimmett and Stirzaker: Sec. 7.1, 7.2, 7.3, 7.4, 7.5 (and corresponding problems at the end of the sections and the end of the chapters)",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "markov-chains.html",
    "href": "markov-chains.html",
    "title": "8  Markov chains",
    "section": "",
    "text": "8.1 Time-homogeneous Markov chains\nLet \\(\\ALPHABET X\\) be a finite set. A stochastic process \\(\\{X_n\\}_{n \\ge 0}\\), \\(X_n \\in \\ALPHABET X\\), is called a Markov chain if it satisfies the Markov property: for any \\(n \\in \\integers_{\\ge 0}\\) and any \\(x_{1:n+1} \\in \\ALPHABET X^{n+1}\\), we have \\[\\begin{equation}\\tag{Markov property}\\label{eq:Markov}\n  \\PR(X_{n+1} = x_{n+1} \\mid X_{1:n} = x_{1:n})\n  = \\PR(X_{n+1} = x_{n+1} \\mid X_n = x_n).\n\\end{equation}\\]\nThe variable \\(X_n\\) is called the state of the Markov chain at time \\(n\\); the set \\(\\ALPHABET X\\) is called state space. The \\(\\eqref{eq:Markov}\\) implies that the current state captures all the information from the past that is relevant for the future. Stated differently, conditioned on the present, the past is independent of the future.\nIndependence is a symmetric relationship. Thus, we expect the Markov property to also hold if time is reversed! Exercise 8.1 asks you to formally prove that.\nWe now present some examples of Markov chains arising in different applications.\nSee the following video from Veritasium for an excellent history of Markov chains and its applications\nIn this course, we will focus on time-homogeneous Markov chain. The Markov chain is called time-homogeneous if the right hand side of \\(\\eqref{eq:Markov}\\) does not depend on \\(n\\). In this case, we describe the Markov chain by a state transition matrix \\(P\\), where \\(P_{ij} = \\PR(X_{n+1} = j | X_n = i)\\). Such Markov chains can also be visualized using state transition diagrams as we illustrate in the examples below.\nIn spite of its simplicity, such simple on-off Markov chains are used in various applications including telecommunication systems, network traffic modeling, machine failure-repair models, gene activation, and others. Some properties of the on-off Markov chain are as follows:\nNote that the gambler’s fortune in Example 8.3 is a random walk on non-negative integers \\(\\{0,1,2, \\dots\\}\\) with absorption at \\(0\\). In the variation where the gambler stops when his fortune reaches $\\(K\\), it is a random walk over \\(\\{0,1,\\dots,K\\}\\) with absorption at both ends: \\(0\\) and \\(K\\).\nTODO: Add other examples",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#time-homogeneous-markov-chains",
    "href": "markov-chains.html#time-homogeneous-markov-chains",
    "title": "8  Markov chains",
    "section": "",
    "text": "Example 8.4 (On-Off Markov chain) The On-Off Markov chain discussed earlier can be modelled with \\(\\ALPHABET X = \\{0, 1\\}\\) and general transition probability matrix of the form. \\[\n  P = \\MATRIX{ 1 - a & a \\\\ b & 1 - b }.\n\\] The transition matrix can be visualized as follows.\n\n\n\nOn-Off Markov chain\n\n\n\n\n\nIf \\(a + b = 1\\), then both rows of the transition matrix are identical. Therefore, the Markov chain has no memory and is equivalent to a Bernoulli process with success probability \\(a = 1-b\\).\nWhen \\(a\\) or \\(b\\) are small, the corresponding state is “sticky”, i.e., when the Markov chain enters a sticky state, it stays there for a long time.\n\n\nExample 8.5 The Ehrenfest model of diffusion presented in Example 8.2 can be modelled as a Markov chain with state space \\(\\{0, 1, \\dots, K\\}\\) and transition probability \\[\n  P_{ij} = \\begin{cases}\n    i/K     & j = i-1 \\\\\n    (K-i)/K & j = i + 1 \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] The transition matrix can be visualized as follows.\n\n\n\nEhrenfest model of diffusion\n\n\n\n\nExample 8.6 (Random walk in one dimension) Imagine a particle which moves in a straight line in unit steps. Each step is one unit to the right with probability \\(p\\) or one unit to the left with probabity \\(q = 1-p\\). It moves until it reaches one of two extreme points, which are called boundary points. The behavior of the particle at the boundary determines several different possibilities.\nWe will consider the case where the state space is \\(\\ALPHABET X = \\{-2, -1, 0, 1, 2\\}\\) and the process starts in state \\(0\\).\n\nAbsorbing random walk: Assume that when the particle reaches a boundary state, it stays there from that time on. We may visualize the Markov chain as follows.\n\n\n\nAbsorbing random walk\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 1 & 0 & 0 & 0 & 0 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                0 & 0 & 0 & 0 & 1}.\n\\]\nReflected random walk: Assume that when the particle reaches a boundary states, it is reflected and returns to the point it came from. We may visualize the Markov chain as follows.\n\n\n\nReflected random walk\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 0 & 1 & 0 & 0 & 0 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                0 & 0 & 0 & 1 & 0}.\n\\]\nRandom walk with restart: Assume that when the particle reaches a boundary state, it restarts in the initial state. We may visualize the Markov chain as follows.\n\n\n\nRandom walk with restart\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 0 & 0 & 1 & 0 & 0 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                0 & 0 & 1 & 0 & 0}.\n\\]\nRandom walk with periodic boundary: Assume that when the particle reaches a boundary state, it moves to the other boundary. We may visualize the Markov chain as follows.\n\n\n\nRandom walk with periodic boundary\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 0 & 0 & 0 & 0 & 1 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                1 & 0 & 0 & 0 & 0}.\n\\]\n\n\n\n\n\nSuccess runs\n\n\n8.1.1 Properties of interest\nDepending on the application, we are typically interested in the following properties of a Markov chain:\n\nIf the chain starts in state \\(i\\), what is the probability that after \\(n\\) steps it is in state \\(j\\)?\nIf the chain starts in state \\(i\\), what is the expected number of visits to state \\(j\\) in \\(n\\) steps?\nWhat is the expected number of steps that it takes for a chain starting in state \\(i\\) to visit state \\(j\\) for the first time?\nWhat is the average number of times that the chain is in state \\(i\\)? How does this depend on the initial state?\n\nIn the rest of this section, we will present results in Markov chain theory that answer the above questions.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#state-occupancy-probabilities",
    "href": "markov-chains.html#state-occupancy-probabilities",
    "title": "8  Markov chains",
    "section": "8.2 State occupancy probabilities",
    "text": "8.2 State occupancy probabilities\nLet \\(μ^{(n)}\\) denote the PMF of the state of the Markov chain at time \\(n\\). This is also called the state occupancy probababilites. We will think of of \\(μ^{(n)}\\) as a row vector. Then, by the law of total probability, we have \\[\n  \\PR(X_n = j) = \\sum_{i \\in \\ALPHABET X} \\PR(X_{n-1} = i) \\PR(X_n = j | X_{n-1} = i)\n\\] or, equivalently, \\[\n  μ^{(n)}_j = \\sum_{i \\in \\ALPHABET X} μ^{(n-1)}_i P_{ij}\n\\] which can be written in matrix form as \\[ μ^{(n)} = μ^{(n-1)} P \\] and, by recusively expanding the right hand side, we have \\[ μ^{(n)} = μ^{(0)} P^n. \\]\nWe will abbreviate \\([P^n]_{ij}\\) as \\(P^{(n)}_{ij}\\).\n\nExample 8.7 Numerically compute the state occupancy probabilities for the different examples of random walk presented in Example 8.6 when \\(p = q = \\tfrac 12\\) for \\(n \\in \\{1, \\dots, 8\\}\\). In all cases, the initial probability \\[\n  μ^{(0)} = \\MATRIX{0 & 0 & 1 & 0 & 0 }.\n\\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nAbsorbing random walk: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & 0 & \\frac{1}{4} & 0 & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & \\frac{1}{8} & 0 & \\frac{1}{8} & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & 0 & \\frac{1}{8} & 0 & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & \\frac{1}{16} & 0 & \\frac{1}{16} & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{15}{32} & 0 & \\frac{1}{16} & 0 & \\frac{15}{32} \\\\\n\\end{array}\n\\right]\\)\n\nReflected random walk: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\nRandom walk with restart: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{8} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{8} & \\frac{1}{8} & \\frac{1}{2} & \\frac{1}{8} & \\frac{1}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{16} & \\frac{1}{4} & \\frac{3}{8} & \\frac{1}{4} & \\frac{1}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{8} & \\frac{3}{16} & \\frac{3}{8} & \\frac{3}{16} & \\frac{1}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{32} & \\frac{3}{16} & \\frac{7}{16} & \\frac{3}{16} & \\frac{3}{32} \\\\\n\\end{array}\n\\right]\\)\n\nRandom walk with periodic boundary: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & 0 & \\frac{1}{4} & 0 & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & \\frac{1}{8} & 0 & \\frac{1}{8} & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & 0 & \\frac{1}{8} & 0 & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & \\frac{1}{16} & 0 & \\frac{1}{16} & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{15}{32} & 0 & \\frac{1}{16} & 0 & \\frac{15}{32} \\\\\n\\end{array}\n\\right]\\)\n\n\n\n\n\n\nExample 8.8 Analytically compute the state occupancy probabilites for the on-off Markov chain of Example 8.4 when it starts from the initial probability distribution \\(μ^{(0)}\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nSince \\(μ^{(n+1)} = μ^{(n)} P\\), we have \\[\\begin{align*}\n  μ^{(n+1)}_0 &= μ^{(n)}_0 (1-a) + μ^{(n)}_1 b\n  \\\\\n  &= μ^{(n)}_0 (1-a) + (1 - μ^{(n)}_0) b\n  \\\\\n  &= μ^{(n)}_0 (1-a-b) + b.\n\\end{align*}\\] If \\(a = b = 0\\), then \\(μ^{(n+1)}_0 = μ^{(n)_0 = \\cdots = μ^{(0)}_0\\). If not, we exploit the fact that \\[\n  b = \\frac{b}{a+b} - \\frac{b}{a+b}(1-a-b)  \n\\] to recursively write \\[\\begin{align*}\n  μ^{(1)}_0 &= μ^{(0)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b) + \\frac{b}{a+b}\n\\end{align*}\\] and \\[\\begin{align*}\n  μ^{(2)}_0 &= μ^{(1)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^2 + \\frac{b}{a+b}(1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^2 + \\frac{b}{a+b}\n\\end{align*}\\] and, so on, to get \\[\\begin{align*}\n  μ^{(n)}_0 &= μ^{(n-1)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^n + \\frac{b}{a+b}(1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^n + \\frac{b}{a+b}.\n\\end{align*}\\] Therefore, \\[\n  μ^{(n)}_1 = 1 - μ^{(n)}_0\n  = \\left(μ^{(1)}_0 - \\frac{a}{a+b}\\right)(1-a-b)^n + \\frac{a}{a+b}.\n\\]\n\n\n\n\n\n\n\n\n\nWarningHow did we figure out the above calculation?\n\n\n\n\n\nThe above analysis appears to be a bit of black magic. To understand what is going on, note that we are interested in computing \\(μ^{(0)} P^n\\). What is an efficient way to compute \\(P^n\\)? Eigen decomposition!. Since \\(P\\) is a row stochastic matrix, we have \\(P \\mathbf{1} = 1.\\) Thus, \\(λ_1 = 1\\) is always an eigenvector of any transition matrix with eigenvector \\(\\mathbf{1}\\).\nFor Example 8.4, we can explicitly compute all eigenvalues by finding the roots of the characteristic equation \\[\\begin{align*}\n\\det(λI - P) &= \\DET{λ - 1 + a & - a \\\\ -b & λ - 1 + b} \\\\\n&= (λ-1 +a)(λ-1+b) - ab \\\\\n&= (λ-1)^2 + (a+b)(λ-1) = (λ-1)(λ-1 + a + b).\n\\end{align*}\\] Thus, the eigenvalues are \\(λ_1 = 1\\) and \\(λ_2 = 1 - a - b\\).\nFor the special case of \\(2 × 2\\) transition matrices, we can find the second eigenvalue by observing that \\(λ_1 = 1\\) is always an eigenvalue and \\(\\TR(P) = λ_1 + λ_2 = 1 + λ_2\\) or \\(\\det(P) = λ_1 λ_2 = λ_2\\).\nTo find the eigenvector, we find a vector \\(v\\) such that \\((λI - P)v = 0\\).\n\nFor \\(λ_1 = 1\\), we already know that \\(v_1 = [1; 1]\\) is an eigenvector.\nFor \\(λ_2 = (1-a-b)\\), we have \\[ λ_2I - P = \\MATRIX{-b & -a \\\\ -b & -a}. \\] Therefore, one possible eigenvector is \\([a; -b]\\).\n\nThen, from spectral decomposition, we know that \\[\n  P = V Λ V^{-1}\n\\] where \\(V = [v_1 v_2] = [1, a; 1, -b]\\) and \\(Λ = \\diag(1, 1-a-b)\\). Therefore, \\[\\begin{align*}\n  μ^{(n)} &= μ^{(0)} P^n = μ^{(0)} V Λ^n V^{-1} \\\\\n  &= \\MATRIX{ μ^{(0)}_0 & 1 - μ^{(0)}_0 }\n     \\MATRIX{1 & a \\\\ 1 & -b }\n     \\MATRIX{1 & 0 \\\\ 0 & 1 - a -b}\n     \\frac{1}{a+b}\n     \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &= \\MATRIX{\\dfrac{1}{a+b} & μ^{(0)}_0 - \\dfrac{b}{a+b} }\n     \\MATRIX{1 & 0 \\\\ 0 & 1 - a -b}\n     \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &=\\MATRIX{\\dfrac{1}{a+b} & 0 \\\\ 0 & \\left(μ^{(0)}_0 - \\dfrac{b}{a+b}\\right)(1-a-b)^n }\n   \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &= \\frac{b}{a+b} + \\left(μ^{(0)}_0 - \\dfrac{b}{a+b}\\right)(1-a-b)^n\n\\end{align*}\\]\n\n\n\nTODO: Add more examples!",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#class-structure",
    "href": "markov-chains.html#class-structure",
    "title": "8  Markov chains",
    "section": "8.3 Class structure",
    "text": "8.3 Class structure\n\nWe say that a state \\(j\\) is accessible from state \\(i\\) (abbreviated as \\(i \\rightsquigarrow j\\)) if there is exists an \\(m \\in \\integers_{\\ge 0}\\) (which may depend on \\(i\\) and \\(j\\)) such that \\([P^m]_{ij} &gt; 0\\). The fact that \\(P^{(m)}_{ij} &gt; 0\\) implies that there exists an ordered sequence of states \\((i_0, \\dots, i_m)\\) such that \\(i_0 = i\\) and \\(i_m = j\\) such that \\(P_{i_k i_{k+1}} &gt; 0\\); thus, there is a path of positive probability from state \\(i\\) to state \\(j\\).\nAccessibility is an transitive relationship, i.e., if \\(i \\rightsquigarrow j\\) and \\(j \\rightsquigarrow k\\) implies that \\(i \\rightsquigarrow k\\).\n\nExample 8.9 Consider the Markov chain shown below.\n\n\nIdentify all states that are accessible from state \\(1\\).\nIdentify all states from which state \\(1\\) is accessible.\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nStates accessible from state \\(1\\) are \\(\\{1,2,3\\}\\).\nStates from which state \\(1\\) is accessible are \\(\\{1,2,3,4,5,6\\}\\).\n\n\n\n\nTwo distinct states \\(i\\) and \\(j\\) are said to communicate (abbreviated to \\(i \\leftrightsquigarrow j\\)) if \\(i\\) is accessible from \\(j\\) (i.e., \\(j \\rightsquigarrow i\\)) and \\(j\\) is accessible from \\(i\\) (\\(i \\rightsquigarrow j\\)). Alternatively, we say that \\(i\\) and \\(j\\) communicate if there exist \\(m, m' \\in \\integers_{\\ge 0}\\) such that \\(P^{(m)}_{ij} &gt; 0\\) and \\(P^{(m')}_{ji} &gt; 0\\).\nFor instance, in Example 8.9, state \\(1\\) communicates with state \\(2\\) but does not communicate with state \\(5\\).\nCommunication is an equivalence relationship, i.e., it is reflexive (\\(i \\leftrightsquigarrow i\\)), symmetric (\\(i \\leftrightsquigarrow j\\) if and only if \\(j \\leftrightsquigarrow i\\)), and transitive (\\(i \\leftrightsquigarrow j\\) and \\(j \\leftrightsquigarrow k\\) implies \\(i \\leftrightsquigarrow k\\)).\nThe states in a finite-state Markov chain can be partitioned into two sets: recurrent states and transient states. A state is recurrent if it is accessible from all states that are accessible from it (i.e., \\(i\\) is recurrent if \\(i \\rightsquigarrow j\\) implies that \\(j \\rightsquigarrow i\\)). States that are not recurrent are transient.\nIt can be shown that a state \\(i\\) is recurrent if and only if \\[\\sum_{m=1}^{\\infty} P^{(m)}_{ii} = \\infty.\\]\nStates \\(i\\) and \\(j\\) are said to belong to the same communicating class if \\(i\\) and \\(j\\) communicate. Communicating classes form a partition the state space. Within a communicating class, all states are of the same type, i.e., either all states are recurrent (in which case the class is called a recurrent class) or all states are transient (in which case the class is called a transient class).\nFor example, in Example 8.9, there are two communication classes: \\(\\{1,2,3\\}\\) and \\(\\{4,5,6\\}\\). The communication class \\(\\{4,5,6\\}\\) is transient while the communication class \\(\\{1,2,3\\}\\) is recurrent.\nA communicating class \\(C\\) is said to be closed if \\[\n    i \\in C \\text{ and } i \\rightsquigarrow j \\implies j \\in C.\n\\] Thus, there is no escape from a closed class. For finite state spaces, a recurrent class is always closed and a transient class is never closed. But this is not the case for countable state Markov chains.\nA state \\(i\\) is called absorbing if \\(\\{i\\}\\) is a closed class, i.e., if \\(P_{ii} = 1\\).\nA Markov chain with a single communicating class (thus, all states communicate with each other and are, therefore, recurrent) is called irreducible.\nThe period of a state \\(i\\), denoted by \\(d(i)\\), is defined as \\[d(i) = \\gcd\\{ t \\in \\integers_{\\ge 1} : [P^t]_{ii} &gt; 0 \\}.\\] If the period is \\(1\\), the state is aperiodic, and if the period is \\(2\\) or more, the state is periodic. It can be shown that all states in the same class have the same period.\nA Markov chain is aperiodic, if all states are aperiodic. A simple sufficient (but not necessary) condition for an irreducible Markov chain to be aperiodic is that there exists a state \\(i\\) such that \\(P_{ii} &gt; 0\\). In general, for a finite and aperiodic Markov chain, there exists a positive integer \\(M\\) such that \\[ P^{(m)}_{ii} &gt; 0,\n        \\quad \\forall m \\ge M, i \\in \\ALPHABET X.\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#hitting-times",
    "href": "markov-chains.html#hitting-times",
    "title": "8  Markov chains",
    "section": "8.4 Hitting times",
    "text": "8.4 Hitting times\n\nWe use the following notation:\n\nFor any event \\(E\\), \\(P_i(E)\\) denotes \\(\\PR(E \\mid X_0 = i)\\)\nFor any random variable \\(Y\\), \\(\\EXP_i[Y]\\) denotes \\(\\EXP[Y \\mid X_0 = i]\\).\n\nLet \\(A\\) be a subset of \\(\\ALPHABET X\\). The hitting time \\(H^A\\) of \\(A\\) is a random variable \\(H^A \\colon \\ALPHABET X \\to \\{0, 1, \\dots \\} \\cup \\{∞\\}\\) given by \\[\n  H^A(ω) = \\min\\{n \\ge 0 : X_n(ω) \\in A\\}.\n\\] The standard convention is that \\(H^A\\) is taken to be \\(∞\\) if \\(X_n \\neq A\\) for any \\(n &gt; 0\\). For a state \\(j \\in \\ALPHABET X\\), we use the short-hand \\(H^j\\) to denote \\(H^{\\{j\\}}\\).\nThe probability that starting from state \\(i\\) the Markov chain ever hits \\(A\\) is then given by \\[\n   h^A_i = P_i(H^A &lt; ∞).\n\\] This is called hitting probability. When \\(A\\) is a closed class, \\(h^A_i\\) is called the absorption probability.\nThe mean-time taken for the Markov chain to reach \\(A\\) is given by \\[\n  m^A_i = \\EXP_i[H^A] = \\sum_{n=0}^{∞} n \\PR_i(H^A = n).\n\\] This is called the mean hitting time.\n\nA remarkable property of Markov chain is that these quantities can be computed by solving a system of linear equations associated with the transition probability matrix \\(P\\).\n\nTheorem 8.1 (Hitting probabilities) The hitting probabilities \\(\\{h^A_i\\}_{i \\in \\ALPHABET X}\\) satisfies the following system of linear equations: \\[\n  h^A_i = \\begin{cases}\n    1, & i \\in A \\\\\n    \\sum_{j \\in \\ALPHABET X} P_{ij} h^A_j, & i \\not\\in A\n  \\end{cases}\n\\] When \\(\\ALPHABET X\\) is finite, the above system has a unique solution; when \\(\\ALPHABET X\\) is countable, the above system may have multiple solutions and the hitting probabilities correspond to the minimal non-negative solution.\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nWhen \\(X_0 = i \\in A\\), the hitting time \\(H^A = 0\\), so \\(h^A_i = 1\\). This proves the first part of the formula.\nFor the second part, consider \\(X_0 = i \\not\\in A\\). Then \\(H^A_i \\ge 1\\). By the Markov property, we have \\[\n\\PR_i(H^A &lt; ∞ \\mid X_1 = j) = \\PR_j(H^A &lt; ∞) = h^A_j.\n\\] Moreover, by the law of total probability, we have \\[\\begin{align*}\nh^A_i &= \\PR_i(H^A &lt; ∞) = \\sum_{j \\in \\ALPHABET X} \\PR_i(H^A &lt; ∞, X_1 = j) \\\\\n&= \\sum_{j \\in \\ALPHABET X} \\PR_i(H^A &lt; ∞ \\mid X_1 = j) \\PR_i(X_1 = j) \\\\\n&= \\sum_{j \\in \\ALPHABET X} P_{ij} h^A_j.\n\\end{align*}\\]\n\n\n\n\nExample 8.10 Consider a gambler’s ruin problem, where the gambler starts with $\\(1\\) and stops either when he is ruined or when his fortune reaches $\\(K\\).\nFind the probability of ruin (i.e., the fortune gets absorbed in state \\(0\\) rather than state \\(K\\)).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFor the ease of notation, we use \\(h_i\\) as a short-form for \\(h^{\\{0\\}}_i\\). The hitting probabilities satisy the linear system of equations:\n\\[\\begin{align*}\n  h_0 &= 1 \\\\\n  h_i &= ph_{i+1} + q h_{i-1}, \\quad i \\in \\{1,\\dots,K-1\\} \\\\\n  h_K &= 0,\n\\end{align*}\\] where \\(q = 1-p\\).\nThe characteristic equation associated with the linear recurrence relationship is \\[\n  λ = p λ^2 + q\n\\] which has two distinct roots, \\(λ_1 = 1\\) and \\(λ_2 = q/p\\) if \\(p \\neq q\\) and a double root at \\(λ_1 = 1\\) if \\(p = q = \\frac 12\\). Therefore, the general solution is of the form \\[\n  h_i = a λ_1^i + b λ_2^i = a + b\\Bigl(\\tfrac {q}{p} \\Bigr)^i\n\\] where we determine the coefficients \\(a\\) and \\(b\\) from the boundary conditions \\(h_0 = 1\\) and \\(h_K = 0\\). Solving for \\(a\\) and \\(b\\), we get that for \\(p \\neq q\\), we have \\[\n  h_i = \\frac{1 - \\Bigl(\\frac qp\\Bigr)^i}{1 - \\Bigl(\\frac qp\\Bigr)^K}\n\\] and for \\(p = q = \\frac 12\\), we have \\[\n  h_i = \\frac{i}{K}.\n\\]\n\n\n\n\nTheorem 8.2 (Mean hitting times) The mean hitting times \\(\\{m^A_i\\}_{i \\in \\ALPHABET X}\\) satisfies the following system of linear equations: \\[\n  m^A_i = \\begin{cases}\n    0, & i \\in A \\\\\n    1 + \\sum_{j \\not\\in A} P_{ij} m^A_j, & i \\not\\in A\n  \\end{cases}\n\\] When \\(\\ALPHABET X\\) is finite, the above system has a unique solution; when \\(\\ALPHABET X\\) is countable, the above system may have multiple solutions and the mean hitting times correspond to the minimal non-negative solution.\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof is similar to the proof of Theorem 8.1. When \\(X_0 = i \\in A\\), the hitting time \\(H^A = 0\\), so \\(m^A_i = 0\\). This proves the first part of the formula.\nFor the second part, consider \\(X_0 = i \\not\\in A\\). Then \\(H^A_i \\ge 1\\). By the Markov property, we have \\[\n  \\EXP_i[ H^A \\mid X_1 = j] = 1 + \\EXP_j[ H^A ] = 1 + m^A_j.\n\\] Moreover, by the law of total probability, we have \\[\\begin{align*}\n  m^A_i &= \\EXP_i[ H^A ]\n  = \\sum_{j \\in \\ALPHABET X} \\EXP_i[ H^A \\mid X_1 = j ] \\PR_i(X_1 = j) \\\\\n  &= \\sum_{j \\in \\ALPHABET X} \\EXP_i[ H^A \\mid X_1 = j ] \\PR_i(X_1 = j) \\\\\n  &= \\sum_{j \\in \\ALPHABET X} P_{ij} \\bigl[ 1 + m^A_j \\bigr]\n  \\\\\n  &= 1 + \\sum_{j \\not\\in A} P_{ij} m^A_j.\n\\end{align*}\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#first-passage-time-and-strong-markov-property",
    "href": "markov-chains.html#first-passage-time-and-strong-markov-property",
    "title": "8  Markov chains",
    "section": "8.5 First passage time and strong Markov property",
    "text": "8.5 First passage time and strong Markov property\n\nDefine the first passage time \\[\n  f^{(n)}_{ij} \\coloneqq P_i(T_j = n) = P_i(X_1 \\neq j, \\dots, X_{n-1} \\neq j, X_n = j).\n\\]\n\\(f^{(n)}_{ij}\\) satisfies the following recursion.\n\n\\(f^{(1)}_{ij} = P_i(X_1 = j) = P_{ij}\\).\nAnd for \\(n &gt; 1\\), \\(f^{(n+1)}_{ij} = \\sum_{k \\neq j} P_{ik} f^{(n)}_{kj}\\).\n\nLet \\[\n  f_{ij} = \\sum_{n=1}^∞ f^{(n)}_{ij} = P_i(T_j &lt; ∞)\n\\] denotes the probability that a chain starting in \\(i\\) eventually visits \\(j\\). In particular \\(f_{jj}\\) denotes the probability that a chain starting in \\(j\\) will return to \\(j\\).\n\\(f_{ij}\\) satisfy the following property.\n\\[\\displaystyle P^{(n)}_{ij} = \\sum_{m=1}^n  f^{(m)}_{ij} P^{(n-m)}_{jj}.\\]\nAn immediate implication of the above is that if \\(j\\) is an absorbing state then \\(P^{(n)}_{ij} = \\sum_{m=1}^n f^{(m)}_{ij} = P_i(T_j \\le n).\\)\nLet \\(N^{(n)}_j = \\sum_{m=1}^{n} \\IND\\{X_m = j\\}\\) denote the number of visits to state \\(j\\) in \\(n\\) steps. Define \\[\n  G_{ij}^{(n)} = \\EXP_i[ N^{(n)}_j ] = \\sum_{m=1}^n P^{(m)}_{ij}\n\\] to be the expected number of visits to state \\(j\\) in \\(n\\) steps, starting in \\(i\\).\nLet \\(N_j = \\lim_{n \\to ∞} N^{(n)}_j \\sum_{m=1}^∞ \\IND\\{X_m = j \\}\\) denote the number of visits to state \\(j\\). Similarly, define \\[\n   G_{ij} = \\EXP_{i}[N_j] = \\lim_{n \\to ∞} G_{ij}^{(n)}\n   = \\sum_{m=1}^{∞} P^{(m)}_{ij}\n\\] to denote the expected number of visits to state \\(j\\) for a chain starting in \\(i\\).\nA state \\(j\\) is recurrent if \\(f_{jj} = 1\\) and transient if \\(f_{jj} &lt; 1\\).\nA state is called periodic if \\(f_{ii}^(n)\\) is non-zero only for multiples of some smallest integer \\(d\\), \\(d &gt; 1\\).\nFor every transient state \\(j\\), we have for every \\(i\\), \\(P_i(N_j &lt; ∞) = 1\\) and \\[ G_{ij} = \\dfrac{f_{ij}}{1 - f_{jj}}.\\] On the other hand, if \\(j\\) is recurrent, then \\(P_j(N_j = ∞) = 1\\) and \\(G_{jj} = ∞\\). Moreover, \\[ P_i(N_j = ∞) = P_i(T_j &lt; ∞) = f_{ij}. \\] So, if \\(f_{ij} = 0\\), then \\(G_{ij} = 0\\) while if \\(f_{ij} = ∞\\) then \\(G_{ij} = ∞\\).\nThus, a state \\(i\\) is recurrent if and only if \\[G_{ii} = \\sum_{n=1}^∞ P^{(n)}_{ii} = ∞. \\]\nA state \\(j\\) is said to be accessible from \\(i\\) (abbreviated as \\(i \\rightsquigarrow j\\)) if there is an ordered string of notes \\((i_0, \\dots, i_m)\\) such that \\(i_0 = i\\) and \\(i_m = j\\) and \\(P_{i_k i_{k+1}} &gt; 0\\). Equivalently, \\(i \\rightsquigarrow j\\) if there exists a \\(m\\) such that \\(P^{(m)}_{ij} &gt; 0\\).\nAccessibility is an transitive relationship, i.e., if \\(i \\rightsquigarrow j\\) and \\(j \\rightsquigarrow k\\) implies that \\(i \\rightsquigarrow k\\).\nIf \\(f_{ij} &gt; 0\\) but \\(f_{ji} &lt; 1\\), then \\(i\\) is transient.\nIf \\(i\\) is recurrent and \\(i \\rightsquigarrow j\\). Then, \\(j\\) is also recurrent and \\(f_{ij} = f_{ji} = 1\\).\nA subset \\(C\\) of \\(\\ALPHABET X\\) is said to be closed if no state inside \\(C\\) can lead to any state outside \\(C\\), i.e., \\[\n    f_{ij} = 0, \\quad \\forall i \\in C \\text{ and } j \\not\\in C.\n\\]\nA closed set \\(C\\) is called irreducible if \\(i \\rightsquigarrow j\\) for all \\(i,j \\in C\\). Thus, if \\(C\\) is an irreducible set, then all states in \\(C\\) are either recurrent or transient.\nConsequently, if \\(C\\) is an irreducible and closed set of recurrent states. Then for all \\(i,j \\in C\\),\n\n\\(f_{ij} = 1\\)\n\\(P_i(N_j = ∞) = 1\\)\n\\(G_{ij} = ∞\\)\n\nIf \\(C\\) is a finite irreducible closed set of states. Then every state in \\(C\\) is recurrent.\nLet \\(\\ALPHABET X_T\\) and \\(\\ALPHABET X_R\\) denote the set of transient and recurrent states. The set \\(\\ALPHABET X_R\\) can be paritioned into a finite or countable number of irreducible closed sets \\(C_1\\), \\(C_2\\), \\(\\dots\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#expected-duration-of-play",
    "href": "markov-chains.html#expected-duration-of-play",
    "title": "8  Markov chains",
    "section": "8.6 Expected duration of play",
    "text": "8.6 Expected duration of play\nLet’s start with a simple example. Suppose we toss a coin multiple times and stop at a heads. What are the expected number of tosses until stopping?\nFrom elementary probability we know that the number of tosses until stopping is a geometric random variable. However, we will model this using a Markov chain where the state denotes the number of consecutive heads so far. Let \\(p\\) denote the probability of heads and \\(q = 1-p\\) denote the probability of tails. Then, the Markov chain model is as follows.\n\n\n\nMarkov chain for coin tossing until one head\n\n\nLet \\(v_i\\) denote the expected number of tosses until stopping when starting at state \\(i\\). Then, we have \\[\\begin{align*}\n  v_0 &= 1 + q v_0 + p v_1, \\\\\n  v_1 &= 0.\n\\end{align*}\\] Solving this system of equations, we get \\(v_0 = 1/(1-q) = 1/p\\).\nNow, let’s try a variation of the above model. Suppose we toss a coin multiple times and stop at two heads. What are the expected number of tosses until stopping.\nWe can model this in the same manner as the before, where the state denotes the number of consecutive heads so far. The Markov chain is as follows:\n\n\n\nMarkov chain for coin tossing until two heads\n\n\nAs before, let \\(v_i\\) denote the expected number of tosses until stopping when starting at state \\(i\\). Then, we have \\[\\begin{align*}\n  v_0 &= 1 + q v_0 + p v_1, \\\\\n  v_1 &= 1 + q v_0 + p v_2, \\\\\n  v_2 &= 0.\n\\end{align*}\\] Solving this system of equations, we get \\(v_0 = 1/(1-p)\\).\nWe can generalize these ideas to find time of hitting a state.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#stationary-distribution",
    "href": "markov-chains.html#stationary-distribution",
    "title": "8  Markov chains",
    "section": "8.7 Stationary distribution",
    "text": "8.7 Stationary distribution\n\nA distribution \\(π\\) is said the be a stationary distribution if \\[\n   π = π P.\n\\]\nStationary distributions can be computed by solving balance equations.\nLet \\(C\\) be an irreducible class of a Markov chain. Then, there is a unique stationary distribution \\(π\\) that assigns positive probability only to states in \\(C\\).\nIf \\(π_1\\) and \\(π_2\\) are stationary distributions of a Markov chain and \\(α \\in (0,1)\\), then \\(α π_1 + (1-α) π_2\\) is also a stationary distribution.\nThus, if a Markov chain has a single irreducible class, then it has a unique stationary distribution; if it has multiple irreducible classes, then it has uncountable number of stationary distributions.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#limiting-distribution",
    "href": "markov-chains.html#limiting-distribution",
    "title": "8  Markov chains",
    "section": "8.8 Limiting distribution",
    "text": "8.8 Limiting distribution\n\nSuppose \\(j\\) is a transient state. Then, we know that \\(N_j &lt; ∞\\) and \\(G_{ij} &lt; ∞\\). Therefore, \\[\n   \\lim_{n \\to ∞}\n   \\frac{N^{(n)}_{j}}{n} = 0, a.s.,\n   \\quad\\text{and}\\quad\n   \\lim_{n \\to ∞}\n   \\frac{G^{(n)}_{ij}}{n} = 0, \\forall i \\in \\ALPHABET X.\n\\]\nSuppose \\(j\\) is a recurrent state. Then, \\[\n   \\lim_{n \\to ∞}\n   \\frac{N^{(n)}_{j}}{n} = μ_j, a.s.,\n   \\quad\\text{and}\\quad\n   \\lim_{n \\to ∞}\n   \\frac{G^{(n)}_{ij}}{n} = μ_j, \\forall i \\in \\ALPHABET X\n\\] where \\(μ_j = \\EXP_{j}[T_j]\\) is the mean return time to state \\(j\\).\n\nA Markov chain is said to have a steady state distribution if \\(π_n\\) converges to a limit as \\(n \\to ∞\\) and the limit does not depend on the initial distribution \\(π_0\\).\nA Markov chain has a steady state distribution if it is ergodic. We can find the steady state distribution by solving the balance equation: \\(π = π P\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#exercises",
    "href": "markov-chains.html#exercises",
    "title": "8  Markov chains",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 8.1 (Time reversal of Markov chains) Let \\(\\{X_n\\}_{n \\ge 1}\\) is a Markov chain. Show that for any \\(N &gt; n\\), \\[\n  \\PR(X_n = x_n \\mid X_{n+1:N} = x_{n+1:N})\n  = \\PR(X_n = x_n \\mid X_{n+1} = x_{n+1}).\n\\] Thus, a time reversed Markov chain is also Markov.\n\n\nExercise 8.2 Suppose \\(\\{X_n\\}_{n \\ge 0}\\) is a Markov chain with transition matrix \\(P\\). For a fixed positive integer \\(k\\), define \\(Y_n = X_{kn}\\). Show that \\(\\{Y_n\\}_{n \\ge 0}\\) is a Markov chain with transition matrix \\(P^k\\).\n\n\nExercise 8.3 Suppose a (6-sided) die is ‘fixed’ so that two consecutive rolls cannot have the same outcome. In particular, if the outcome of a roll is \\(i\\), then the next roll cannot be \\(i\\); all \\(5\\) other outcomes are equally likely.\n\nModel the above as a Markov chain.\nIf the outcome of the first roll is \\(1\\), what is the probability that the outcome of the \\(n\\)th roll is also \\(1\\)?",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  }
]