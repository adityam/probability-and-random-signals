[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability and Random Signals II",
    "section": "",
    "text": "Course Outline",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#general-information",
    "href": "index.html#general-information",
    "title": "Probability and Random Signals II",
    "section": "General Information (Fall 2025)",
    "text": "General Information (Fall 2025)\n\nInstructor\n\n\nAditya Mahajan\nOffice Hours: Tuesday 10:00am–11:00am\n\n\nTeaching Assistants\n\n\nZiqi Huang\n\n\nLectures\n\n\n8:35am–9:55am Monday, Wednesday (ENGTR 2100)\n\n\nTutorials\n\n\n12:35pm–1:25pm Friday, (ENGTR 2110)\n\n\nPrerequisites\n\n\nECSE 205 (Probability and Random Signals I)\nECSE 206 or ECSE 316 (Signals and Systems)\n\n\nCommunication\n\nUse the discussion board on myCourses for all questions related to the course. Only personal emails related to medical exceptions for missing a deliverable will be answered.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Probability and Random Signals II",
    "section": "Course Content",
    "text": "Course Content\n\n\n\n\n\n\n\nWeek\nMaterial Covered\n\n\n\n\n1\nProbability spaces, algebra of events, axioms of probability\n\n\n2\nRandom variables\n\n\n3\nRandom vectors\n\n\n4\nConditional probability and conditional expectation\n\n\n5\nMoment generating functions and sums of random variables\n\n\n6\nProbability inequalities\n\n\n7\nReview and Mid-Term\n\n\n8\nConvergence of random variables and the strong law of large numbers\n\n\n9\nMarkov chains\n\n\n10\nMarkov chains (continued)\n\n\n11\nPoisson processes\n\n\n12\nGaussian processes and minimum mean squared error estimation\n\n\n13\nWide sense stationary processes\n\n\n\nThe material for the lecture notes is taken from various sources including the textbook and the reference book, as well as the lecture notes of Prof. Ioannis Psaromiglikos (McGill) and Prof. Ashutosh Nayyar (USC).",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#course-material",
    "href": "index.html#course-material",
    "title": "Probability and Random Signals II",
    "section": "Course Material",
    "text": "Course Material\n\nTextbook\n\n\nGrimmett and Stirzaker, Probability and Random Processes, 4th edition, Oxford University Press, 2020.\n\n\n“Engineering” Graduate Probability textbooks\n\n\nJ.A. Gubner, Probability and Random Processes for Electrical and Computer Engineers, Cambridge University Press, 2006.\nS.H. Chan, Introduction to Probability for Data Science, Michigan Publishing, 2021.\nH. Hsu, Probability, Random Variables, and Random Processes, McGraw Hill, 1997.\n\n\nExercise books\n\n\nF. Mosteller, Fifty challenging problems in probability with solutions, Courier Corporation, 1987. Available online\nGrimmett and Stirzaker, One Thousand Exercises in Probability, Oxford University Press, 2000.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Probability and Random Signals II",
    "section": "Evaluation",
    "text": "Evaluation\n\nAssignments (25%) Weekly homework assignments. Typically, each assignment will consist of four or five questions, out of which one or two randomly selected questions will be graded.\nMid Term (25%) Closed book in-class exam. Oct 8 (during class time)\nFinal Exam (50%) Closed book, in-person exam. Will be scheduled by the exam office and the dates will be announced later.\nThe final exam will cover all the material seen in the class during the term.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#marking-policy",
    "href": "index.html#marking-policy",
    "title": "Probability and Random Signals II",
    "section": "Marking policy",
    "text": "Marking policy\n\nAssignments must be submitted electronically on myCourses as a PDF. You may write the assignments on paper and then scan them as a PDF (there are several such apps available for all phone platforms), or write on a tablet and convert to PDF, or type using a word processor.\nThere will no make-up examination for students who miss a mid-term.\n\nStudent who miss the exam due to a valid reason (see Faculty of Engineering policy) should notify the instructor within a week of the exam and provide necessary documentation.\nIf, and only if, proper documentation for a missed exam is presented, the marks for the missed exam will be shifted to the final exam.\nStudents who miss the mid-term exam for any other reason (e.g., no medical note, going to the exam at the wrong time, or on the wrong day, etc.) will get zero marks on the exam.\n\nAny request for reevaluation of a mid-term or an assignment must be made in writing within a week of its return. Note that requesting a re-grade will mean that you WHOLE assignment or exam will be re-graded.\n\n\nRight to submit in English or French written work that is to be graded.\n\nIn accord with McGill University’s Charter of Students’ Rights, students in this course have the right to submit in English or in French any written work that is to be graded.\n\nAcademic Integrity\n\nMcGill University values academic integrity. Therefore all students must understand the meaning and consequences of cheating, plagiarism and other academic offences under the Code of Student Conduct and Disciplinary Procedures (see McGill’s guide to academic honesty for more information).\nL’université McGill attache une haute importance à l’honnêteté académique. Il incombe par conséquent à tous les étudiants de comprendre ce que l’on entend par tricherie, plagiat et autres infractions académiques, ainsi que les conséquences que peuvent avoir de telles actions, selon le Code de conduite de l’étudiant et des procédures disciplinaires (pour de plus amples renseignements, veuillez consulter le guide pour l’honnêteté académique de McGill.)",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#course-delivery",
    "href": "index.html#course-delivery",
    "title": "Probability and Random Signals II",
    "section": "Course delivery",
    "text": "Course delivery\nThe course is taught in a “chalk and board” style; there will be no power point presentations. All students are expected to attend lectures and take notes. Partial notes on some of the material will be provided, but are not a substitute for the material covered in class.\n© Instructor-generated course materials (e.g., handouts, notes, summaries, exam questions) are protected by law and may not be copied or distributed in any form or in any medium without explicit permission of the instructor. Note that infringements of copyright can be subject to follow up by the University under the Code of Student Conduct and Disciplinary Procedures.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#additional-notes",
    "href": "index.html#additional-notes",
    "title": "Probability and Random Signals II",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nAs the instructor of this course I endeavor to provide an inclusive learning environment. However, if you experience barriers to learning in this course, do not hesitate to discuss them with me or contact the office of Student Accessibility and Achievement.\nEnd-of-course evaluations are one of the ways that McGill works towards maintaining and improving the quality of courses and the student’s learning experience. You will be notified by e-mail when the evaluations are available. Please note that a minimum number of responses must be received for results to be available to students.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "probability-spaces.html",
    "href": "probability-spaces.html",
    "title": "1  Probability spaces",
    "section": "",
    "text": "1.1 Background\nThis is a graduate course on probability and random signals. I am going to assume that everyone is familiar with the basics of undergraduate probability. For example, you should be able to answer the following questions:\nYou should also be familiar with the following concepts:\nSome of you might have also seen the following concepts\nIn this course, we will revisit these topics with a more formal approach.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#background",
    "href": "probability-spaces.html#background",
    "title": "1  Probability spaces",
    "section": "",
    "text": "A fair 6-sided die is rolled twice. What is the probability that the sum of the rolls equals 7?\nA biased coin with \\(\\PR({\\rm heads}) = 3/4\\) is tossed 10 times. What is the probability of obtaining 3 consecutive heads?\n\n\n\nRandom variables, probability distributions, and expectations\nConditional distributions and independent random variables\n\n\n\nThe law of large numbers\nThe central limit theorem",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#why-do-we-care-about-probability-theory",
    "href": "probability-spaces.html#why-do-we-care-about-probability-theory",
    "title": "1  Probability spaces",
    "section": "1.2 Why do we care about probability theory",
    "text": "1.2 Why do we care about probability theory\nProbability theory is used to model, analyze, and design various engineering systems. Broadly speaking, there are three forms of random phenomenon that arise in systems: noise, uncertainty, and randomness.\n\nNoise. Noise refers to effects beyond our control. For example, in a digital communication system, when a waveform indicating a binary \\(0\\) is transmitted, background radiation may distort the signal, causing the receiver to incorrectly decode it as \\(1\\). This interference can be modeled as noise in the channel. Similar examples arise in computer networks, photonics, and other areas.\nUncertainty. Uncertainty refers to effects that arise due to lack of information. For example, to bid in an electricity market, a wind farm may need to know how much energy it can generate in the next hour, but this depends on the speed of wind which depends on a lot of factors and is difficult to predict precise. This lack of knowledge can be modelled as uncertainty and the system modeler may have a quantified belief on that uncertainty based on historical data. Similar examples arise in circuits (where quality of a chip may depend on the vagaries in the manufacturing process).\nRandomness. Sometimes introducing randomness can improve system performance. For example, power of two choices algorithm in multi-server load balancing randomly selects two servers and assigns the task to the one with lighter load. This simple strategy effectively reduces the peak load in the servers without needing to query all servers. Similar algorithms are used in bandwidth allocation in ad-hoc networks and randomized routing in networks on chips.\n\nIn this course, we will not focus on how such probabilistic models are constructed. Instead we will study the mathematical properties these models should satisfy and explore the implications of those properties. For the modeling and application-specific details, consider the following courses:\n\nECSE 506: Stochastic Control and Decision Theory\nECSE 508: Multi-agent systems\nECSE 510: Filtering and Prediction for Stochastic Systems\nECSE 511: Introduction to Digital Communication\nECSE 515: Optical Fibre Communications\nECSE 518: Telecommunication Network Analysis\nECSE 521: Digital Communication 1\nECSE 541: Design of Multiprocessor Systems-on-Chip\nECSE 551: Machine Learning for Engineers\nECSE 554: Applied Robotics\nECSE 608: Machine Learning\nECSE 610: Wireless Communication\nECSE 620: Information Theory\nECSE 621: Statistical Detection and Estimation\nECSE 623: Digital Communications 2\nECSE 626: Statistical Computer Vision\n\nAs you can see, probability theory is a foundational tool for Electrical Engineering.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#review-of-set-theory",
    "href": "probability-spaces.html#review-of-set-theory",
    "title": "1  Probability spaces",
    "section": "1.3 Review of Set Theory",
    "text": "1.3 Review of Set Theory\n\nA set is a collection of objects. We say that a set \\(B\\) is a subset of set \\(A\\) (written as \\(B \\subseteq A\\)) if all elements of \\(B\\) are also elements of \\(A\\). We say that \\(B\\) is a proper subset (written \\(B \\subsetneq A\\)) if \\(B \\subseteq A\\) and \\(B \\neq A\\).\n\nExercise 1.1 Let \\(A = \\{1, 2, 3\\}\\). Find all subsets of \\(A\\).\n\nThe set of all subsets of \\(A\\) is also called the power set of \\(A\\) (denoted by \\(2^A\\)). The notation \\(2^A\\) represents that the power set of \\(A\\) contains \\(2^{|A|}\\) elements. For example, your answer to Exercise 1.1 must have \\(2^3 = 8\\) elements.\nGiven two sets \\(A\\) and \\(B\\), we define the set difference \\(A\\setminus B\\) to be all elements of \\(A\\) not in \\(B\\). Note that mathematically \\(A \\setminus B\\) is well defined even if \\(B \\not\\subseteq A\\). In particular \\[\nA \\setminus B = A \\setminus (A \\cap B).\n\\]\n\nExercise 1.2 Compute \\(A \\setminus B\\) for the following:\n\n\\(A = \\{1,2,3,4\\}\\) and \\(B = \\{1, 2\\}\\).\n\\(A = \\{1,2,3,4\\}\\) and \\(B = \\{1, 2, 5\\}\\).\n\n\nGiven a collection \\(\\{A_1, A_2, \\dots, A_n\\}\\) of sets, we define two operations:\n\nUnion \\(A_1 \\cup A_2  \\cup \\cdots \\cup A_n\\) as follows \\[\n  \\bigcup_{i=1}^n A_i = \\{ a: a \\in A_i \\text{ for some } i \\}.\n\\] This means that an element belongs to \\(A_1 \\cup A_2  \\cup \\cdots \\cup A_n\\) if it belongs to at least one of \\(A_1\\), \\(A_2\\), \\(\\ldots\\), \\(A_n\\).\nIntersection \\(A_1 \\cap A_2  \\cap \\cdots \\cap A_n\\) as follows \\[\n  \\bigcap_{i=1}^n A_i = \\{ a: a \\in A_i \\text{ for all } i \\}\n\\] This means that an element belongs to \\(A_1 \\cap A_2  \\cap \\cdots \\cap A_n\\) if it belongs to all of \\(A_1\\), \\(A_2\\), \\(\\ldots\\), \\(A_n\\).\n\nA collection \\(\\{A_1, A_2, \\dots, A_n\\}\\) is disjoint if for every \\(i \\neq j\\), \\(A_i \\cap A_j = \\emptyset\\), where \\(\\emptyset\\) denotes the empty set.\nGiven a universal set \\(Ω\\) and a collection \\(\\{B_1, B_2, \\dots, B_m\\}\\) of subsets of \\(Ω\\), we say that \\(\\{B_1, B_2, \\dots, B_m\\}\\) is a partition of \\(Ω\\) if \\(\\{B_1, B_2, \\dots, B_m\\}\\) are pairwise disjoint and their union equals the universal set \\(Ω\\).\n\n\n\n\n\n\nFigure 1.1: Example of a partition\n\n\n\n\nExample 1.1 Let \\(Ω = \\{1,2,3,4\\}\\). The following are partitions of \\(Ω\\):\n\n\\(\\{ \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\} \\}\\).\n\\(\\{ \\{1, 2\\}, \\{3, 4\\} \\}\\).\n\\(\\{ \\{1\\}, \\{2, 3\\}, \\{4\\} \\}\\).\n\nThe follow are not partitions of \\(Ω\\) [Explain why?]\n\n\\(\\{ \\{1\\}, \\{2\\}, \\{3\\}, \\}\\).\n\\(\\{ \\{1, 2, 3\\}, \\{3, 4\\} \\}\\).\n\\(\\{ \\{1\\}, \\{2, 3\\}, \\{4, 5\\} \\}\\).\n\n\nIn most of our discussion, we will work with a pre-specified universal set \\(Ω\\). In this setting we use \\(A^c\\) (read as the complement of \\(A\\)) as a short hand for \\(Ω\\setminus A\\).\n\n\n\n\n\n\nPartitions are useful because they allow breaking up a set into disjoint pieces. In particular, suppose \\(\\{B_1, \\dots, B_m\\}\\) is a partition and \\(A\\) is any subset of \\(Ω\\).\nThen, \\[\nA = (A \\cap B_1) \\cup (A \\cap B_2) \\cup \\cdots \\cup (A \\cap B_m)\n\\] where each of the components is disjoint.\n\n\n\nProperties of set operations\n\nCommutative \\[A \\cup B = B \\cup A\n\\quad\\text{and}\\quad\nA \\cap B = B \\cap A\\]\nAssociative \\[A \\cup (B \\cup C)= (A \\cup B) \\cup C\n\\quad\\text{and}\\quad\nA \\cap (B \\cap C)= (A \\cap B) \\cap C\\]\nDistributive \\[A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cap C)\n\\quad\\text{and}\\quad\nA \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\]\nDe Morgan’s Law \\[(A \\cup B)^c = A^c \\cap B^c\n\\quad\\text{and}\\quad\n(A \\cap B)^c = A^c \\cup B^c\\]\n\n\nExercise 1.3 Use distributive property to simplify:\n\n\\([1,4] \\cap ([0,2] \\cup [3,5])\\).\n\\([2,4] \\cup ([3,5] \\cap [1,4])\\).\n\n\nGiven a universal set \\(Ω\\), a collection \\(\\ALPHABET F = \\{F_1, F_2, \\dots, F_m\\}\\) of subsets of \\(Ω\\) is called an algebra if it satisfies the following properties:\n\n\\(\\emptyset \\in \\ALPHABET F\\) and \\(Ω \\in \\ALPHABET F\\).\nClosed under complements: if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nClosed under finite unions and finite intersections: if \\(A_1, \\dots, A_n \\in \\ALPHABET F\\), then \\[\nA_1 \\cup A_2 \\cup \\cdots \\cup A_n \\in \\ALPHABET F\n\\quad\\text{and}\\quad\nA_1 \\cap A_2 \\cap \\cdots \\cap A_n \\in \\ALPHABET F\n\\]\n\nWe will sometimes use the notation “\\((Ω,\\ALPHABET F)\\) is an algebra of sets” or “\\(\\ALPHABET F\\) is an algebra on \\(Ω\\)”. Some examples of algebras are as follows:\n\nThe smallest algebra associated with \\(Ω\\) is \\(\\{\\emptyset, Ω\\}\\).\nIf \\(A\\) is any subset of \\(Ω\\), then \\(\\{\\emptyset, A, A^c, Ω\\}\\) is an algebra.\nFor any set \\(Ω\\), the power-set \\(2^Ω\\) is an algebra on \\(Ω\\). As an illustration, check that the power set defined in Exercise 1.1 is an algebra.\n\nThese are all examples of a general principle: The power-set of any partition of a set is an algebra.\n\nIf the partition is \\(\\{Ω\\}\\), then the power-set \\(\\{ \\emptyset, Ω \\}\\) is an algebra.\nIf the partition is \\(\\{A, A^c\\}\\), then the power-set \\(\\{ \\emptyset, A, A^c, Ω\\}\\) is an algebra.\nIf the partition is the collection of all singleton elements of a set, then the power-set \\(2^Ω\\) is an algebra.\n\nWe will use the notation \\(σ(\\ALPHABET D)\\) to denote the algebra generated by a partition \\(\\ALPHABET D\\).\nThe reverse is also true. If \\(\\ALPHABET F\\) is an algebra on a finite space \\(Ω\\), then there is a unique partition \\(\\ALPHABET D = \\{D_1, \\dots, D_m\\}\\) such that \\(\\ALPHABET F = 2^{\\ALPHABET D}\\). The elements \\(D_i\\) are called atoms of \\(\\ALPHABET F\\).\nLet \\(\\ALPHABET D_1\\) and \\(\\ALPHABET D_2\\) be two partitions of \\(Ω\\). We say \\(\\ALPHABET D_1\\) is finer than \\(\\ALPHABET D_2\\) if \\(σ(\\ALPHABET D_1) \\subseteq σ(\\ALPHABET D_2)\\). For instance, suppose \\(Ω = \\{1, 2, 3, 4\\}\\) and \\[\n  \\ALPHABET D_1 = \\{ \\{1,2,3\\}, \\{4\\} \\}\n  \\quad\\text{and}\\quad\n  \\ALPHABET D_2 = \\{ \\{1,2\\}, \\{3\\}, \\{4\\} \\}\n\\] then, \\(\\ALPHABET D_1\\) is finer than \\(\\ALPHABET D_2\\).\nIf a partition \\(\\ALPHABET D_1\\) is finer than \\(\\ALPHABET D_2\\), we also say that \\(\\ALPHABET D_2\\) is coarser than \\(\\ALPHABET D_1\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#functions",
    "href": "probability-spaces.html#functions",
    "title": "1  Probability spaces",
    "section": "1.4 Functions",
    "text": "1.4 Functions\n\nA function \\(f\\) is a rule that assign each input from a set \\(\\ALPHABET X\\) (called the domain) to exactly one output in another set \\(\\ALPHABET Y\\) (called the co-domain). Symbolically, this is written as \\[\n  f \\colon \\ALPHABET X \\to \\ALPHABET Y\n\\] and we say \\(f\\) maps \\(\\ALPHABET X\\) to \\(\\ALPHABET Y\\).\nThe set of values \\(\\{ f(x) : x \\in \\ALPHABET X\\}\\) is called the range. By definition, the range is a subset of the co-domain \\(\\ALPHABET Y\\), but the range may or may not be equal to \\(\\ALPHABET Y\\). For example, consider the function \\(f \\colon \\reals \\to \\reals\\) defined by \\(f(x) = x^2\\). The range of the function is \\(\\reals_{\\ge 0}\\) which is a strict subset of the co-domain \\(\\reals\\).\nA function is called onto or surjective if its range is equal to its co-domain.\nA function is called one-to-one or injective if different inputs maps to different outputs.\nA function which is both onto and one-to-one is called bijective. A bijective function is invertible because for every \\(y \\in \\ALPHABET Y\\), there is a unique \\(x\\) such that \\(f(x) = y\\).\nFor a set \\(B \\subset \\ALPHABET Y\\), the preimage or inverse image of \\(B\\) under \\(f\\) is defined as \\[\n  f^{-1}(B) = \\{ x \\in \\ALPHABET X : f(x) \\in  B \\},\n\\] which is a subset of \\(\\ALPHABET X\\).\n\n\nExample 1.2 Consider the following functions:\n\n\\(f_1 \\colon \\reals \\to \\reals\\)\n\\(f_2 \\colon \\reals \\to \\reals_{\\ge 0}\\)\n\\(f_3 \\colon \\reals_{\\ge 0} \\to \\reals\\)\n\\(f_4 \\colon \\reals_{\\ge 0} \\to \\reals_{\\ge 0}\\)\n\nall given by \\(f_i(x) = x^2\\), \\(i \\in \\{1, \\dots, 4\\}\\).\nExplain if each of the function is onto, into, or bijective.\n\n\nExample 1.3 Let \\(f \\colon \\reals \\to \\reals\\) be given by \\(f(x) = 2x + 1\\). Find \\(f^{-1}([3,7])\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe need to solve for \\(2x + 1 \\in [3,7]\\), which is equivalent to \\[\n  3 \\le 2x + 1 \\le 7\n\\] which is equivalent to \\[\n  1 \\le x \\le 3.\n\\]\nThus, \\(f^{-1}([3,7]) = [1,3]\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#mathematical-model-of-probability",
    "href": "probability-spaces.html#mathematical-model-of-probability",
    "title": "1  Probability spaces",
    "section": "1.5 Mathematical model of probability",
    "text": "1.5 Mathematical model of probability\n\nTo mathematically model probability statements, we need to model the sequence of events that may lead to the occurrence of \\(A\\): this is called a random experiment; the result of an experiment is called an outcome.\nIn general, the outcome of an experiment is not certain. We can only talk about the collection of possible outcomes. The collection of possible outcomes of an experiment is called the sample space and denoted by \\(Ω\\).\n\nExercise 1.4 What is the sample space for the toss of a coin?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\(Ω = \\{ H, T \\}\\).\n\n\n\n\nExercise 1.5 What is the sample space for the roll of a (6-sided) die?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\(Ω = \\{ 1,2,3,4,5,6 \\}\\).\n\n\n\nAn event is any subset of the sample space. If the outcome of the random experiment belongs to the event \\(A\\), we say that “the event \\(A\\) has occurred”. Some examples of events are:\n\nHead occurs in Exercise 1.4 (\\(A = \\{H\\}\\))\nBoth head and tail occur in Exercise 1.4 (\\(A = \\emptyset\\); this is an event that cannot happen, sometimes called the impossible event)\nAn even number is thrown in Exercise 1.5 (\\(A = \\{2,4,6\\}\\)).\n\nNote that events are subset of the sample space but not all subsets of a sample space may be events. The reasons are too complicated to explain, but the high-level explanation is that everything is okay for discrete sample spaces, but weird things can happen in continuous sample spaces.\nProbability (denoted by \\(\\PR\\)) is a function which assigns a number between \\(0\\) and \\(1\\) to every event. This number indicates what is the chance that the event occurs. Such a function should satisfy some axioms, which we will explain below.\nFirst, to define a function, we need to define it’s domain and co-domain Let’s denote the domain (i.e., the set of all events to which we can assign a probability) by \\(\\ALPHABET F\\). We expect probability to satisfy certain properties, which imposes constraints on the domain:\n\nProbability of an impossible event (e.g., getting both heads and tails when we toss a coin) should be zero. Thus, \\(\\emptyset \\in \\ALPHABET F\\).\nProbability of something happening (e.g., getting either a head or a tail when we toss a coin) should be one. Thus, \\(Ω \\in \\ALPHABET F\\).\nIf we assign probability to an event \\(A\\) then we should be able to assign probability to “\\(A\\) does not occur” i.e., \\(A^c\\). Thus, if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nIf we can talk about probability of \\(A\\) and \\(B\\), then we should be able to talk about probability that either \\(A\\) or \\(B\\) occurs and both \\(A\\) and \\(B\\) occur. Thus, if \\(A, B \\in \\ALPHABET F\\), then \\(A \\cup B \\in \\ALPHABET F\\) and \\(A \\cap B \\in \\ALPHABET F\\).\n\nThus, the domain of \\(\\PR\\) must be an algebra! However, when we go beyond finite sample spaces, being an algebra is not sufficient. But we first provide some examples of probability for finite sample spaces.\n\nExample 1.4 (Uniform probability) Consider a finite set \\(Ω\\) with \\(\\ALPHABET F = 2^Ω\\). The uniform probability \\(\\PR\\) on \\(Ω\\) is given by \\[\n\\PR(A) = \\frac{\\ABS{A}}{\\ABS{Ω}},\n\\quad \\forall A \\in \\ALPHABET F\n\\]\n\nAn illustration of uniform probability distribution on the outcomes of a fair coin or a fair dice.\nIn general, when \\(Ω = \\{ω_1, \\dots, ω_n\\}\\) is finite and \\(\\ALPHABET F = 2^{Ω}\\), we can think of probability as an assignment of a weight \\(p(ω_i)\\) to each outcome \\(ω_i\\) such that\n\n\\(0 \\le p(ω_i) \\le 1\\)\n\\(p(ω_1) + \\cdots + p(ω_n) = 1\\).\n\nThen, for any event \\(A \\in \\ALPHABET F\\) (i.e., any \\(A \\subseteq Ω\\), we define \\[\n\\PR(A) = \\sum_{i : ω_i \\in A } p(ω_i).\n\\] This is effectively how probability was defined in undergraduate probability.\n\nExample 1.5 Consider a six-sided die where \\(Ω = \\{1, 2, \\dots, 6 \\}\\), \\(\\ALPHABET F = 2^Ω\\) and \\(\\PR\\) is given by \\[\n\\PR(A) = \\sum_{ω \\in A} p(ω),\n\\quad \\forall A \\in \\ALPHABET F\n\\] where \\[\np(1) = p(2) = p(3) = p(4) = p(5) = \\frac 2{15}\n\\quad\\text{and}\\quad\np(6) = \\frac{1}{3}.\n\\]\nVerify that\n\n\\(\\PR(Ω) = 1\\)\n\\(\\PR(\\{2,3,5\\}) = \\frac{6}{15}\\).\n\\(\\PR(\\{1,3,4,5\\}) = \\frac{8}{15}\\).\n\n\nExample 1.5 can be visualized as follows. First we visualize the probability space as in Figure 1.2.\n\n\n\n\n\n\nFigure 1.2: Probability space of Example 1.5\n\n\n\nTo compute \\(\\PR(\\{2, 3, 5\\})\\), we look at the event \\(A = \\{2, 3, 5\\}\\) and count the probability of all the cells inside \\(A\\), as shown in Figure 1.3.\n\n\n\n\n\n\nFigure 1.3: Computing \\(\\PR(A)\\)\n\n\n\nWe now present an example to illustrate that restricting the domain of \\(\\PR\\) to be an algebra is not sufficient.\n\nExample 1.6 A coin is tossed repeatedly until a head turns up. The sample space is \\(Ω = \\{ω_1, ω_2, \\dots\\}\\) where \\(ω_n\\) denotes the event that the first \\(n-1\\) tosses are tails followed by a head.\n\nSuppose we are interested in finding the probability of the event that the coin is tossed an even number of times, i.e., \\(A = \\{ω_2, ω_4, \\dots\\}\\). Note that \\(ω_2, ω_4, \\dots \\in \\ALPHABET F\\). However, \\(A\\) is a set. If we want to assign probability to \\(A\\) in terms of probability of \\(ω_n\\), we require \\(\\ALPHABET F\\) to be closed under countable unions. This motivates the following definition.\nGiven a universal set \\(Ω\\), a collection \\(\\ALPHABET F = \\{F_1, F_2, \\dots\\}\\) of subsets of \\(Ω\\) is called a \\(\\boldsymbol{σ}\\)-algebra if it satisfies the following properties:\n\n\\(\\emptyset \\in \\ALPHABET F\\) and \\(Ω \\in \\ALPHABET F\\).\nClosed under complements: if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nClosed under countable unions: if \\(A_1, A_2, \\dots \\in \\ALPHABET F\\), then \\[\n\\bigcup_{n=1}^∞ A_n \\in \\ALPHABET F\n\\]\n\n\n\n\n\n\n\nThe distinction between algebras and \\(σ\\)-algebras is technical. The reason that we need to consider \\(σ\\)-algebras is to do with the definition of probability on continuous sample spaces. Take \\(Ω = [0,1]\\) and consider a random experiment where “any outcome is equally likely”. Intuitively we capture this feature by assuming that for any interval \\([a,b]\\) with \\(0 \\le a \\le b \\le 1\\), we have \\[\\begin{equation}\\label{eq:uniform}\n\\PR([a,b]) = b - a.\n\\end{equation}\\]\nWe have seen that if we want \\(\\PR\\) to be a meaningful measure, the domain \\(\\ALPHABET F\\) must at least be an algebra. We have also seen that the power-set \\(2^Ω\\) is always an algebra. So, it is tempting to take \\(\\ALPHABET F = 2^{[0,1]}\\). However, it turns out that \\(2^{[0,1]}\\) includes some weird sets (technically, non-measurable sets) due to which we cannot define a function \\(\\PR\\) on \\(2^{[0,1]}\\) that satisfies \\(\\eqref{eq:uniform}\\).\nTo workaround this technical limitation, we revisit the minimum requirements that we need from the domain of \\(\\PR\\). Since we are interested in \\(\\PR([a,b])\\), \\(\\ALPHABET F\\) must contain intervals (and therefore all finite unions and intersections of intervals). Since we are working with continuous sample spaces, we also want \\(\\PR\\) to be continuous, i.e., for any sequence of sets \\(\\{A_n\\}_{n \\ge 1}\\), we want \\(\\PR(\\lim_{n \\to ∞} A_n) = \\lim_{n \\to ∞} \\PR(A_n)\\). It turns out that the additional requirement of continuity implies that \\(\\ALPHABET F\\) must be closed under countable unions as well. Thus, the domain \\(\\ALPHABET F\\) must at least be a \\(σ\\)-algebra.\nSo, we restrict to the simplest choice of the domain \\(\\ALPHABET F\\) needed for \\(\\eqref{eq:uniform}\\) and continuity to hold. For technical reasons, we need another property known as completeness. See Sec. 1.6 of the textbook.\n\n\n\n\n\n\n\n\n\nTip\\(σ\\)-algebra generated by a collection and Borel \\(σ\\)-algebra\n\n\n\nGiven collection \\(\\ALPHABET S\\) of subsets of \\(Ω\\), we have the following:\n\nThe power-set \\(2^Ω\\) contains \\(\\ALPHABET S\\). Therefore, there is at least one \\(σ\\)-algebra containing \\(\\ALPHABET S\\).\nIf \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) are \\(σ\\)-algebras containing \\(\\ALPHABET S\\), then \\(\\ALPHABET F_1 \\cap \\ALPHABET F_2\\) also contains \\(\\ALPHABET S\\).\n\nThus, if we take the intersection of all \\(σ\\)-algebras containing \\(\\ALPHABET S\\), we get the smallest \\(σ\\)-algebra containing \\(\\ALPHABET S\\), which is sometimes denoted by \\(σ(\\ALPHABET S)\\).\nOne commonly used \\(σ\\)-algebra is the Borel \\(σ\\)-algebra, which is defined as follows. Let \\(Ω\\) be a subset of \\(\\reals\\) and \\(\\ALPHABET S\\) be the collection of all open intervals in \\(Ω\\). Then \\(σ(\\ALPHABET S)\\) is called the “Borel \\(σ\\)-algebra on Ω” and often denoted by \\(\\mathscr{B}(Ω)\\).\nNote: Borel \\(σ\\)-algebra is usually defined for any topological space. We restrict our definition to subsets of reals.\n\n\nA pair \\((Ω, \\ALPHABET F)\\) where \\(Ω\\) is a set and \\(\\ALPHABET F\\) is a \\(σ\\)-algebra on \\(Ω\\) is called a measurable space.\n\nDefinition 1.1 (Probability) Given a measurable space \\((Ω, \\ALPHABET F)\\), probability \\(\\PR \\colon \\ALPHABET F \\to [0,1]\\) is a function that satisfies the following axioms of probability\n\nNon-negativity. \\(\\PR(A) \\ge 0\\).\nNormalization. \\(\\PR(Ω) = 1\\).\nCountable additivity. If \\(A_1, A_2, \\dots Ω\\) is a collection of disjoint events in \\(\\ALPHABET F\\), then, \\[\n\\PR\\biggl( \\bigcup_{n=1}^∞ A_n \\biggr) =\n\\sum_{n=1}^∞ \\PR(A_n).\n\\]\n\nThe collection \\((Ω, \\ALPHABET F, \\PR)\\) is called a probability space.\n\nSome immediate implications of the axioms of probability are the following.\n\nLemma 1.1 (Properties of probability measures)  \n\nProbability of complement. \\(\\PR(A^c) = 1 - \\PR(A)\\).\nMonotonicity. If \\(A \\subset B\\), then \\(\\PR(B) = \\PR(A) + \\PR(B \\setminus A) \\ge \\PR(A)\\).\nInclusion-exclusion. Given two events \\(A\\) and \\(B\\), \\[\n\\PR(A \\cup B) = \\PR(A) + \\PR(B) - \\PR(A \\cap B).\n\\]\nContinuity. Let \\(A_1, A_2, \\dots\\) be (weakly) increasing sequence of events, i.e., \\(A_1 \\subseteq A_2 \\subseteq A_3 \\subseteq \\cdots\\). Define \\[\n  A = \\lim_{n \\to ∞} A_n = \\bigcup_{n=1}^∞ A_n.\n\\] Then, \\[\n  \\PR(A) = \\lim_{n \\to ∞} \\PR(A_n).\n\\]\nSimilarly, let \\(B_1, B_2, \\dots\\) be (weakly) decreasing sequence of events, i.e., \\(B_1 \\supseteq B_2 \\supseteq B_3 \\supseteq \\cdots\\). Define \\[\n   B = \\lim_{n \\to ∞} B_n = \\bigcup_{n=1}^∞ B_n.\n\\] Then, \\[\n   \\PR(B) = \\lim_{n \\to ∞} \\PR(B_n).\n\\]\nUnion bound. For any sequence of events \\(\\{A_n\\}_{n \\ge 1}\\), we have \\[\n\\PR\\biggl( \\bigcup_{n=1}^{∞} A_n \\biggr) \\le\n\\sum_{n=1}^{∞} \\PR(A_n).\n\\]\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof of parts (a)–(c) is elementary and left as an exercise. Part (d) is more technical and is essentially equivalent to countable additivity. See the textbook for a proof. Union bound is an immediate consequence of inclusion-exclusion and continuity.\n\n\n\nLet \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr B[0,1]\\), and \\(\\PR\\) be any probability measure on \\((Ω, \\ALPHABET F)\\). Take any \\(a \\in (0,1)\\).\n\nConsider \\(A_n = \\bigl[0, a - \\frac 1n \\bigr)\\). Then \\(A = \\lim_{n \\to ∞} A_n =  [0, a)\\).\nConsider \\(A_n = \\bigl[0, a - \\frac 1n \\bigr]\\). Then \\(A = \\lim_{n \\to ∞} A_n =  [0, a)\\).\nConsider \\(B_n = \\bigl[0, 1 + \\frac 1n \\bigr)\\). Then \\(B = \\lim_{n \\to ∞} B_n = [0,a]\\).\nConsider \\(B_n = \\bigl[0, 1 + \\frac 1n \\bigr]\\). Then \\(B = \\lim_{n \\to ∞} B_n = [0,a]\\).\n\nIn these examples, continuity implies that \\(\\PR(A) = \\lim_{n \\to ∞} \\PR(A_n)\\) and \\(\\PR(B) = \\lim_{n \\to ∞} \\PR(B_n)\\).\n\n\n\n\n\n\nTipSome terminology\n\n\n\n\nAn event \\(A\\) is called null if \\(\\PR(A) = 0\\). Null event should not be confused with impossible event \\(\\emptyset\\).\nWe say that \\(A\\) occurs almost surely (abbreviated to a.s.) if \\(\\PR(A) = 1\\).\n\n\n\nConsider \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr B([0,1])\\), and \\(\\PR\\) to be the uniform probability distribution on \\(Ω\\). Consider the event \\(A\\) that the outcome is a rational number. \\(A\\) is a countable set (because the set of rational numbers is countable). For any \\(x \\in A\\), \\(\\{x\\} \\in \\ALPHABET F\\), and \\(\\PR(\\{x\\}) = 0\\) (we can infer this from the previous exercise by thinking of \\(\\{x\\}\\) as the limit of intervals \\(\\bigl[x, x+ \\frac 1n\\bigr]\\)). Thus, by countable additivity, \\(\\PR(A) = 0\\). Hence, \\(A\\) is null.\nThe above analysis implies that \\(\\PR(A^c) = 1\\), thus the event that the outcome is irrational occurs almost surely.\n\n\nExercise 1.6 Consider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) and events \\(A, B \\in \\ALPHABET F\\). Using axioms of probability, show that:\n\nIf \\(B \\subseteq A\\), then \\(\\PR(A \\setminus B) = \\PR(A) - \\PR(B)\\). Hence, argue that \\(\\PR(A) \\ge \\PR(B)\\).\n\\(\\PR(A \\cup B) = \\PR(A) + \\PR(B) - \\PR(A \\cap B)\\).\n\\(\\PR(A) = \\PR(A \\cap B) + \\PR(A \\cap B^c)\\).\n\n\n\nExercise 1.7 Consider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) and events \\(A, B, C \\in \\ALPHABET F\\). Prove that \\[\n    \\PR(A \\cup B \\cup C) = 1 - \\PR(A^c \\mid B^c \\cap C^c) \\PR(B^c \\mid C^c) \\PR(C^c).\n  \\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#conditional-probability",
    "href": "probability-spaces.html#conditional-probability",
    "title": "1  Probability spaces",
    "section": "1.6 Conditional Probability",
    "text": "1.6 Conditional Probability\n\nConditional probabilities quantify the uncertainty of an event when it is known that another event has occurred\n\nDefinition 1.2 Let \\((Ω,\\ALPHABET F, \\PR)\\) be a probability space and \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(B) &gt; 0\\). Then, the conditional probability that \\(A\\) occurs given that \\(B\\) occurs is defined as \\[\n\\PR(A | B) = \\dfrac{ \\PR(A \\cap B) }{ \\PR(B) }.\n\\]\n\nThe notation \\(\\PR(A | B)\\) is read as “probability of \\(A\\) given \\(B\\)” or “probability of \\(A\\) conditioned on \\(B\\)”.\n\nExercise 1.8 Suppose we roll a fair six-sided die (a fair die means that all outcomes are equally likely). Consider the events \\(A\\) that the outcomes is prime and \\(B\\) that the outcome is a multiple of \\(3\\). Compute \\(\\PR(A | B)\\) and \\(\\PR(B | A)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe have \\(Ω = \\{1, 2, 3, 4, 5, 6\\}\\), \\(A = \\{2, 3, 5\\}\\), and \\(B = \\{3, 6\\}\\). Note that \\(\\PR(A) = \\frac 12\\) and \\(\\PR(B) = \\frac 13\\).\nThus, \\[ \\PR(A | B) = \\frac{ \\PR(A \\cap B) }{ \\PR(B) }\n= \\frac{ \\PR(\\{3\\}) }{ \\PR(\\{3,6\\}) }\n= \\frac{ \\ABS{\\{3\\}} }{ \\ABS{\\{3,6\\}} } = \\frac {1}{2}.\n\\] Similarly, \\[ \\PR(B | A) = \\frac{ \\PR(B \\cap A) }{ \\PR(A) }\n= \\frac{ \\PR(\\{3\\}) }{ \\PR(\\{2,3,5\\}) }\n= \\frac{ \\ABS{\\{3\\}} }{ \\ABS{\\{2,3,5\\}} } = \\frac {1}{3}.\n\\]\n\n\n\n\nExercise 1.9 Suppose we roll two fair six-sided dice. Consider the event \\(A\\) that the maximum of the two rolls is less than or equal to \\(8\\) and the event \\(B\\) that the minimum of the two rolls is greater than or equal to \\(6\\). Compute \\(\\PR(A|B)\\) and \\(\\PR(B|A)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nNote that \\(Ω = \\{ 1, 2,3, 4, 5, 6\\}^2\\) and \\(\\PR\\) is uniform probability on all outcomes. The sets \\(A\\), \\(B\\), and \\(A \\cap B\\) are shown in Figure 1.4. Note that \\(\\PR(A) = \\PR(B) = \\frac{26}{36} = \\frac{13}{18}\\).\n\n\n\n\n\n\nFigure 1.4: The different events in Exercise 1.9\n\n\n\nThus, we have \\[\n\\PR(A|B) = \\frac{ \\PR(A \\cap B) } { \\PR(B) }\n= \\frac{ \\ABS{ A \\cap B} }{ \\ABS{B} }\n= \\frac{16}{26} = \\frac{8}{13}\n\\] and \\[\n\\PR(B|A) = \\frac{ \\PR(B \\cap A) } { \\PR(A) }\n= \\frac{ \\ABS{ B \\cap A} }{ \\ABS{A} }\n= \\frac{16}{26} = \\frac{8}{13}\n\\]\n\n\n\n\n\n\n\n\n\nTipConditional probabilities are probabilities\n\n\n\nConditional probabilities are legitimate probability measures on \\((Ω, \\ALPHABET F)\\). In particular, fix event \\(B\\) with \\(\\PR(B) &gt; 0\\). Then\n\n\\(\\PR(A \\mid B) \\ge 0\\).\n\\(\\PR(Ω \\mid B) = \\dfrac{\\PR(Ω \\cap B)}{\\PR(B)} = 1\\).\nFor disjoint events \\(A_1, A_2 \\in \\ALPHABET F\\), \\[\\PR(A_1 \\cup A_2 \\mid B) =\n\\frac{ \\PR( (A_1 \\cup A_2) \\cap B) }{ \\PR(B) } =\n\\frac{ \\PR( (A_1 \\cap B) \\cup (A_2 \\cap B) ) }{ \\PR(B) } =\n\\frac{ \\PR(A_1 \\cap B) + \\PR (A_2 \\cap B) }{ \\PR(B) } =\n\\PR(A_1 \\mid B) + \\PR(A_2 \\mid B)\\] where we have used the fact that \\((A_1 \\cap B)\\) and \\((A_2 \\cap B)\\) are disjoint.\n\n\n\n\nExercise 1.10 Given an event \\(B\\) with \\(\\PR(B) &gt; 0\\), show that\n\n\\(\\PR(A^c | B) = 1 - P(A|B)\\).\n\\(\\PR(A_1 \\cup A_2 | B) = \\PR(A_1 | B) + \\PR(A_2 | B) - \\PR(A_1 \\cap A_2 | B)\\).\nIf \\(A_1 \\subset A_2\\) then \\(\\PR(A_1 | B) \\le \\PR(A_2 | B)\\).\n\n\nThe definition of conditional probability gives rise to the chain rule.\n\nLemma 1.2 (Chain rule of probability) Let \\(A\\) and \\(B\\) be events in a probability space \\((Ω, \\ALPHABET F, \\PR)\\).\n\nIf \\(\\PR(B) &gt; 0\\), then \\(\\PR(A \\cap B) = \\PR(A | B) \\PR(B)\\).\nIf \\(\\PR(A) &gt; 0\\), then \\(\\PR(A \\cap B) = \\PR(B | A) \\PR(A)\\).\n\n\nCombining the chain rule with the basic properties of partitions, we get the law of total probability.\n\nLemma 1.3 (Law of total probability) Let \\(\\{B_1, B_2, \\dots, B_m\\}\\) be a partition of \\(Ω\\) such that \\(\\PR(B_i) &gt; 0\\) for all \\(i\\). Then, \\[\n\\PR(A) = \\sum_{i=1}^m \\PR(A \\cap B_i)\n= \\sum_{i=1}^m \\PR(A | B_i) \\PR(B_i).\n\\]\n\n\n\n\n\n\nFigure 1.5: Illustration of Law of total probability\n\n\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nConsider \\(m=2\\), in which case the result can be simplified as \\[\\PR(A) = \\PR(A|B)\\PR(B) + \\PR(A|B^c) \\PR(B^c).\\]\nTo prove this observe that \\[\\begin{equation}\\label{eq:two-step}\nA = A \\cap (B \\cup B^c) = (A \\cap B) \\cup (A \\cap B^c).\n\\end{equation}\\] The events \\(A \\cap B\\) and \\(A \\cap B^c\\) are disjoint. Therefore, by additivity, we have \\[\n\\PR(A) = \\PR(A \\cap B) + \\PR(A \\cap B^c).\n\\] Then, by the definition of conditional probability, we have \\(\\PR(A \\cap B) = \\PR(A|B) \\PR(B)\\) and \\(\\PR(A \\cap B^c) = \\PR(A|B^c) \\PR(B^c)\\). Substituting in the above, we get \\(\\eqref{eq:two-step}\\).\nThe argument for the general case is similar.\n\n\n\n\nExercise 1.11 There are two routes for a packet to be transmitted from a source to the destination.\n\nThe packet takes route \\(R_1\\) with probability \\(\\frac 34\\) and takes route \\(R_2\\) with probability \\(\\frac 14\\).\nOn route \\(R_1\\), the packet is dropped with probability \\(\\frac 13\\).\nOn route \\(R_2\\), the packet is dropped with probability \\(\\frac 14\\).\n\nFind the probability that the packet reaches the destination.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe start by defining some events: Let \\(R_1\\) denote the event that the packet took route \\(R_1\\) and \\(R_2\\) denote the event that the packet took route \\(R_2\\). Let \\(D\\) denote the event that the packet was dropped.\nThen, the information given in the question can be written as:\n\n\\(\\PR(R_1) = \\frac 34\\) and \\(\\PR(R_2) = \\frac 14\\).\n\\(\\PR(D | R_1) = \\frac 13\\). Thus, \\(\\PR(D^c | R_1) = 1 - \\PR(D | R_1) = \\frac 23\\).\n\\(\\PR(D | R_2) = \\frac 14\\). Thus, \\(\\PR(D^c | R_2) = 1 - \\PR(D | R_2) = \\frac 34\\).\n\nThen, by the law of total probability, we have \\[\\begin{align*}\n\\PR(D^c) &= \\PR(D^c | R_1) \\PR(R_1) + \\PR(D^c | R_2) \\PR(R_2) \\\\\n&= \\frac 34 \\frac 23 + \\frac 14 \\frac 34\n= \\frac {11}{16}.\n\\end{align*}\\]\n\n\n\n\nLemma 1.4 (Bayes rule) For any events \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(A), \\PR(B) &gt; 0\\), we have \\[\n\\PR(B|A) = \\dfrac{\\PR(A|B)\\PR(B)}{\\PR(A)}.\n\\]\nIn general, if \\(\\{B_1, B_2, \\dots, B_m\\}\\) is a partition of \\(Ω\\) such that \\(\\PR(B_i) &gt; 0\\) for all \\(i\\). Then, \\[\n\\PR(B_i|A) =\n\\dfrac{ \\PR(A|B_i) \\PR(B_i) }\n{\\displaystyle \\sum_{j=1}^m \\PR(A|B_j) \\PR(B_j)}\n\\] where we have used the law of total probability (Lemma 1.3) in the denominator.\n\n\nExercise 1.12 Consider the model of Exercise 1.11. Suppose we know that the packet was dropped. What is the probability that it was transmitted via route \\(R_1\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall the events \\(R_1\\), \\(R_2\\), and \\(D\\) defined in the solution of Exercise 1.11. We were given that \\[\\PR(R_1) = \\frac 34, \\quad \\PR(R_2) = \\frac 14, \\quad\n\\PR(D|R_1) = \\frac 13, \\quad \\PR(D|R_2) = \\frac 14.\\] We had compute that \\[\n\\PR(D) = 1 - \\PR(D^c) = \\frac 5{16}.\n\\]\nThus, by Bayes rule, we have \\[\n\\PR(R_1 | D) = \\frac{ \\PR(D | R_1) \\PR(R_1) }{ \\PR(D) }\n= \\frac{ \\frac 13 \\frac 34 } { \\frac 5{16} } = \\frac 45.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#independence",
    "href": "probability-spaces.html#independence",
    "title": "1  Probability spaces",
    "section": "1.7 Independence",
    "text": "1.7 Independence\n\nIn general, the knowledge that an event \\(B\\) has occurred changes the probability of event \\(A\\), since \\(\\PR(A)\\) is replaced by \\(\\PR(A|B)\\). If the knowledge that \\(B\\) has occurred does not does not change our belief about \\(A\\), i.e., when \\(\\PR(A|B) = \\PR(A)\\), we say “\\(A\\) and \\(B\\) are independent”. This leads to the following definition.\n\nDefinition 1.3 The events \\(A, B \\in \\ALPHABET F\\) are called independent if \\[\n\\PR(A|B) = \\PR(A)\n\\quad\\text{or}\\quad\n\\PR(B|A) = \\PR(B).\n\\] An alternative but equivalent definition is \\[\n\\PR(A \\cap B) = \\PR(A) \\PR(B).\n\\]\nWe will use the notation \\(A \\independent B\\) to denote that the events \\(A\\) and \\(B\\) are independent.\n\n\nExample 1.7 The events \\(A\\) and \\(B\\) defined in Exercise 1.8 are independent.\n\n\nExample 1.8 The events \\(A\\) and \\(B\\) defined in Exercise 1.9 are not independent.\n\n\nExercise 1.13 \\(A \\independent B\\) implies the following:\n\n\\(A \\independent B^c\\).\n\\(A^c \\independent B\\).\n\\(A^c \\independent B^c\\).\n\n\nIndependence is what separates probability theory from the more general measure theory.\n\n\n\n\n\n\nIt is common for students to make the mistake and think that independence means \\(A \\cap B = \\emptyset\\). This is not true!\n\n\n\n\n\n\n\n\n\nTipIndependence of \\(σ\\)-algebras\n\n\n\nIn the discussion below, we assume that the probability space \\((Ω, \\ALPHABET F, \\PR)\\) is fixed.\n\nTwo sub-\\(σ\\)-algebras \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) of \\(\\ALPHABET F\\) are said to be independent if every event \\(A_1 \\in \\ALPHABET F_1\\) is independent of every event \\(A_2 \\in \\ALPHABET F_2\\).\nFor any event \\(A\\), let \\(σ(A)\\) denote the smallest \\(σ\\)-algebra containing \\(A\\), i.e., \\(σ(A) = \\{\\emptyset, A, A^c, Ω\\}\\).\nExercise 1.13 implies that independence of \\(A\\) and \\(B\\) implies the independence of \\(σ(A)\\) and \\(σ(B)\\). The reverse implication is trivially true. Thus, independence of events is equivalent to the independence of the smallest \\(σ\\)-algebra containing those events.\n\n\n\n\nDefinition 1.4 A family of events \\(\\{A_1, A_2, \\dots, A_n\\}\\) is called independent (sometimes mutually independent if for all non-empty subset of indices \\(\\{k_1, \\dots, k_m\\} \\subset \\{1,\\dots,n\\}\\), we have \\[\n\\PR\\bigl( A_{k_1} \\cap A_{k_2} \\cap \\cdots \\cap A_{k_m} \\bigr)\n= \\PR(A_{k_1}) \\PR(A_{k_2}) \\cdots \\PR(A_{k_m}).\n\\]\n\n\nExercise 1.14 Three bits are transmitted over a noisy channel. For each bit, the probability of correct reception is \\(λ\\). The error events for the three transmissions are mutually independent. Find the probability that two bits are received correctly.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFor \\(i \\in \\{1, 2, 3\\}\\), let\n\n\\(E_i\\) denote the event that bit \\(i\\) is received incorrectly\n\\(C_i\\) denote the event that bit \\(i\\) is received correctly\n\nMoreover let \\(S\\) denote the event that two bits are received correctly. Then, \\[\nS = (C_1 \\cap C_2 \\cap E_3) \\cup (C_1 \\cap E_2 \\cap C_3)\n\\cap (E_1 \\cap C_2 \\cap C_3).\n\\] Note that the three events in the right hand side are disjoint. Thus, \\[\\begin{align*}\n\\PR(S) &=\n\\PR(C_1 \\cap C_2 \\cap E_3) +  \\PR(C_1 \\cap E_2 \\cap C_3) +\n\\PR(E_1 \\cap C_2 \\cap C_3) \\\\\n&=\n\\PR(C_1)\\PR(C_2)\\PR(E_3) +  \\PR(C_1)\\PR(E_2)\\PR(C_3) +\n\\PR(E_1)\\PR(C_2)\\PR(C_3) \\\\\n&= 3 (1-λ) λ^2.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nWarningPairwise independence vs independence\n\n\n\nA family of events \\(\\{A_1, A_2, \\dots, A_n\\}\\) is pairwise independent if for every \\(i,j \\in \\{1, \\dots, n\\}\\), \\(i \\neq j\\), we have \\[ \\PR(A_i \\cap A_j) = \\PR(A_i) \\PR(A_j). \\]\nPairwise independence is weaker than Independence. For instance, three events \\(A\\), \\(B\\), and \\(C\\) are pairwise independent if \\[\n\\PR(A \\cap B) = \\PR(A) \\PR(B),\n\\quad\n\\PR(B \\cap C) = \\PR(B) \\PR(C),\n\\quad\\text{and}\\quad\n\\PR(C \\cap A) = \\PR(C) \\PR(A).\n\\] For independence, in addition to the above, we also need \\[ \\PR(A \\cap B \\cap C) = \\PR(A)\\PR(B) \\PR(C). \\]\nThe following example illustrates shows that independence is stronger than pairwise independence. Consider an urn with \\(M\\) red balls and \\(M\\) blue balls. Two balls are drawn at random, one at a time, with replacement. Consider the following events:\n\n\\(A\\) is the event that the first ball is red.\n\\(B\\) is the event that the second ball is blue.\n\\(C\\) is the event that both balls are of the same color.\n\nObserve that\n\n\\(A \\cap B\\) is the event that the first is red and second is blue.\n\\(B \\cap C\\) is the event that both balls are blue.\n\\(C \\cap A\\) is the event that both balls are red.\n\\(A \\cap B \\cap C = \\emptyset\\)\n\nTherefore,\n\n\\(\\PR(A) = \\PR(B) = \\PR(C) = \\frac 12\\).\n\\(\\PR(A \\cap B) = \\PR(B \\cap C) = \\PR(C \\cap A) = \\frac 14\\).\n\\(\\PR(A \\cap B \\cap C) = \\emptyset\\).\n\nThus, \\(A\\), \\(B\\), \\(C\\) are pairwise independent but not independent.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#product-spaces",
    "href": "probability-spaces.html#product-spaces",
    "title": "1  Probability spaces",
    "section": "1.8 Product spaces",
    "text": "1.8 Product spaces\n\nSo far, we have restricted attention to the outcome of one experiment. It is also possible to construct probability models which combine the outcome of two independent experiments, e.g., suppose we toss a coin and also roll a die. Let \\((Ω_1, \\ALPHABET F_1, \\PR_1)\\) and \\((Ω_2, \\ALPHABET F_2, \\PR_2)\\) be the probability spaces associated with the two experiments? What is the probability space \\((Ω, \\ALPHABET F, \\PR)\\) of the joint experiments?\nThe sample space should obviously be \\(Ω = Ω_1 \\times Ω_2\\). When \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) are finite, then we can simply define \\(\\ALPHABET F = \\ALPHABET F_1 \\times \\ALPHABET F_2\\) and for any \\(A = (A_1, A_2) \\in \\ALPHABET F\\), \\(\\PR(A)\\) to be \\(\\PR_1(A_1) \\PR_2(A_2)\\). Note that \\(\\PR(Ω) = \\PR_1(Ω_1)\\PR_2(Ω_2) = 1\\), thus \\((Ω, \\ALPHABET F, \\PR)\\) is a valid probability space. This space is called direct product of probability spaces \\((Ω_1, \\ALPHABET F_1, \\PR_1)\\) and \\((Ω_2, \\ALPHABET F_2, \\PR_2)\\).\nYou would have implicitly constructed such product spaces when dealing with joint experiments (like the coin toss and die roll example above) in your undergrad courses. Such constructions were correct because of the following.\nGiven \\(A_1 \\in \\ALPHABET F_1\\) and \\(A_2 \\in \\ALPHABET F_2\\), define the events \\[\n   B_1 = \\{ ω  = (ω_1, ω_2) \\in Ω : ω_1 \\in A_1 \\}\n   \\quad\\text{and}\\quad\n   B_2 = \\{ ω  = (ω_1, ω_2) \\in Ω : ω_2 \\in A_2 \\}.\n\\] Then, \\[\n   \\PR(B_1 \\cap B_2) = \\PR_1(A_1) \\PR_2(A_2) = \\PR(B_1) \\PR(B_2).\n\\] Thus, events \\(B_1\\) and \\(B_2\\) are independent.\nIt is a bit more complicated when \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) are not finite. The difficulty is that \\(\\ALPHABET F_1 \\times \\ALPHABET F_2\\) is not a \\(σ\\)-algebra. So, take \\(\\ALPHABET F\\) to be \\(σ(\\ALPHABET F_1 \\times \\ALPHABET F_2)\\) (which is the smallest \\(σ\\)-algebra containing \\(\\ALPHABET F_1 \\times \\ALPHABET F_2\\)) and define \\(\\PR\\) to be the extension of \\(\\PR_1 \\times \\PR_2\\) from \\(\\ALPHABET F_1 \\times \\ALPHABET F_2\\) to \\(σ(\\ALPHABET F_1 \\times \\ALPHABET F_2)\\) (one can show that such an extension exists). Such product space is often written as \\[\n(Ω, \\ALPHABET F, \\PR) = (Ω_1 \\times Ω_2, \\ALPHABET F_1 \\otimes \\ALPHABET F_2, \\PR_1 \\otimes \\PR_2). \\]\n\nWe will not worry too much about the technical details of such product spaces, but will use the above notation at times in the course.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#notes",
    "href": "probability-spaces.html#notes",
    "title": "1  Probability spaces",
    "section": "Notes",
    "text": "Notes\nThe material for this section is fairly standard and adapted from various sources. Both Grimmit and Stirzaker and Gubner have an excellent coverage of this material. The idea of categorizing random phenomenon as noise, uncertainty, or randomness is borrowed from Maxim Raginsky’s course notes. See the book Against the Gods, which provides a fascinating historical account of the development of probability theory and, why a conceptual leap was needed to recognize that uncertainty could be modeled and measured.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability spaces</span>"
    ]
  },
  {
    "objectID": "random-variables.html",
    "href": "random-variables.html",
    "title": "2  Random variables",
    "section": "",
    "text": "2.1 Real-valued random variables",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#real-valued-random-variables",
    "href": "random-variables.html#real-valued-random-variables",
    "title": "2  Random variables",
    "section": "",
    "text": "The standard notation in probability theory is to use uppercase letters such as \\(X\\), \\(Y\\), \\(Z\\), etc. to denote random variables and the corresponding lowercase letters \\(x\\), \\(y\\), \\(z\\), etc. to denote the possible numerical values of these variables.\nIf \\(X\\) is a (real-valued) random variable on \\((Ω, \\ALPHABET F, \\PR)\\), then measurability implies that for every \\(B \\in \\mathscr{B}(\\reals)\\), \\(X^{-1}(B) \\in \\ALPHABET F\\). Rather than explicitly defining the measure \\(\\PR_X\\), we simply use the notation \\[\\PR(X \\in B) \\coloneqq \\PR(\\{ω \\in Ω : X(ω) \\in B \\}).\\]\nIf we take \\(B\\) to be the interval \\((-∞, x]\\), then the event \\(X^{-1}(B) = \\{ ω \\in Ω : X(ω) \\le x \\}\\). Such events are abbreviated as \\(\\{ω : X(ω) \\le x \\}\\) or \\(\\{X \\le x\\}\\). Thus, we have\n\n\\(\\PR(X \\le x) = \\PR(\\{ω \\in Ω : X(ω) \\le x \\})\\).\n\\(\\PR(X = x) = \\PR(\\{ω \\in Ω : X(ω) = x \\})\\).\n\\(\\PR(x &lt; X \\le y) = \\PR(\\{ω \\in Ω : x &lt; X(ω) \\le y \\})\\).\n\nThe cumulative distribution function (CDF) of a (real-valued) random variable \\(X\\) is the function \\(F_X \\colon \\reals \\to [0,1]\\) given by \\[\nF_X(x) \\coloneqq \\PR(X \\le x) = \\PR(\\{ ω \\in Ω : X(ω) \\le x \\}).\n\\]\nFor instance, in Example 2.1, the CDF is given by \\[\nF_X(x) = \\begin{cases}\n0, & \\hbox{if } x &lt; 0,  \\\\\n\\frac 14, & \\hbox{if } 0 \\le x &lt; 1, \\\\\n\\tfrac 34, & \\hbox{if } 1 \\le x &lt; 2,\\\\\n1, &\\hbox{if } 2 \\le x.\n\\end{cases}\\]\n\n\n\n\n\n\nFigure 2.3: CDF of number of heads\n\n\n\nSome examples of CDF of random variables\n\nExample 2.2 (Constant random variables) The simplest random variable takes a constant value on the whole domain \\(Ω\\), i.e., \\[\nX(ω) = c, \\quad \\forall ω \\in Ω\n\\] where \\(c\\) is a constant. The CDF \\(F_X(x) = \\PR(X \\le x)\\) is the step function \\[\nF_X(x) = \\begin{cases}\n0, & x &lt; c \\\\\n1, & x \\ge c.\n\\end{cases}\n\\]\nSlightly more generally, we say that \\(X\\) is almost surely a constant if there exists a \\(c \\in \\reals\\) such that \\(\\PR(X=c) = 1\\).\n\n\n\n\n\n\nFigure 2.4: CDF of a constant random variable\n\n\n\n\n\nExample 2.3 (Indicator functions) Let \\(A\\) be an event. Define the indicator of event \\(A\\), denoted by \\(\\IND_{A} \\colon Ω \\to \\reals\\), as \\[ \\IND_{A}(ω) = \\begin{cases}\n    1, & \\hbox{if } ω \\in A \\\\\n    0, & \\hbox{otherwise }\n\\end{cases}.\\]\n\n\nExample 2.4 (Bernoulli random variable) A Bernoulli random variable takes two possible values: value \\(0\\) with probability \\(1-p\\) and value \\(1\\) with probability \\(p\\). It’s CDF is given by \\[\nF_X(x) = \\begin{cases}\n0, & x &lt; 0 \\\\\n1 - p, & 0 \\le x &lt; 1 \\\\\n1, & x \\ge 1.\n\\end{cases}\n\\] Observe that \\(\\IND_A\\) is a Bernoulli random variable which takes values \\(1\\) and \\(0\\) with probabilities \\(\\PR(A)\\) and \\(1 - \\PR(A)\\).\n\n\n\n\n\n\nFigure 2.5: CDF of a Bernoulli random variable\n\n\n\n\n\nLemma 2.1 (Properties of CDFs)  \n\n\\(\\PR(X &gt; x) = 1 - F_X(x)\\).\n\\(\\PR(x &lt; X \\le y) = F_X(y) - F_X(x)\\).\n\\(\\lim_{x \\to -∞} F_X(x) = 0\\) and \\(\\lim_{x \\to +∞} F_X(x) = 1\\).\nCDFs are non-decreasing, i.e., if \\(x &lt; y\\), then \\(F_X(x) \\le F_X(y)\\).\nCDFs are right continuous, i.e., \\(\\lim_{h \\downarrow 0}F_X(x+h) = F_X(x)\\).\n\\(\\PR(X = x) = F_X(x) - F_X(x^{-})\\), where \\(F_X(x^{-})\\) is defined as \\(\\lim_{h \\downarrow 0} F_X(x - h)\\)\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\nBy definition, \\(\\{X &gt; x\\}^c = Ω\\setminus \\{X \\le x\\}\\). Thus, \\[\\PR(X &gt; x) = 1 - \\PR(X \\le x) = 1 - F_X(x).\\]\n\\[\\PR(x &lt; X \\le y) = \\PR(X \\le y) - \\PR(X \\le x) = F_X(y) - F_X(x).\\] [See assignment 1 for the first equality.]\nDefine the increasing sequence of events \\(A_n = \\{ X \\le n\\}\\), \\(n \\in \\naturalnumbers\\). By continuity of probability, we have \\[\\begin{align*}\n  &\\quad & \\PR\\biggl( \\bigcup_{n=1}^{∞} A_n \\biggr)\n  & = \\lim_{n \\to ∞} \\PR(A_n) \\\\\n  \\implies && \\PR(\\{X &lt; ∞\\}) &= \\lim_{n \\to ∞} \\PR(X \\le n) \\\\\n  \\implies && \\PR(Ω) &= \\lim_{n \\to ∞} F_X(n) \\\\\n  \\implies && 1 &= \\lim_{n \\to ∞} F_X(n).\n\\end{align*}\\]\nThe reverse argument is similar where we consider the decreasing sequence of events \\(B_n = \\{ X_n \\le -n \\}\\), \\(n \\in \\naturalnumbers\\). Then, by continuity of probability, we have \\[\\begin{align*}\n     &\\quad & \\PR\\biggl( \\bigcap_{n=1}^{∞} B_n \\biggr)\n     & = \\lim_{n \\to ∞} \\PR(B_n) \\\\\n     \\implies && \\PR(\\{X &lt; -∞\\}) &= \\lim_{n \\to ∞} \\PR(X \\le -n) \\\\\n     \\implies && \\PR(\\emptyset) &= \\lim_{n \\to ∞} F_X(-n) \\\\\n     \\implies && 0 &= \\lim_{n \\to ∞} F_X(-n).\n   \\end{align*}\\]\nRecall that \\[\\begin{align*}\n   F_X(x) &= \\PR(X \\le x) \\\\\n   F_X(y) &= \\PR(X \\le y)\n\\end{align*}\\] Observe that since \\(x &lt; y\\), we have \\(\\{X \\le x\\} \\subseteq \\{X \\le y\\}\\). Hence, by monotonicity of probability, we have \\[\\PR(X \\le x) \\le \\PR(X \\le y),\\] which proves the result.\nConsider the decreasing sequence of sets: \\[ A_n = \\{ X \\le x + \\tfrac 1n \\}, \\quad n \\in \\naturalnumbers. \\] Then, by continuity of probability, we have \\[\\begin{align*}\n  &\\quad & \\PR\\biggl( \\bigcap_{i=1}^{∞} A_n \\biggr)\n  & = \\lim_{n \\to ∞} \\PR(A_n) \\\\\n  \\implies && \\PR(X \\le x) &= \\lim_{n \\to ∞} \\PR(X \\le x + \\tfrac 1n ) \\\\\n  \\implies && F_X(x) &= \\lim_{n \\to ∞} F_X(x + \\tfrac 1n).\n\\end{align*}\\]\nDefine the decreasing sequence of sets \\[ A_n = \\biggl\\{ x - \\frac 1n &lt; X \\le x \\biggr\\},\n\\quad n \\in \\naturalnumbers.\\] Observe that by the previous property \\[\\PR(A_n) = F_X(x) - F_X(x - \\tfrac 1n).\\] Since \\(A_n\\) is a decreasing sequence of sets, we have \\[\\begin{align*}\n& \\quad &\n\\PR\\biggl( \\bigcap_{n=1}^∞ A_n \\biggr)\n&= \\lim_{n \\to ∞} \\PR(A_n) \\\\\n\\implies && \\PR(X = x) &= F_X(x) - \\lim_{n \\to ∞} F_X(x - \\tfrac 1n) \\\\\n&&& = F_X(x) - F_X(x^{-}).\n\\end{align*}\\]\n\n\n\n\n\n\nExample 2.5 For \\(x &lt; y\\), express the following in terms of the CDF:\n\n\\(\\PR(x \\le X \\le y)\\).\n\\(\\PR(x \\le X &lt; y)\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#classification-of-random-variables",
    "href": "random-variables.html#classification-of-random-variables",
    "title": "2  Random variables",
    "section": "2.2 Classification of random variables",
    "text": "2.2 Classification of random variables\nThere are three types of random variables\n\nA random variable \\(X\\) is said to be discrete if it takes values in a finite or countable subset \\(\\ALPHABET X \\coloneqq \\{x_1, x_2, \\dots\\}\\) of \\(\\reals\\). A discrete random variable has a probability mass function (PMF) \\(p \\colon \\reals \\to [0,1]\\) which satisfies the following properties:\n\n\\(p(x) = \\PR(X = x) = F_X(x) - F_X(x^{-})\\).\n\\(F_X(x) = \\sum_{x_n : x_n \\le x} p(x_n).\\)\n\nThus, for a discrete random variable, the CDF is a piecewise constant function\nFigure 2.3, Figure 2.4, Figure 2.5 are all examples of discrete random variables.\nA random variable \\(X\\) is called continuous if there exists an integrable function \\(f \\colon \\reals \\to [0, ∞)\\) called the probability density function such that the CDF can be written as \\[\nF_X(x) = \\int_{-∞}^x f_X(x) dx.\n\\]\nThus, for a continuous random variable, the CDF is a continuous function\n\n\n\n\n\n\nFigure 2.6: CDF of a continuous random variable\n\n\n\nA random variable is called mixed if it is neither discrete nor continuous. For a mixed random variable, the CDF has jumps at a finite or countable infinite number of points and it is continuous over one or many intervals.\n\n\n\n\n\n\nFigure 2.7: CDF of a mixed random variable\n\n\n\nAs an example, consider the following random experiment. A fair coin is tossed: if the outcome is heads, then \\(X \\sim \\text{Bernoulli}(0.5)\\); if the outcome is tails; then \\(X \\sim \\text{Uniform}(0,1)\\). Thus (from the law of total probability), the CDF of \\(X\\) is given by \\[\n   F_X(x) = \\begin{cases}\n   0, & \\hbox{if } x &lt; 0 \\\\\n   \\frac 14 & \\hbox{if } x = 0 \\\\\n   \\frac 14 + \\frac x2 & \\hbox{if } 0 &lt; x &lt; 1 \\\\\n   1 & \\hbox{if } x \\ge 1.\n   \\end{cases}\n   \\] and shown in Figure 2.7.\n\n\nLemma 2.2 (Properties of discrete and continuous random variables)  \n\nProperties of discrete random variables\nFor a discrete random variable \\(X\\), define the probability mass function (PMF) \\(P_X \\colon \\reals \\to \\reals\\) as \\[ P_X(x) = F_X(x) - F_X(x^{-}).\\] By construction, \\(P_X(x) \\ge 0\\) and \\(\\sum_{x \\in \\ALPHABET X} P_X(x) = 1\\).\nThen, for any event \\(A \\in \\mathscr{B}(\\reals)\\), \\[\\PR(X \\in A) = \\sum_{x \\in \\ALPHABET X \\cap A} P_X(x).\\]\nProperties of continuous random variables\nFor a continuous random variable \\(X\\), define the probability density function (PDF) \\(f_X \\colon \\reals \\to \\reals\\) as \\[\n   f_X(x) = \\frac{d}{dx} F_X(x).\n\\] By construction, \\(f_X(x) \\ge 0\\) and \\(\\int_{-∞}^{∞} f_X(x)\\, dx = 1\\).\nThen, for any event \\(A \\in \\mathscr{B}(\\reals)\\), \\[\\PR(X \\in A) = \\int_{x \\in A} f_X(x)\\,dx.\\]\n\n\n\n2.2.1 Examples of discrete random variables\nWe now consider some other examples of discrete random variables, which are typically described by specifying their PMF.\n\nExample 2.6 (Binomial random variable) A Binomial random variable is the sum of independent and identically distributed Bernoulli random variables (we will prove this fact later). For example, if a biased coin (with \\(\\PR(H) = p\\)) is tossed \\(n\\) times, then the number of heads is a binomial random variable with parameters \\(n\\) and \\(p\\), which is denoted by \\(\\text{Binomial}(n,p)\\). For such a random variable, \\[\nP_X(k) = \\binom n k p^k (1-p)^{n-k}, \\quad 0 \\le k \\le n.\n\\]\n\n\nExample 2.7 (Geometric random variable) A geometric random variable is the number of trials in i.i.d. Bernoulli random variables. For example, if a biased coin (with \\(\\PR(H) = p\\)) is tossed repeatedly, the number of tosses needed for the first head is a geometric random variable with parameter \\(p \\in (0,1)\\), which is denoted by \\(\\text{Geo}(p)\\). For such a random variable, \\[\nP_X(k) = (1-p)^{k-1} p, \\quad k \\in \\integers_{&gt; 0}.\n\\]\n\nA geometric random variables have the property of being memoryless: the distribution of waiting time does not depend on how much time has already elapsed.\n\nExample 2.8 (Poisson random variable) Poisson random variables model many different phenomenon ranging from photoelectric effect in photonics to inter-packet arrival times in computer networks. A random variable is said to Poisson random variable with parameter \\(λ &gt; 0\\), which is denoted by \\(\\text{Poisson}(λ)\\), if \\[\nP_X(k) = \\frac{λ^k}{k!} e^{-λ}, \\quad k \\in \\integers_{\\ge 0}.\n\\]\n\nPoisson random variables model rare events. It describes the limit of a Binomial random variable with success probability \\(p = λ/n\\) as the number \\(n\\) of trials increases. Poisson random variables have the stability property that sum of Poisson random variables is Poisson\n\nExample 2.9 (Uniform random variable) A random variable is said to have a (discrete) uniform distribution over a discrete set \\(\\ALPHABET S\\) if \\[P_X(k) = \\frac 1{\\ABS{\\ALPHABET S}}, \\quad k \\in \\ALPHABET S.\\]\n\n\n\n2.2.2 Some examples of continuous random variables\n\nExample 2.10 (Uniform random variable) A random variable is said to have a (continuous) uniform distribution over an interval \\([a, b]\\), where \\(a &lt; b\\) if \\[f_X(x) = \\frac 1{b - a}, \\quad x \\in [a,b].\\]\n\n\nExample 2.11 (Exponential random variable) A random variable is said to have an exponential distribution with parameter \\(λ &gt; 0\\), which is denoted by \\(\\text{exp}(λ)\\) if \\[f_X(x) = λ e^{-λ x}, \\quad x \\ge 0.\\]\n\nExponential random variables arise in queueing theory, network traffic, and photonics. They have the property of being memoryless: the distribution of the waiting time does not depend on how much time has already elapsed.\n\nExample 2.12 (Gaussian random variable) A random variable is said to have a Gaussian distribution with mean \\(μ\\) and standard deviation \\(σ &gt; 0\\), which is denoted by \\(\\mathcal N(μ, σ^2)\\) if \\[f_X(x) = \\frac 1{\\sqrt{2 π}\\, σ}\n\\exp\\left( -\\frac {(x-μ)^2}{2 σ^2} \\right),\n\\quad x \\in \\reals.\\]\nA Gaussian distribution is also called a Normal distribution. When \\(μ = 0\\) and \\(σ^2 = 1\\), the distribution is called standard Normal.\n\nThe Gaussian distribution is perhaps the most important continuous distribution because of its role in the Central Limit Theorem. Gaussian random variables have the stability property that sum of Gaussian random variables is Gaussian.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#functions-of-random-variables",
    "href": "random-variables.html#functions-of-random-variables",
    "title": "2  Random variables",
    "section": "2.3 Functions of random variables",
    "text": "2.3 Functions of random variables\nWe often encounter situations where we are interested in functions of random variables. Functions of random variables are random variables.\nIn particular, suppose \\[\n  X \\colon (\\ALPHABET Ω, \\ALPHABET F) \\to (\\reals, \\mathscr{B}(\\reals))\n\\] is a random variable and \\(g \\colon \\reals \\to \\reals\\) is a measurable function.\nSince \\(g\\) is measurable, for any (Borel) subset \\(B\\) of \\(\\reals\\), we have that \\(C = g^{-1}(B) \\in \\mathscr B(\\reals)\\). Therefore, \\(X^{-1}(C) \\in \\ALPHABET F\\). Thus, we can think of \\(Y\\) as a random variable.\nSince \\(Y\\) is a random variable, it is possible to compute its CDF and PMF/PDF as appropriate. We discuss the details separately for discrete and continuous random variables.\n\n2.3.1 Functions of discrete random variables\nIn the discrete time, it is trivial to find the PMF of \\(Y\\) in terms of PMF of \\(X\\). For example, consider the random variable \\(X\\) defined in Example 2.1. Let \\(g \\colon \\{0, 1, 2\\} \\to \\{0, 1\\}\\) be given by \\[\n  g(0) = 0, \\quad g(1) = 0, \\quad g(2) = 1.\n\\] Then, \\[\n  P_Y(0) = P_X(0) + P_X(1)\n  \\quad\\text{and}\\quad\n  P_Y(1) = P_X(2).\n\\]\nHowever, it will be useful to visualize this slightly differently. We start with revisiting measurability for discrete random variables. The main point is that\n\nA discrete random variable creates a partition on the sample space (we expand on this point below).\nThe power-set of \\(\\{A_1, A_2, \\dots\\}\\) is called the \\(σ\\)-algebra generated by \\(X\\) and denoted by \\(σ(X)\\). This \\(σ\\)-algebra captures the crux of measurability.\n\nWe now discuss the partition generated by a discrete random variable. Let \\(X\\) be a random variable and \\(\\ALPHABET X = \\{x_1, x_2, \\dots, x_n\\}\\) be the range of \\(X\\). Define \\[A_i = \\{ω \\in Ω : X(ω) = x_i \\} = X^{-1}(x_i).\\] Then, \\(X\\) can be written as \\[\n  X(ω) = \\sum_{i=1}^{n} x_i \\IND_{A_i}(ω).\n\\]\nNote that \\(\\{A_1, \\dots, A_n \\}\\) are disjoint events and their union is the entire sample space \\(Ω\\) (because one of \\(x_i\\)’s must occur). Thus, \\(\\{A_1, \\dots, A_n \\}\\) is a partition of \\(Ω\\).\nAs an illustration, let’s reconsider Example 2.1. In this case, the range of \\(X\\) is \\(\\ALPHABET X = \\{0, 1, 2\\}\\). The partition corresponding to \\(X\\) is shown in Figure 2.8.\n\n\n\n\n\n\nFigure 2.8: Illustration of the partition created by \\(X\\) for Example 2.1\n\n\n\nThe partition corresponding to \\(Y = g(X)\\) is shown in Figure 2.9. Observe that this partition is a coarsening of the partition corresponding to \\(X\\).\n\n\n\n\n\n\nFigure 2.9: Illustration of the partition created by $ = g(X)$ for Example 2.1\n\n\n\nThe PMFs of \\(X\\) and \\(Y\\) can be obtained by summing up the masses in each element of their respective partitions.\n\n\n2.3.2 Functions of continuous random variables\nFor continuous random variables, computing the PDF of a function of a random variable is more involved. We first illustrate the main idea via some examples.\n\nExample 2.13 Suppose \\(X \\sim \\text{Uniform}(0,2)\\). Consider a function \\(g\\) given by \\[\ng(x) = \\begin{cases}\n  x & x \\in (0,1] \\\\\n  2 - x & x \\in (1,2] \\\\\n  0 & \\hbox{otherwise}\n\\end{cases}\n\\] Define \\(Y = g(X)\\). Find \\(F_Y(y)\\) and \\(f_Y(y)\\).\n\ng_exm_1 = [\n  {x: -1, y: 0},\n  {x: 0, y: 0},\n  {x: 1, y: 1},\n  {x: 2, y: 0},\n  {x: 3, y: 0}\n];\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  width: 600,\n  height: 200,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(g_exm_1, {x: \"x\", y: \"y\", stroke: \"blue\"}),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 2.10: The function \\(g(x)\\) for Example 2.13\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFrom the definition of \\(g\\), we know that the range of \\(g\\) is \\([0,1]\\). Thus, we know that the support of \\(Y\\) is \\([0,1]\\).\n\nFor any \\(y &lt; 0\\), the event \\(\\{Y \\le y\\} = \\emptyset\\). Therefore, \\(F_Y(y) = 0\\).\nFor any \\(y &gt; 1\\), the event \\(\\{Y \\le y\\} = Ω\\). Therefore, \\(F_Y(y) = 1\\).\nNow consider a \\(y \\in (0,1)\\). We have \\[\n\\{Y \\le y \\} = \\{ X \\le y \\} \\cup \\{X \\ge 2 - y \\}.\n\\] Thus, \\[\nF_Y(y) = F_X(y) + F_X(2-y) = \\frac {y}{2} + 1 - \\frac{2-y}{2} = y.\n\\] Thus, \\[\n   f_Y(y) = \\dfrac{d}{dy} F_Y(y) = 1, \\quad y \\in [0,1].\n\\] Thus, \\(Y\\) is \\(\\text{Uniform}(0,1)\\).\n\n\n\n\n\nExample 2.14 Suppose \\(X \\sim \\text{Uniform}(0,4)\\). Consider a function \\(g\\) given by \\[\ng(x) = \\begin{cases}\n  x & x \\in (0,1] \\\\\n  1 & x \\in (1, 3) \\\\\n  4- x & x \\in (3,4] \\\\\n  0 & \\hbox{otherwise}\n\\end{cases}\n\\] Define \\(Y = g(X)\\). Find \\(F_Y(y)\\) and \\(f_Y(y)\\).\n\ng_exm_2 = [\n  {x: -1, y: 0},\n  {x: 0, y: 0},\n  {x: 1, y: 1},\n  {x: 3, y: 1},\n  {x: 4, y: 0},\n  {x: 5, y: 0}\n];\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  width: 600,\n  height: 200,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(g_exm_2, {x: \"x\", y: \"y\", stroke: \"blue\"}),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 2.11: The function \\(g(x)\\) for Example 2.14\n\n\n\n\n\n\nExample 2.15 Suppose \\(X \\sim \\mathcal{N}(μ,σ^2)\\). Show that \\(Z = (X - μ)/σ\\) is a standard normal random variable, i.e., \\(Z \\sim \\mathcal{N}(0,1)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe can write the CDF \\(F_Z(z)\\) as \\[\\begin{align*}\nF_Z(z) &= \\PR(Z \\le z) = \\PR\\left( \\frac{X - μ}{σ} \\le z \\right)\n= \\PR(X \\le σ z + μ)\n\\\\\n&= \\int_{-∞}^{σ z + μ} \\frac{1}{\\sqrt{2 π}\\, σ} \\exp\\left(\n- \\frac{(x-μ)^2}{2 σ^2}\\, dx \\right)\n\\\\\n&= \\int_{-∞}^{z} \\frac{1}{\\sqrt{2 π}} \\exp\\left(\n- \\frac{y^2}{2}\\, dy \\right)\n\\end{align*}\\] where the last step uses the change of variables \\(y = (x-μ)/σ\\).\nThus, \\[f_Z(z) = \\frac{d F_Z(z)}{dz} = \\frac{1}{\\sqrt{2 π}} e^{-z^2/2}.\\] Thus, \\(Z \\sim \\mathcal{N}(0,1)\\).\n\n\n\n\nExample 2.16 (Generating non-uniform random variables by inverting the CDF) Suppose \\(F \\colon \\reals \\to [0,1]\\) is a function that satisfies the following properties: there exist a pair \\((a,b)\\) with \\(a &lt; b\\) (we allow \\(a\\) to be \\(-∞\\) and \\(b\\) to be \\(∞\\)) such that\n\n\\(F(x) = 0\\) for \\(x \\le a\\)\n\\(F(x) = 1\\) for \\(x \\ge b\\)\n\\(F(x)\\) is strictly increasing in \\((a,b)\\).\n\nThus, \\(F\\) satisfies the properties of the CDF of a continuous random variable and \\(F\\) is invertible in the interval \\((a,b)\\).\nSuppose \\(U \\sim \\text{Uniform}(0,1)\\) and \\(X = F^{-1}(U)\\). Show that \\(F_X(x) = F(x)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe can write the CDF \\(F_X(x)\\) as \\[\nF_X(x) = \\PR(X \\le x) = \\PR(F^{-1}(U) \\le x)\n\\]\nSince \\(F\\) is strictly increasing, \\(F^{-1}(U) \\le x\\) is equivalent to \\(U \\le F(x)\\). Thus, \\[\nF_X(x) = \\PR(U \\le F(x)) = F_U(F(x)) = F(x)\n\\] where the last step uses the fact that \\(U\\) is uniform over \\((0,1)\\).\n\n\n\n\n\n2.3.3 Change of variables formula\nIt is possible to obtain a general change of variables formula to obtain \\(f_Y\\).\n\nSuppose \\(g\\) is a continuous and one-to-one function (from \\(\\text{Range}(X)\\) to \\(\\text{Range}{Y}\\)). Thus, \\(g\\) must be either strictly increasing or strictly decreasing, and in both cases the inverse \\(g^{-}\\) is well defined.\n\nIf \\(g^{-1}\\) is strictly increasing, we have \\[ F_Y(y) = \\PR(Y \\le y) = \\PR(X \\le g^{-1}(y) = F_X(g^{-1}(y))\\] Therefore, \\[ f_Y(y) = \\frac{d F_X(g^{-1}(y))}{dy} = f_X(g^{-1}(y)) \\frac{d g^{-1}(y)}{dy}. \\]\nIf \\(g^{-1}\\) is strictly decreasing, we have \\[ F_Y(y) = \\PR(Y \\le y) = \\PR(X \\ge g^{-1}(y)) = 1 - F_X(g^{-1}(y)).\\] Therefore, \\[ f_Y(y) = - \\frac{d F_X(g^{-1}(y))}{dy} = - f_X(g^{-1}(y)) \\frac{d g^{-1}(y)}{dy}. \\]\nThe above two formulas can be combined as \\[ \\bbox[5pt,border: 1px solid]{f_Y(y) = f_X(g^{-1}(y)) \\left| \\frac{d g^{-1}(y)}{dy} \\right|} \\]\n\nFrom calculus, we know that if \\(h(y) = g^{-1}(y)\\), then \\(h'(y) = 1/g'(h(y))\\). Thus, the above expression can be simplified as \\[ \\bbox[5pt, border: 1px solid]{f_Y(y) = \\frac{f_X(x)}{\\ABS{g'(x)}}, \\quad \\text{where } x = g^{-1}(y).} \\]\n\nExample 2.17 Resolve Example 2.15 using the above formula.\n\nIf the transform \\(g(x)\\) is not one-to-one (as in Example 2.13), we can obtain \\(f_Y(y)\\) as follows. Suppose \\(y = g(x)\\) has finite roots, denoted by \\(\\{x^{(k)}\\}_{k=1}^m\\). Then, \\[ f_Y(y) = \\sum_{k=1}^m \\frac{f_X(x^{(k)})}{\\ABS{g'(x^{(k)})}}. \\]\n\nExample 2.18 Resolve Example 2.13 using the above formula.\n\nThe change of variables formula can be used to verify some common ways of generating non-uniform random variables from uniform random variables. For example:\n\nSuppose \\(U \\sim \\text{Uniform}(0,1)\\) and \\(λ &gt; 0\\). Then, \\(X = - \\ln(1 - U)/λ\\) has \\(\\text{exp}(λ)\\) distribution.\n\nNote that the change of variable formula only works for continuous function. For non-continuous functions, we need to use first principles (or the idea in Example 2.16). For example:\n\nSuppose \\(U \\sim \\text{Uniform}(0,1)\\) and \\(p \\in (0,1)\\). Then \\(X = \\IND_{\\{U \\le p\\}}\\) has \\(\\text{Bernoulli}(p)\\) distribution.\n\nSee Devroye (1986) for a general discussion on generating non-uniform random variables,",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables</span>"
    ]
  },
  {
    "objectID": "random-variables.html#expectation-of-random-variables",
    "href": "random-variables.html#expectation-of-random-variables",
    "title": "2  Random variables",
    "section": "2.4 Expectation of random variables",
    "text": "2.4 Expectation of random variables\nSuppose we generate \\(N\\) i.i.d. (independent and identically distributed) samples \\(\\{s_1, s_2, \\dots, s_N\\}\\) of a random variable \\(X\\) and compute the average: \\[ m = \\frac 1N \\sum_{n=1}^N s_n. \\] When \\(X\\) is discrete and takes values \\(\\{x_1, x_2, \\dots, x_n\\}\\), we expect that the number of times we obtain a value \\(x_i\\) is approximately \\(NP_X(x_i)\\) when \\(N\\) is large. Thus, \\[ m \\approx \\frac 1N \\sum_{i=1}^n x_i \\, N P_X(x_i)  = \\sum_{i=1}^n x_i P_X(x_i). \\]\nThis quantity is called the expectation or the expected value or the mean value of the random variable \\(X\\) and denoted by \\(\\EXP[X]\\).\n\nDefinition 2.2 The expectation of a random variable \\(X\\) is defined as follows:\n\nwhen \\(X\\) is discrete and takes values \\(\\{x_1, x_2, \\dots, x_n \\}\\), then \\[\\EXP[X] = \\sum_{i=1}^n x_i P_X(x_i).\\]\nwhen \\(X\\) is continuous, then \\[\\EXP[X] = \\int_{-∞}^{∞} x f_X(x)\\, dx. \\]\nThus, we can think of the expected value as the center of mass of the PDF.\n\n\n\n\n\n\n\n\nWarningDoes the summation or integration exist?\n\n\n\nWhen \\(X\\) takes countably or uncountably infinite values, we need to be a bit more precise by what we mean by the summation (or the integration) formula above. In particular, we do not want the answer to depend on the order in which we do the summation or the integration (i.e., we do not want \\(∞ - ∞\\) situation). This means that the sum or the integral should be :absolutely convergent. Such random variables are called integrable random variables.\nFormally, expectation is defined only for integrable random variables.\nTo illustrate why this is important, consider a discrete random variable defined over \\(\\integers\\setminus\\{0\\}\\) where \\[\nP_X(n) = P_X(-n) = \\frac {1}{2C n^2}, \\quad n \\in \\naturalnumbers\n\\] where \\(C\\) is a normalizing constant given by \\[\nC = \\sum_{n=1}^∞ \\frac 1{n^2} = \\frac{π^2}{6}.\n\\] Then, observe that \\[\\begin{align*}\n\\EXP[X] &= \\sum_{n=1}^∞ \\frac{n}{2 C n^2}\n+ \\sum_{n=-∞}^{-1} \\frac{n}{2 C n^2} \\\\\n&= \\frac 1{2C} \\sum_{n=1}^∞ \\frac{1}{n}\n+ \\frac 1{2C} \\sum_{n=-∞}^{-1} \\frac{1}{n} \\\\\n&= \\frac{∞}{2C} - \\frac{∞}{2C}\n\\end{align*}\\] which is undefined.\nThe concern here is that the summation is undefined. Mathematically, we are okay when the summation is infinity. For example, consider another random variable \\(Y\\) defined over \\(\\naturalnumbers\\) for which \\[\nP_X(n) = \\frac {1}{C n^2}, \\quad n \\in \\naturalnumbers\n\\] where \\(C\\) is as defined above. This is called the Zipf distribution. By following an argument same as above, we see that \\[\\EXP[Y] = ∞.\\]\n\n\n\nExample 2.19 Find the mean of \\(X\\) defined in Example 2.1.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\[\n\\EXP[X] = \\frac 14 \\cdot 0 + \\frac 12 \\cdot 1 + \\frac 14 \\cdot 2 = 1.\n\\]\n\n\n\n\nExample 2.20 Find the expected value of the random variables with the following distributions:\n\n\\(\\text{Bernoulli}(p)\\).\n\\(\\text{Binomial}(n,p)\\).\n\\(\\text{Geo}(p)\\).\n\\(\\text{Poisson}(λ)\\).\n\\(\\text{Uniform}(a,b)\\).\n\\(\\text{Exp}(λ)\\).\n\n\n\nLemma 2.3 For any (measurable) function \\(g \\colon \\reals \\to \\reals\\), we have\n\nwhen \\(X\\) is discrete and takes values \\(\\{x_1, x_2, \\dots, x_n \\}\\), then \\[\\EXP[g(X)] = \\sum_{i=1}^n g(x_i) P_X(x_i).\\]\nwhen \\(X\\) is continuous, then \\[\\EXP[g(X)] = \\int_{-∞}^{∞} g(x) f_X(x)\\, dx. \\]\n\nBoth expressions are defined only when the sum/integral is absolutely convergent.\n\n\n\n\n\n\n\nNoteHow to avoid a proof\n\n\n\n\n\nThis result is sometimes called *the law of the unconscious statistician (LOTUS). One typically shows this result by defining a new random variable \\(Y = g(X)\\), computing its PMF/PDF \\(f_Y\\) and then using the definition in Definition 2.2.\nA simpler proof is to define expectation by Lemma 2.3 for any (measurable) function \\(g\\). Then the definition of Definition 2.2 is a special case for \\(g(x) = x\\). No proofs needed!\n\n\n\n\nExample 2.21 Suppose \\(X \\sim \\text{Unif}[-1,1]\\). Compute \\(\\EXP[X^2]\\).\n\n\nLemma 2.4 (Properties of expectation)  \n\nLinearity. For any (measurable) functions \\(g\\) and \\(h\\) \\[\\EXP[g(X) + h(X)] = \\EXP[ g(X)] + \\EXP[ h(X) ]. \\] As a special case, for a constant \\(c\\), \\[\\EXP[X + c] = \\EXP[X] + c.\\]\nScaling. For any constant \\(c\\), \\[\\EXP[cX] = c\\EXP[X].\\]\nBounds. If \\(a \\le X(ω) \\le b\\) for all \\(ω \\in Ω\\), then \\[ a \\le \\EXP[X] \\le b. \\]\nIndicator of events. For any (Borel) subset \\(B\\) of \\(\\reals\\), we have \\[\\EXP[ \\IND_{\\{ X \\in B \\}}] = \\PR(X \\in B). \\]\n\n\n\nA continuous random variable is said to be symmetric if \\(f_X(-x) = f_X(x)\\) for all \\(x \\in \\reals\\). A symmetric random variable has mean \\(0\\).\nA continuous random variable is said to be symmetric around \\(m\\) if \\(f(m - x) = f(m + x)\\), for all \\(x \\in \\reals\\). The mean of such a random variable is \\(m\\).\n\n\n2.4.1 Higher moments\n\nThe \\(m\\)-th moment, \\(m \\ge 1\\) of a random variable \\(X\\) is defined as \\(\\EXP[X^m]\\).\nThe \\(m\\)-th central moment is defined as \\(\\EXP[(X - μ)^m]\\), where \\(μ = \\EXP[X]\\).\nFor second central moment (i.e., \\(m=2\\)) is called variance. The variance satisfies the following: \\[\\VAR(X) = \\EXP[X^2] - (\\EXP[X])^2.\\]\nThe positive square root of variance is called the standard deviation. Variance is often denoted by \\(σ^2\\) and the standard deviation by \\(σ\\).\n\n\nExample 2.22 Find the variance of \\(X\\) defined in Example 2.1.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe first compute \\[\n\\EXP[X^2] = \\frac 14 \\cdot 0^2 + \\frac 12 \\cdot 1^2 + \\frac 14 \\cdot 2^2 = \\frac 32.\n\\] Therefore, \\[\n\\VAR(X) = \\EXP[X^2] - \\EXP[X]^2 = \\frac 32 - 1 = \\frac 12.\n\\]\n\n\n\n\nLemma 2.5 (Properties of variance)  \n\nScaling. For any constant \\(c\\), \\[\\VAR(cX) = c^2 \\VAR(X).\\]\nShift invariance. For any constant \\(c\\), \\[\\VAR(X + c) = \\VAR(X).\\]\n\n\nThe mean and variance of common random variables is show in Table 2.1\n\n\n\nTable 2.1: Mean and variance of common random variables\n\n\n\n\n\n\n\n\n\n\n\nRandom variable\nParameter(s)\nMean\nVariance\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p\\)\n\\(p(1-p)\\)\n\n\nBinomial\n\\((n,p)\\)\n\\(np\\)\n\\(np(1-p)\\)\n\n\nGeometric\n\\(p\\)\n\\(\\dfrac 1p\\)\n\\(\\dfrac{1-p}{p}\\)\n\n\nPoisson\n\\(λ\\)\n\\(λ\\)\n\\(λ\\)\n\n\nUniform\n\\((a,b)\\)\n\\(\\frac 12 (a+b)\\)\n\\(\\frac 1{12}(b-a)^2\\)\n\n\nExponential\n\\(λ\\)\n\\(\\dfrac 1 λ\\)\n\\(\\dfrac 1{λ^2}\\)\n\n\nGaussian\n\\((μ,σ)\\)\n\\(μ\\)\n\\(σ^2\\)\n\n\n\n\n\n\n\nExample 2.23 Let \\(X\\) be a real-valued random variable where \\(μ = \\EXP[X]\\). Let \\(t\\) be any real number. Then show that \\[\n  \\EXP[(X-t)^2] = \\VAR(X) + (t-μ)^2.\n\\] Argue that the above equation implies that \\[\n  \\VAR(X) = \\min_{t \\in \\reals} \\EXP[(X -t)^2].\n\\]\nRemark: The above relationship suggests a single-pass algorithm to compute \\(\\VAR(X)\\) from a set of samples: choose any \\(t\\) and compute \\(μ = \\EXP[X]\\) and \\(\\EXP[(X-t)^2]\\) in a single pass. Then, \\(\\VAR(X)\\) can be computed bs \\(\\VAR(X) = \\EXP[(X-t)^2] + (t-μ)^2\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe know that \\[\\begin{align*}\n  \\EXP[(X-t)^2] &= \\EXP[\\bigl((X-μ) + (μ-t) \\bigr)^2]\n   \\\\\n   &= \\EXP[(X-μ)^2 + (μ-t)^2  + 2 (X - μ)(μ-t) ]\n   \\\\\n   &\\stackrel{(a)}= \\EXP[(X-μ)^2] + (t-μ)^2 + 2(μ-t)\\EXP[(X-μ)]\n   \\\\\n    &\\stackrel{(b)}= \\VAR(X) + (t-μ)^2\n\\end{align*}\\] where \\((a)\\) uses the linearity property of expectations and \\((b)\\) uses the fact that \\(\\EXP[X-μ] = μ -μ = 0\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables</span>"
    ]
  },
  {
    "objectID": "random-vectors.html",
    "href": "random-vectors.html",
    "title": "3  Random vectors",
    "section": "",
    "text": "3.1 Classification of random vectors\nSuppose \\(X\\) and \\(Y\\) are two random variables defined on the same probability space. The CDFs \\(F_X\\) and \\(F_Y\\) provide information about their individual probabilities. However, we are often interested in functions of the two random variables, e.g., \\[\n  X + Y, \\quad XY, \\quad \\max(X,Y), \\quad \\min(X,Y), \\quad \\text{etc.}\n\\] To understand how they behave together, we need to think of the random vector \\((X,Y)\\) taking values in \\(\\reals^2\\). The natural way to do so is to think of the joint CDF \\[\nF_{X,Y}(x,y) = \\PR(\\{ ω \\in Ω : X(ω) \\le x, Y(ω) \\le y \\})\n\\] where we may write the right hand side as \\(\\PR(X \\le x, Y \\le y)\\) for short.\nAs was the case for random variables, we can also classify random vectors as discrete, continuous, and mixed.\nThe above discussion generalizes in the obvious manner to more than two random variables. Thus, we can talk about random vectors \\(X = (X_1, \\dots, X_n) \\in \\reals^n\\). In practice, we often do not make a distinction between random variables and random vectors and refer both of them simply as random variables.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#classification-of-random-vectors",
    "href": "random-vectors.html#classification-of-random-vectors",
    "title": "3  Random vectors",
    "section": "",
    "text": "A random vector \\((X,Y)\\) is called jointly discrete if it takes values in a countable subset of \\(\\reals^2\\) (we denote this subset by \\(\\ALPHABET X \\times \\ALPHABET Y\\)). The jointly discrete random variables have a joint PMF \\(P_{X,Y} \\colon \\reals^2 \\to [0,1]\\) given by \\[ \\PR(X = x, Y = y) = P_{X,Y}(x,y). \\]\nA random vector \\((X, Y)\\) is called jointly continuous (or absolutely continuous) if there exists an integrable function \\(f_{X,Y} \\colon \\reals^2 \\to [0, ∞)\\) such that \\[ F_{X,Y}(x,y) = \\int_{-∞}^x \\int_{-∞}^{y} f_{X,Y}(u,v)\\, du dv, \\quad\nx,y \\in \\reals \\] \\(f_{X,Y}\\) is called the joint PDF and can be computed as \\[\n   f_{X,Y}(x,y) = \\frac{∂^2}{∂x ∂y}F_{X,Y}(x,y).\n\\]\n\n\n\n\n\n\n\nTipCan we always define a density if CDF is continuous?\n\n\n\nContinuity of the CDF is not sufficient for a joint density to exist. Consider the joint CDF \\[\n  F_{X,Y}(x,y) = \\begin{cases}\n    0, & \\min(x,y) &lt; 0 \\\\\n    \\min(x,y), & 0 \\le \\min(x,y) \\le 1 \\\\\n    1, & \\min(x,y) &gt; 1\n  \\end{cases}\n\\] This is a valid CDF. It corresponds to \\[\n  (X,Y) = (U,U), \\quad \\text{where } U \\sim \\text{Unif}[0,1],\n\\] i.e., the distribution is uniform along the diagonal of the unit square.\nThe diagonal is a set a Lebesgue measure zero. That means for any ordinary function \\(f_{X,Y}\\) \\[\n  \\iint_{A} f_{X,Y}(x,y)\\, dx\\,dy = 0\n\\] for all subsets \\(A\\) of the diagonal. Outside the diagonal, the density must be zero. Hence, the density cannot integrate to one over the entire plane. Therefore, mathematically, a density does not exist. Such random variables are said to be singular.\nSometimes in the engineering literature, we sometimes set the PDF to be \\[\n    f_{X,Y}(x,y) = δ(x-y), \\quad 0 \\le x,y \\le 1\n\\] which produces the correct \\(F_{X,Y}\\) but delta functions are generalized functions. Thus, although the CDF is continuous, it does not have a joint density.\n\n\n\nLemma 3.2 (Properties of PMFs and PDFs)  \n\nProperties of PMFs\n\nNormalization. For a jointly discrete random vector \\((X,Y)\\), \\[\\sum_{x,y \\in \\ALPHABET X × \\ALPHABET Y}P_{X,Y}(x,y) = 1.\\]\nFor any event \\(A \\in \\ALPHABET F\\), \\[\\PR((X,Y) \\in A) = \\sum_{(x,y) \\in (\\ALPHABET X \\times \\ALPHABET Y) \\cap A} P_{X,Y}(x,y).\\]\nMarginalization.\n\n\\(\\displaystyle \\sum_{x \\in \\ALPHABET X} P_{X,Y}(x,y) = P_Y(y)\\).\n\\(\\displaystyle \\sum_{y \\in \\ALPHABET Y} P_{X,Y}(x,y) = P_X(x)\\).\n\n\nProperties of PDFs\n\nNormalization. For a jointly continuous random vector \\((X,Y)\\), \\[\\int_{-∞}^{∞} \\int_{-∞}^{∞} f_{X,Y}(x,y)\\, dxdy = 1.\\]\nFor any event \\(A \\in \\ALPHABET F\\), \\[\\PR((X,Y) \\in A) = \\iint_{(x,y) \\in A} f_{X,Y}(x,y)\\,dxdy.\\]\nMarginalization.\n\n\\(\\displaystyle \\int_{-∞}^{∞} f_{X,Y}(x,y) dx = f_Y(y)\\).\n\\(\\displaystyle \\int_{-∞}^{∞} f_{X,Y}(x,y) dy = f_X(x)\\).\n\n\n\n\n\n\nExample 3.3 Consider jointly discrete random variables \\(X \\in \\{1,2,3\\}\\) and \\(Y \\in \\{1, 2, 3\\}\\) with joint PMF \\[\n  P_{X,Y} = \\MATRIX{\n    0.1 & 0.1 & 0.2 \\\\\n    0.2 & 0.1 & 0   \\\\\n    0.3 & 0   & 0   \n  }\n\\]\n\nFind the marginals \\(P_X\\) and \\(P_Y\\).\nFind the probability of the event \\(A = \\{ X + Y = 3 \\}\\).\n\n\n\nExample 3.4 Consider \\(F_{X,Y}\\) given in Example 3.2.\n\nFind the joint density \\(f_{X,Y}\\)\nFind the marginal densities \\(f_X\\) and \\(f_Y\\).\nFind the probability of the event \\(A = \\{ X + Y \\le 1 \\}\\).\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFor \\(x,y &gt; 0\\), we have \\[\n  f_{X,Y}(x,y) = \\frac{∂^2}{∂x ∂y}F_{X,Y}(x,y) = x e^{-x(y+1)}.\n\\] Thus, \\[\n  f_{X,Y}(x,y) = \\begin{cases}\n    x e^{-x(y+1)}, & x,y &gt; 0 \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\]\nThus, \\[\n  f_X(x) = \\int_{-∞}^{∞} f_{X,Y}(x,y)\\,dy =\n  \\begin{cases}\n    e^{-x}, & x &gt; 0 \\\\\n    0, & x \\le 0\n  \\end{cases}\n\\] and \\[\n  f_Y(y) = \\int_{-∞}^{∞} f_{X,Y}(x,y)\\,dx =\n  \\begin{cases}\n    \\dfrac{1}{(1+y)^2}, & y &gt; 0 \\\\\n    0, & y \\le 0\n  \\end{cases}\n\\]\n\n\n\n\nExample 3.5 Consider a joint PDF \\[\n  f_{X,Y}(x,y) = c x y, \\quad 0 \\le y \\le x \\le 1.\n\\] Find the constant \\(c\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe know that the joint PDF must integrate to \\(1\\). Thus, \\[\\begin{align*}\n  1 &= \\int_{-∞}^{∞} f_{X,Y}(x,y)\\, dx\\, dy \\\\\n  &= \\int_{0}^{1} \\int_{0}^x c xy\\, dy\\, dx  \\\\\n  &= \\int_{0}^{1} c x\\frac{x^2}{2} dx \\\\\n  &= c \\frac{x^3}{8}\\biggr|_{0}^1 = \\frac{c}{8}\n\\end{align*}\\] Therefore, \\(c=8\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#independence-of-random-vectors",
    "href": "random-vectors.html#independence-of-random-vectors",
    "title": "3  Random vectors",
    "section": "3.2 Independence of random vectors",
    "text": "3.2 Independence of random vectors\n\nDefinition 3.1 Two random variables \\(X\\) and \\(Y\\) defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\) are said to be independent if the sigma algebras \\(σ(X)\\) and \\(σ(Y)\\) are independent.\n\nThe above definition means that if we take any (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\), then the events \\(\\{X \\in B_1\\}\\) and \\(\\{X \\in B_2\\}\\) are independent, i.e., \\[\n\\PR(X \\in B_1, Y \\in B_2) = \\PR(X \\in B_1) \\PR(Y \\in B_2).\n\\]\nUsing this, we can show that following:\n\n\\(X\\) and \\(Y\\) are independent if and only if \\[\n   F_{X,Y}(x,y) = F_X(x) F_Y(y), \\quad \\forall x, y \\in \\reals.\n\\]\nTwo jointly continuous random variables \\(X\\) and \\(Y\\) are independent if and only if \\[\n   f_{X,Y}(x,y) = f_X(x) f_Y(y), \\quad \\forall x, y \\in \\reals.\n\\]\nTwo jointly discrete random variables \\(X\\) and \\(Y\\) are independent if and only if \\[\n   P_{X,Y}(x,y) = P_X(x) P_Y(y), \\quad \\forall x, y \\in \\reals.\n\\]\n\n\nExample 3.6 Consider the random variables \\(X\\) and \\(Y\\) with the joint PMF \\(P_{X,Y}\\) given in Example 3.3. Are these random variables independent?\n\n\nExample 3.7 Consider the random variables \\(X\\) and \\(Y\\) with the joint CDF \\(F_{X,Y}\\) given in Example 3.2. Are these random variables independent?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nObserve that \\(F_{X,Y}(x,y) \\neq F_X(x) F_Y(y)\\). Hence, the two random variables are not independent.\nWe can also see that because \\(f_{X,Y}(x,y) \\neq f_X(x) f_Y(y)\\).\n\n\n\n\nExample 3.8 Consider random variables \\(X\\) and \\(Y\\) with joint PDF \\(f_{X,Y}\\) which is a uniform distribution on the unit square. Are \\(X\\) and \\(Y\\) independent?\n\n\nExample 3.9 Consider random variables \\(X\\) and \\(Y\\) with joint PDF \\(f_{X,Y}\\) which is a uniform distribution on the unit triangle Are \\(X\\) and \\(Y\\) independent?\n\nThese definitions extend naturally to any number of random variables:\n\nA sequence of random variables \\(X_1, \\dots, X_n\\) are independent if and only if \\[\n  F_{X_1, \\dots, X_n}(x_1, \\dots, x_n) = \\prod_{i=1}^n F_{X_i}(x_i).\n\\]\nA sequence of jointly continuous random variables \\(X_1, \\dots, X_n\\) are independent if and only if \\[\n  f_{X_1, \\dots, X_n}(x_1, \\dots, x_n) = \\prod_{i=1}^n f_{X_i}(x_i).\n\\]\nA sequence of jointly discrete random variables \\(X_1, \\dots, X_n\\) are independent if and only if \\[\n  P_{X_1, \\dots, X_n}(x_1, \\dots, x_n) = \\prod_{i=1}^n P_{X_i}(x_i).\n\\]\n\nUnlike independence of events, we do not need to separately check for independence of subsets of random variables because that is automatically implied due to the marginalization property.\n\nExample 3.10 Suppose (X, Y, Z) are independent with joint PDF: \\[\n  f_{X,Y,Z}(x,y,z) = f_X(x)\\, f_Y(y)\\, f_Z(z).\n\\]\nShow that \\(X\\) and \\(Y\\) are independent.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTo find the joint PDF of \\(X\\) and \\(Y\\), marginalize over \\(Z\\): \\[\nf_{X,Y}(x,y) = \\int_{-\\infty}^{\\infty} f_{X,Y,Z}(x,y,z)\\, dz\n= \\int_{-\\infty}^{\\infty} f_X(x)\\, f_Y(y)\\, f_Z(z)\\, dz.\n\\]\nSince \\(f_X(x)\\) and \\(f_Y(y)\\) do not depend on \\(z\\): \\[\nf_{X,Y}(x,y) = f_X(x)\\, f_Y(y) \\int_{-\\infty}^{\\infty} f_Z(z)\\, dz\n= f_X(x)\\, f_Y(y) \\cdot 1.\n\\]\nThus, \\(X\\) and \\(Y\\) remain independent after marginalization.\n\n\n\nAn immediate implication of the definition of independence is the following.\n\nProposition 3.1 Let \\(X\\) and \\(Y\\) be independent random variables defined on a common probability space. Consider \\(U = g(X)\\) and \\(V = h(Y)\\) for some (measurable) functions \\(g\\) and \\(h\\). Then, \\(U\\) and \\(V\\) are independent.\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nConsider any (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\) and consider the events \\(\\{ U \\in B_1 \\}\\) and \\(\\{ V \\in B_2 \\}\\). Note that\n\n\\(\\{ U \\in B_1 \\} = \\{ X \\in g^{-1}(B_1) \\}\\).\n\\(\\{ V \\in B_2 \\} = \\{ Y \\in h^{-1}(B_2) \\}\\).\n\nSince the random variables \\(X\\) and \\(Y\\) are independent, the events \\(\\{ X \\in g^{-1}(B_1) \\}\\) and \\(\\{ Y \\in h^{-1}(B_2) \\}\\). Which implies that the events \\(\\{ U \\in B_1 \\}\\) and \\(\\{ V \\in B_2 \\}\\) are independent. Consequently, the random variables \\(U\\) and \\(V\\) are independent.\n\n\n\n\nProposition 3.2 Let \\(X\\) and \\(Y\\) be independent random variables defined on a common probability space. Then \\(X\\) and \\(Y\\) are independent if and only if \\[\\begin{equation}\\label{eq:expectation-product}\n  \\EXP[ g(X) h(Y) ] = \\EXP[ g(X) ] \\EXP[ h(Y) ]\n\\end{equation}\\] for all (measurable) functions \\(g\\) and \\(h\\).\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThere are two claims here.\n\nIf \\(X\\) and \\(Y\\) are independent then \\(\\eqref{eq:expectation-product}\\) holds.\nIf \\(\\eqref{eq:expectation-product}\\) holds, then \\(X\\) and \\(Y\\) are independent.\n\nWe will prove the first claim assuming that \\(X\\) and \\(Y\\) are continuous. Similar argument works for the discrete case as well. \\[\\begin{align*}\n  \\EXP[ g(X) h(Y) ]\n  &= \\int_{-∞}^∞ \\int_{-∞}^∞ g(x) h(y) f_{X,Y}(x,y)\\, dx dy \\\\\n  &\\stackrel{(a)}= \\int_{-∞}^∞ \\int_{-∞}^∞ g(x) h(y) f_{X}(x) f_{Y}(y)\\, dy dx \\\\\n  &\\stackrel{(b)}= \\int_{-∞}^∞ \\left[ \\int_{-∞}^∞ g(x)f_{X}(x)\\, dx \\right]h(y) f_{Y}(y)  \\, dy \\\\\n  &\\stackrel{(c)}= \\left[ \\int_{-∞}^∞ g(x)f_{X}(x)\\, dx \\right]\n  \\left[\\int_{-∞}^∞  h(y) f_{Y}(y)  \\, dy \\right] \\\\\n  &= \\EXP[ g(X) ] \\EXP [ h(Y) ]\n\\end{align*}\\] where \\((a)\\) follows from the fact that \\(X \\independent Y\\), \\((b)\\) and \\((c)\\) are simple algebra, and the last step uses the definition of expectation.\nTo prove the second claim, pick any (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\) and consider the functions \\(g(x) = \\IND_{B_1}(x)\\) and \\(h(y) = \\IND_{B_2}(y)\\). Observe that \\[\\begin{align*}\n  \\PR(X \\in B_1, Y \\in B_2)\n  &= \\EXP[\\IND_{ \\{ X \\in B_1, Y \\in B_2 \\}}] \\\\\n  &\\stackrel{(d)}= \\EXP[\\IND_{ \\{ X \\in B_1 \\}} \\IND_{\\{ Y \\in B_2 \\}}] \\\\\n  &\\stackrel{(e)}=\\EXP[\\IND_{ \\{ X \\in B_1 \\}}\\ \\EXP[ \\IND_{\\{ Y \\in B_2 \\}}] \\\\\n  &\\stackrel{(f)}= \\PR(X \\in B_1) \\PR(Y \\in B_2)\n\\end{align*}\\] where \\((d)\\) follows from basic algebra, \\((e)\\) follows from \\(\\eqref{eq:expectation-product}\\), and \\((f)\\) follows from expectation of an indicator.\nThe above equation shows that for any arbitrary (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\), \\(\\PR(X \\in B_1, Y \\in B_2) = \\PR(X \\in B_1) \\PR(Y \\in B_2)\\). Hence, \\(\\{X \\in B_1\\} \\independent \\{Y \\in B_2 \\}\\). Since \\(B_1\\) and \\(B_2\\) were arbitrary, we have \\(X \\independent Y\\).\n\n\n\n\nExample 3.11 Let \\(X\\) and \\(Y\\) be independent random variables defined on a common probability space. Show that\n\n\\(\\EXP[XY] = \\EXP[X] \\EXP[Y]\\).\n\\(\\VAR(X + Y) = \\VAR(X) + \\VAR(Y)\\).\n\n\n\nDefinition 3.2 A collection of random variables \\(X_1, \\dots, X_n\\) is called independent and identically distributed (i.i.d.) if all random variables are independent and have the same distribution, i.e., \\[\n  F_{X_1}(x_1) = F_{X_2}(x_2) = \\cdots = F_{X_n}(x_n).\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#functions-of-random-variables",
    "href": "random-vectors.html#functions-of-random-variables",
    "title": "3  Random vectors",
    "section": "3.3 Functions of random variables",
    "text": "3.3 Functions of random variables\nIn an interconnected systems, the output of one system is used as input to another system. To analyze such systems, it is important to understand how to analyze functions of random variables.\nThe same idea can be used for functions of multiple random variables as we illustrate via the following examples.\n\nExample 3.12 Let \\(X\\) and \\(Y\\) be random variables defined on a common probability space. Define \\[\nU = \\max(X,Y)\n\\quad\nV = \\min(X,Y).\n\\] Find \\(F_U\\) and \\(F_V\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nWe first look at \\(F_U\\). By definition \\[ F_U(u) = \\PR(X \\le u, Y \\le u) = F_{X,Y}(u,u).\\]\nNow consider \\(F_V\\). The event \\(\\{V \\le v\\}\\) can be expressed as \\[ \\{ V \\le v \\} = \\{ X \\le v \\} \\cup \\{Y \\le v \\} \\cap \\{X \\le v \\} \\cap \\{Y \\le v\\}.\\] Thus, \\[F_V(v) = F_X(v) + F_Y(v) - F_{X,Y}(v,v). \\]\n\n\n\n\n\nExample 3.13 Suppose \\(X_1\\) and \\(X_2\\) are continuous random variables and \\(Y = X_1 + X_2\\). Find the PDF \\(f_Y(y)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe can write the CDF \\(F_Y(y)\\) as follows: \\[\nF_Y(y)\n= \\int_{-∞}^∞ \\int_{-∞}^{y - x_1} f_{X_1,X_2}(x_1, x_2)\\, d x_2 d x_1 \\\\\n\\] Therefore, \\[\\begin{align*}\nf_Y(y) &= \\frac{d F_Y(y)}{dy} \\\\\n&= \\int_{-∞}^∞ \\frac{d}{dy} \\int_{-∞}^{y-x_1} f_{X_1, X_2}(x_1, x_2) \\, dx_2\\, dx_1 \\\\\n&= \\int_{-∞}^∞ f_{X_1, X_2}(x_1, y - x_1)\\, dx_1.\n\\end{align*}\\]\n\n\n\n\nExample 3.14 Repeat Example 3.13 when \\(X_1\\) and \\(X_2\\) are independent.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nIn this case, \\(f_{X_1, X_2}(x_1, x_2) = f_{X_1}(x_1) f_{X_2}(x_2)\\). Therefore, we get \\[f_Y(y) = \\int_{-∞}^{∞} f_{X_1}(x_1) f_{X_2}(y - x_1) d x_1 = (f_{X_1} * f_{X_2})(y)\\] where \\(*\\) denotes convolution.\n\n\n\n\nExample 3.15 Repeat Example 3.14 when \\(X_1 \\sim \\text{Poisson}(λ_1)\\) and \\(X_2 \\sim \\text{Poisson}(λ_2)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall that for a Poisson random variable \\(X\\) with parameter \\(λ\\) \\[\nP_X(k) = e^{-λ} \\frac{λ^k}{k!}, \\quad k \\ge 0\n\\]\nThus, \\[\\begin{align*}\nP_Y(n) &= (P_{X_1} * P_{X_2})(n)\n= \\sum_{k=-∞}^{∞} P_{X_1}(k) P_{X_2}(n-k)\n\\\\\n&=\\sum_{k=0}^{n} P_{X_1}(k) P_{X_2}(n-k)  \n\\\\\n&= \\sum_{k=0}^n e^{-λ_1 - λ_2} \\frac{ λ_1^k λ_2^{n-k} }{ k! (n-k)! }\n\\\\\n&= e^{-(λ_1 + λ_2)} \\frac{1}{n!}\n\\sum_{k=0}^n \\frac{n!}{k!(n-k)!} λ_1^k λ_2^{n-k}\n\\\\\n&= e^{-(λ_1 + λ_2)} \\frac{(λ_1 + λ_2)^n}{n!}\n\\end{align*}\\]\nThus, \\(Y \\sim \\text{Poisson}(λ_1 + λ_2)\\).\n\n\n\n\n3.3.1 Change of variables formulas\nFor continuous random variables, it is possible to obtain a general change of variable formula to obtain the PDF of functions of random variable in terms of their joint PDF.\nSuppose \\(\\{X_1, \\dots, X_n\\}\\) are jointly continuous random variables with joint PDF \\(f\\). Consider \\(n\\) random variables: \\[\\begin{align*}\n    Y_1 &= g_1(X_1, \\dots, X_n), \\\\\n    Y_2 &= g_2(X_1, \\dots, X_n), \\\\\n    \\vdots &= \\vdots \\\\\n    Y_n &= g_n(X_1, \\dots, X_n).\n\\end{align*}\\] We can view this as an equation between two \\(n\\)-dimensional vectors \\(Y = \\VEC(Y_1, \\dots, Y_n)\\) and \\(X = \\VEC(X_1, \\dots, X_n)\\) written as \\[ Y = g(X) \\]\nAs was the case for the scalar system, for a given \\(y \\in \\reals^n\\), the vector equation \\(y = g(x)\\) may have zero, one, or multiple solutions.\n\nIf \\(y = g(x)\\), \\(y \\in \\reals^n\\) has no solution, then \\[ f_Y(y) = 0. \\]\nIf \\(y = g(x)\\), \\(y \\in \\reals^n\\) has one solution \\(x \\in \\reals^n\\), then \\[ f_Y(y) = \\frac{f_X(x)}{\\ABS{J(x)}}, \\quad \\text{where } y = g(x)\\] and \\(J(x)\\) denotes the Jacobian on \\(g(x)\\) evaluated at \\(x = (x_1, \\dots, x_n)\\), i.e., \\[\n\\def\\1#1#2{\\dfrac{∂ g_{#1}}{∂ x_{#2}}}\nJ(x_1, \\dots, x_n) =\n\\DET{ \\1 11 & \\cdots & \\1 1n \\\\\n      \\vdots & \\vdots & \\vdots \\\\\n      \\1 n1 & \\cdots & \\1 nn }\n\\]\nIf \\(y = g(x)\\), \\(y \\in \\reals^n\\) has multiple solutions given by \\(\\{x^{(1)}, \\dots, x^{(m)}\\}\\), then \\[ f_Y(y) =\n\\sum_{k=1}^m \\frac{f_X(x^{(k)})}{\\ABS{J(x^{(k)})}}.\\]\n\n\nExample 3.16 Resolve Example 3.12 using the change of variables formula.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nLet \\(g_1(x,y) = \\max\\{x, y\\}\\) and \\(g_2(x,y) = \\min\\{x,y\\}\\). Define \\[ U = g_1(X,Y) \\quad\\text{and}\\quad V = g_2(X,Y).\\]\nDefine \\(g(x,y) = \\VEC(g_1(x,y), g_2(x,y))\\). Note that \\(g\\) is not differentiable at \\(x=y\\).\n\nWhen \\(x &gt; y\\), we have \\(g_1(x,y) = x\\) and \\(g_2(x,y) = y\\). Thus, \\[\nJ(x,y) = \\DET{\\1 11 & \\1 12 \\\\ \\1 21 & \\1 22}\n= \\DET{1 & 0 \\\\ 0 & 1} = 1. \\]\nWhen \\(x &lt; y\\), we have \\(g_1(x,y) = y\\) and \\(g_2(x,y) = x\\). Thus, \\[\nJ(x,y) = \\DET{\\1 11 & \\1 12 \\\\ \\1 21 & \\1 22}\n= \\DET{0 & 1 \\\\ 1 & 0} = -1. \\]\n\nWe now compute \\(f_{U,V}(u,v)\\).\n\nIf \\(u &lt; v\\), then the equation \\((u,v) = g(x,y)\\) has no solution. So we set \\[ f_{U,V}(u,v) = 0. \\]\nIf \\(u &gt; v\\), then the equation \\((u,v) = g(x,y)\\) has two solutions: \\(\\{ (u,v), (v,u) \\}\\). Thus, \\[\nf_{U,V}(u,v) = \\frac{f_{X,Y}(u,v)}{\\ABS{1}} + \\frac{f_{X,Y}(v,u)}{\\ABS{-1}}\n= f_{X,Y}(u,v) + f_{X,Y}(v,u). \\]\nIf \\(u = v\\), then the equation \\((u,u) = g(x,y)\\) has one solution \\((u,u)\\). Thus, \\[ f_{U,V}(u,u) = f_{X,Y}(u,u). \\] Note that \\(u = v\\) is a line in two-dimensional space. (Formally, it is a set of measure zero.) Hence, the choice of \\(f_{U,V}\\) at \\(u = v\\) will not affect any probability computations. So we can also set \\[ f_{U,V}(u,u) = 0. \\]\n\nFrom the joint PDF \\(f_{U,V}\\), we can compute the marginals as follows:\n\nFor \\(U\\), we have \\[\nf_U(u) = \\int_{-∞}^{∞} f_{U,V}(u,v) dv\n= \\int_{-∞}^{u} \\bigl[ f_{X,Y}(u,v) + f_{X,Y}(v,u) \\bigr] dv.\n\\] Therefore, \\[\nF_U(u) = \\int_{-∞}^{u} f_U(\\tilde u) d\\tilde u\n= \\int_{-∞}^u \\int_{-∞}^{\\tilde u} \\bigl[ f_{X,Y}(\\tilde u,v) + f_{X,Y}(v,\\tilde u) \\bigr] dv d\\tilde u.\n\\] Note that \\[ \\int_{-∞}^u \\int_{-∞}^{\\tilde u} f_{X,Y}(\\tilde u, v) dv d\\tilde u\n= \\int_{-∞}^u \\int_{-∞}^{x} f_{X,Y}(x, y) dy dx\n\\] and \\[\\begin{align*}\n\\int_{-∞}^u \\int_{-∞}^{\\tilde u} f_{X,Y}(v, \\tilde u) dv d\\tilde u\n&= \\int_{-∞}^u \\int_{-∞}^y f_{X,Y}(x,y) dx dy\n\\\\\n&= \\int_{-∞}^u \\int_{x}^u f_{X,Y}(x,y) dy dx\n\\end{align*}\\] where the last step follows from changing the order of integration.\nSubstituting these back in the expression for \\(F_U(u)\\), we get \\[\nF_U(u)\n= \\int_{-∞}^u \\int_{-∞}^{x} f_{X,Y}(x, y) dy dx\n+ \\int_{-∞}^u \\int_{x}^u f_{X,Y}(x,y) dy dx\n= \\int_{-∞}^u \\int_{-∞}^u f_{X,Y}(x,y) dy dx = F_{X,Y}(u,u). \\]\nFor \\(V\\), we can follow similar algebra as above.\n\n\n\n\n\nExample 3.17 Let \\(X\\) and \\(Y\\) be random variables defined on a common probability space. Define \\[\n  U = X^2 \\quad\\text{and}\\quad V = X + Y.\n\\] Find \\(F_{U,V}\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nLet’s consider the system of equations \\[\n  u = x^2 \\quad\\text{and}\\quad v = x + y\n\\] for a given value of \\((u,v)\\). First observe that \\[\n  J(x,y) = \\DET{ 2x & 0 \\\\ 1 & 1 } = 2x.\n\\]\n\nIf \\(u &lt; 0\\), then the system of equations has no solutions. Therefore, \\[\n   F_{U,V}(u,v) = 0, \\quad u &lt; 0.\n\\]\nIf \\(u = 0\\), then the system of equations has one solution: \\[\n   x^{(1)} = 0 \\quad\\text{and}\\quad y^{(1)} = v.\n\\] However, \\(J(0,v) = 0\\). So, \\[\n   f_{U,V}(0,v) = \\frac{f_{X,Y}(0,v)}{J(0,v)}\n\\] is undefined. However, since \\(u = 0\\) is a line in two-dimensions (i.e., a set of measure zero), the choice of \\(f_{U,V}\\) at \\(u = 0\\) will not affect any probability computations. So, we set \\[\n   f_{U,V}(0,v) = 0.\n\\]\nIf \\(u &gt; 0\\), then the system of equations has two solutions: \\[\n   (x^{(1)}, y^{(1)}) = (+\\sqrt{u}, v - \\sqrt{u})\n   \\quad\\text{and}\\quad\n   (x^{(2)}, y^{(2)}) = (-\\sqrt{u}, v + \\sqrt{u})\n\\] Therefore, \\[\n   f_{U,V}(u,v) = \\frac{f_{X,Y}(\\sqrt{u}, v - \\sqrt{u})}{2 \\sqrt{u}}\n   + \\frac{f_{X,Y}(-\\sqrt{u}, v + \\sqrt{u})}{2 \\sqrt{u}}.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#covariance-of-random-vectors",
    "href": "random-vectors.html#covariance-of-random-vectors",
    "title": "3  Random vectors",
    "section": "3.4 Covariance of random vectors",
    "text": "3.4 Covariance of random vectors\nWe start with the definition of covariance of two real-valued random variables. We will then generalize the notion to random vectors.\n\n3.4.1 Real-valued random variables\nLet \\(X\\) and \\(Y\\) be real-valued random variables defined on the same probability space.\n\nCovariance measures how two random variables vary together. In particular, let \\(X\\) and \\(Y\\) be jointly distributed random variables and let \\(μ_X\\) and \\(μ_Y\\) denote their means. Then, \\[ \\COV(X,Y) = \\EXP[(X - μ_X) (Y - μ_Y)].\\] Properties of expectation imply that \\[\n\\COV(X,Y) = \\EXP[XY] - \\EXP[X] \\EXP[Y].\n\\]\nCorrelation coefficient between \\(X\\) and \\(Y\\) is defined as \\[ρ_{XY} = \\frac{\\COV(X,Y)}{\\sqrt{\\VAR(X) \\VAR(Y)}}.\\]\nThe correlation coefficient satisfies \\(\\ABS{ρ_{XY}} \\le 1\\) with equality if and only if \\(\\PR(aX + bY = c) = 1\\) for some \\(a,b,c \\in \\reals\\). [The proof follows from Cauchy-Schwartz inequality, which we will study later]\n\\(X\\) and \\(Y\\) are said to be uncorrelated if \\(ρ_{XY} = 0\\), which is equivalent to \\(\\COV(X,Y) = 0\\) or \\(\\EXP[XY] = \\EXP[X] \\EXP[Y]\\).\nNote that \\[\\begin{align*}\n  \\VAR(X + Y) &= \\EXP[ ((X - \\EXP[X]) + (Y - \\EXP[Y]) )^2 ]\n  \\\\\n  &= \\VAR(X) + \\VAR(Y) + 2\\COV(X,Y).\n\\end{align*}\\] Thus, when \\(X\\) and \\(Y\\) are uncorrelated, we have \\[ \\VAR(X + Y) = \\VAR(X) + \\VAR(Y). \\]\nIndependent random variables are uncorrelated but the reverse is not true.\n\n\nExample 3.18 Consider the probability space \\((Ω, \\ALPHABET F, \\PR)\\) where \\(Ω = [0, 2 π)\\), \\(\\ALPHABET F\\) is the Borel \\(σ\\)-algebra on \\([0, 2 π)\\) and \\(\\PR\\) is the uniform distribution on \\(Ω\\). Define \\(X(ω) = \\cos ω\\) and \\(Y(ω) = \\sin ω\\). Show that \\(X\\) and \\(Y\\) are uncorrelated but not independent.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe event \\(\\{X = 1\\}\\) corresponds to \\(ω = 0\\) and therefore \\(\\{Y = 0\\}\\). Thus, \\(X\\) and \\(Y\\) are not independent.\nObserve that\n\n\\(\\displaystyle \\EXP[X] = \\int_{0}^{2 π} \\cos ω \\frac{1}{2 π}\\, d ω = 0\\).\n\\(\\displaystyle \\EXP[Y] = \\int_{0}^{2 π} \\sin ω \\frac{1}{2 π}\\, d ω = 0\\).\n\\(\\displaystyle \\EXP[XY] = \\int_{0}^{2 π} \\cos ω \\sin ω \\frac{1}{2 π}\\, d ω =\n\\frac{1}{4{π}} \\int_0^{2 π} \\cos 2 ω\\, d ω = 0\\).\n\nThus, \\[\\EXP[XY] = \\EXP[X]\\EXP[Y].\\]\n\n\n\n\n\n3.4.2 Random vectors\nThe idea of covariance generalizes to random vectors. First we recall the definition of expectation for random vectors and random matrices.\n\nFor a random vector \\(X = [X_1, \\dots, X_n] \\in \\reals^n\\), we have \\[ \\EXP[X] = \\MATRIX{ \\EXP[X_1] & \\cdots & \\EXP[X_n] }. \\]\nFor a random matrix \\(X = \\MATRIX{ X_{1,1} & \\cdots & X_{1,n} \\\\ X_{2,1} & \\cdots & X_{2,n} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nX_{m,1} & \\cdots & X_{m,n} } \\in \\reals^{m \\times n}\\), we have \\[ \\EXP[X] = \\MATRIX{ \\EXP[X_{1,1}] & \\cdots & \\EXP[X_{1,n}] \\\\ \\EXP[X_{2,1}] & \\cdots & \\EXP[X_{2,n}] \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\EXP[X_{m,1}] & \\cdots & \\EXP[X_{m,n}] }.\n\\]\n\nWith the above notation, we have the following.\n\nThe covariance matrix of a random vector \\(X \\in \\reals^n\\) is defined as \\[Σ_X = \\COV(X) = \\EXP[ (X - μ_X) (X - μ_X)^\\TRANS].\\]\nWhen \\(X = [X_1, \\dots, X_n]^\\TRANS\\), the covariance can be written as \\[\n  Σ_X = \\MATRIX{\n      \\VAR(X_1) & \\COV(X_1, X_2) & \\cdots & \\COV(X_1, X_n) \\\\\n      \\COV(X_2, X_1) & \\VAR(X_2) & \\cdots & \\COV(X_2, X_n) \\\\\n      \\vdots & \\ddots & \\ddots & \\vdots \\\\\n      \\COV(X_n, X_1) & \\COV(X_n, X_2) & \\cdots & \\VAR(X_n) }\n\\]\nIt is easy to see that \\(Σ_X = Σ_X^\\TRANS\\). Thus, the covariance matrix is symmetric.\nIf the components \\(X_1, \\dots, X_n\\) are independent, then \\(Σ_X\\) is a diagonal matrix.\n\\(Σ_X\\) is positive semidefinite, i.e., its eigenvalues are real and non-negative. To see this, observe that for any (deterministic) vector \\(v\\), we have \\[\\begin{align*}\n  v^\\TRANS Σ_X v\n  &= v^\\TRANS \\EXP[ (X - μ_X) (X - μ_X)^\\TRANS ] v \\\\\n  &= \\EXP[ v^\\TRANS (X - μ_X) (X - μ_X)^\\TRANS v ]  \\\\\n  &= \\EXP[ w^\\TRANS w ] = \\EXP[ \\NORM{w}^2 ] \\ge 0.\n\\end{align*}\\] where \\(w = (X - μ_X)^\\TRANS v\\).\nThe cross covariance matrix of random vectors \\(X \\in \\reals^n\\) and \\(Y \\in \\reals^m\\) is a \\(n × p\\) matrix given by \\[\nΣ_{XY} = \\COV(X,Y) = \\EXP[ (X - μ_X) (Y - μ_Y)^\\TRANS ].\n  \\]\nTwo random vectors \\(X\\) and \\(Y\\) are called uncorrelated if \\[ Σ_{XY} = 0 \\implies\n\\EXP[XY^\\TRANS] = \\EXP[X] \\EXP[Y]^\\TRANS. \\]\nTwo random vectors \\(X\\) and \\(Y\\) are called orthogonal if \\[\\EXP[X Y^\\TRANS] = 0 \\]\n\n\n\n3.4.3 Linear transformation of mean and covariance\nConsider a random vector (not necessarily Gaussian) \\(X = [X_1, \\dots, X_n]^\\TRANS\\) with mean \\(μ_X\\) and covariance \\(Σ_X\\).\nLet \\(A \\in \\reals^{n × n}\\) be a deterministic matrix and \\(b \\in \\reals^n\\) be a deterministic vector.\nDefine \\(Y = A X + b\\) and let \\(μ_Y\\) and \\(Σ_Y\\) denote the mean and covariance of \\(Y\\). Then, \\[\n  μ_Y = A μ_X + b\n  \\quad\\text{and}\\quad\n  Σ_Y = A Σ_X A^\\TRANS\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe property of the mean follows immediately from properties of expectation. For the covariance, note that \\[\\begin{align*}\n  Σ_Y &= \\EXP[ (Y - μ_Y) (Y - μ_Y)^\\TRANS ] \\\\\n  &= \\EXP[ (A X - A μ_X) (AX - A μ_X)^\\TRANS ] \\\\\n  &= \\EXP[ A (X - μ_X) (X - μ_X)^\\TRANS A^\\TRANS ] \\\\\n  &= A Σ_X A^\\TRANS\n\\end{align*}\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "random-vectors.html#multi-dimensional-gaussian",
    "href": "random-vectors.html#multi-dimensional-gaussian",
    "title": "3  Random vectors",
    "section": "3.5 Multi-dimensional Gaussian",
    "text": "3.5 Multi-dimensional Gaussian\nA \\(n\\)-dimensional Gaussian with mean \\(μ\\) and covariance \\(Σ\\), written as \\(\\mathcal{N}(μ, Σ)\\), has the PDF \\[\n  f_X(x) = \\frac{1}{\\sqrt{(2 π)^n \\det(Σ)}}\n  \\exp\\left( - \\tfrac{1}{2} (x - μ)^\\TRANS Σ^{-1} (x-μ) \\right),\n  \\quad x \\in \\reals^n.\n\\]\nWe consider a few special cases to understand the definition.\n\nConsider the special case when all components are independent, let \\[\n   μ = \\MATRIX{μ_1 \\\\ \\vdots μ_n},\n  \\quad\\text{and}\\quad\n  Σ = \\MATRIX{σ_1^2 & \\cdots & 0 \\\\\n              \\vdots & \\ddots & \\vdots \\\\\n              0 & \\cdots & σ_n^2 }\n\\] where \\(μ_i = \\EXP[X_i]\\) and \\(σ_i^2 = \\VAR(X_i)\\). Observe that \\[\n  \\det(Σ) = \\prod_{i=1}^n σ_i^2\n\\] and \\[\n  (x - μ)^\\TRANS Σ^{-1} (x - μ) = \\sum_{i=1}^n \\frac{(x_i - μ_i)^2}{σ_i^2}.\n\\] Thus, \\[\n  f_X(x) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 π σ_i^2}}\n  \\exp\\left(-\\frac{(x_i-μ_i)^2}{2 σ_i^2} \\right)\n\\] which is the product of the marginal distributions.\nConsider the two-dimensional Gaussian vector and let \\(σ_i^2 = \\VAR(X_i)\\) and \\(ρ = \\COV(X_1,X_2)/σ_1 σ_2\\). Then, \\[\n   Σ = \\MATRIX{ σ_1^2 & ρ σ_1 σ_2 \\\\ ρ σ_1 σ_2 & σ_2^2 }.\n\\] When \\(\\ABS{ρ} = 1\\), the distribution is singular. When \\(\\ABS{ρ} &lt; 1\\), \\(\\det(Σ) = (1 - ρ^2)σ_1^2 σ_2^2\\) and \\[\n   Σ^{-1} = \\frac{1}{(1 - ρ^2)σ_1^2 σ_2^2}\n   \\MATRIX{ σ_2^2 & -ρ σ_1^2 σ_2^2 \\\\ -ρ σ_1^2 σ_2^2 & σ_1^2}.\n\\] Thus, \\[\n(x-μ)^\\TRANS Σ^{-1} (x - μ)\n= \\frac{1}{1 - ρ^2} \\biggl[\n   \\frac{(x_1 - μ_1)^2}{2 σ_1^2 }\n   - 2 ρ \\frac{(x_1 - μ_1)(x_2 - μ_2)}{σ_1 σ_2}\n   + \\frac{(x_2 - μ_2)^2}{2 σ_2^2 }\n\\biggr].\n\\] Thus, the level set \\[(x - μ)^\\TRANS Σ^{-1} (x - μ) = k^2\\] is an ellipse centered at \\(μ\\).\n\nWhen \\(ρ = 0\\), the axes of the ellipse are aligned with the coordinates.\nWhen \\(ρ &gt; 0\\), the ellipse tilts towards the \\(x_1 = x_2\\) diagonal\nWhen \\(ρ &lt; 0\\), the ellipse tilts towards the anti-diagonal.\n\n\n\n3.5.1 Linear transformation of Gaussian vectors\n\nIf real-valued random variables \\(X\\) and \\(Y\\) are jointly Gaussian, then \\(X + Y\\) is Gaussian. We will prove this later using moment generating functions.\nIf \\(X\\) and \\(Y\\) are jointly Gaussian, then they are independent if and only if they are uncorrelated.\nLet \\(X\\) let a Gaussian random vector with mean \\(μ_X\\) and covariance \\(Σ_X\\) and \\(Y = AX + b\\) for a constant matrix \\(A\\) and vector \\(b\\). Then \\(Y\\) is also a Gaussian vector with \\[μ_Y = A μ_X  + b\n\\quad\\text{and}\\quad\n  Σ_Y = A Σ_X A^\\TRANS.\n\\]\nLet \\(X \\sim \\mathcal{N}(μ, Σ)\\) and define \\(Z = Σ^{-\\frac 12}(X - μ)\\). Then, \\[\n   μ_Z = Σ^{-1}(μ - μ) = 0\n   \\quad\\text{and}\\quad\n   Σ_Z = Σ^{-\\frac 12} Σ Σ^{-\\frac 12} = I.\n\\] Thus, \\(Z \\sim \\mathcal{N}(0, I)\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random vectors</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html",
    "href": "conditional-expectation.html",
    "title": "4  Conditional probability and conditional expectation",
    "section": "",
    "text": "4.1 Conditioning on events\nConditional probability is perhaps the most important aspect of probability theory as it explains how to incorporate new information in a probability model. However, formally defining conditional probability is a bit intricate. In the notes, I will first provide an intuitive high-level explanation of conditional probability. We will then do a deeper dive, trying to develop a bit more intuition about what is actually going on.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html#conditioning-on-events",
    "href": "conditional-expectation.html#conditioning-on-events",
    "title": "4  Conditional probability and conditional expectation",
    "section": "",
    "text": "Recall that conditional probability for events is defined as follows: given a probability space \\((Ω, \\ALPHABET F, \\PR)\\) and events \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(B) &gt; 0\\), we have \\[\n\\PR(A \\mid B) = \\frac{\\PR(A \\cap B)}{\\PR(B)}.\n\\]\nBuilding on this definition, we can define the conditional CDF of a random variable \\(X\\) conditioned on an event \\(C\\) (such that \\(\\PR(C) &gt; 0\\)) as follows: \\[\nF_{X|C}(x \\mid C) = \\PR(X \\le x \\mid C) = \\frac{\\PR( \\{ X \\le x \\} \\cap C)}{\\PR(C)}.\n\\]\nAs we pointed out, conditional probabilities are probabilities, the conditional CDF defined above satisfies the properties of regular CDFs. In particular\n\n\\(0 \\le F_{X|C}(x\\mid C) \\le 1\\)\n\\(\\displaystyle \\lim_{x \\to -∞} F_{X|C}(x \\mid C) = 0\\)\n\\(\\displaystyle \\lim_{x \\to +∞} F_{X|C}(x \\mid C) = 1\\)\n\\(F_{X|C}(x \\mid C)\\) is non-decreasing function.\n\\(F_{X|C}(x \\mid C)\\) is right-continuous function.\n\nSince \\(F_{X|C}\\) is CDF, we can classify random variables conditioned on an event as discrete or continuous in the usual way. In particular\n\nIf \\(F_{X|C}\\) is piecewise constant, then \\(X\\) conditioned on \\(C\\) is a discrete random variable which takes values in a finite or countable subset \\(\\text{range}(X\\mid C) = \\{x_1, x_2, \\dots\\}\\) of \\(\\reals\\). Furthermore, \\(X\\) conditioned on \\(C\\) has a conditional PMF \\(P_{X|C} \\colon \\reals \\to [0,1]\\) defined as \\[\n  P_{X|C}(x\\mid C) = F_{X|C}(x\\mid C) - F_{X|C}(x^{-} \\mid C).\n\\]\nIf \\(F_{X|C}\\) is continuous, then \\(X\\) conditioned on \\(C\\) is a continuous random variable which has a conditional PDF \\(f_{X|C}\\) given by \\[\nf_{X|C}(x\\mid C) = \\frac{d}{dx} F_{X|C}(x \\mid C).\n\\]\nIf \\(F_{X|C}\\) is neither piecewise constant nor continuous, then \\(X\\) conditioned on \\(C\\) is a mixed random variable.\n\nTherefore, a random variable conditioned on an event behaves exactly like a regular random variable. We can define conditional expectation \\(\\EXP[X \\mid C]\\), conditional variance \\(\\VAR(X \\mid C)\\) in the obvious manner.\nIf \\(X\\) is a random variable with \\(\\EXP[\\ABS{X}] &lt; ∞\\) and \\(C\\) is an event with \\(\\PR(C) &gt; 0\\), then \\(\\EXP[X | C]\\) is defined as the expectation with respect with the conditional CDF \\(F_{X|C}\\). In particular, \\[\n   \\EXP[ X \\mid C] = \\frac{\\EXP[ X \\IND_{C}]}{\\PR(C)}.\n\\]\nAn immediate implication of the law of total probability is the following.\nIf \\(C_1, C_2, \\dots, C_n\\) is a partition of \\(Ω\\), then \\[\n  F_X(x) = \\sum_{i=1}^n F_{X|C_i}(x \\mid C_i) \\PR(C_i).\n\\] Furthermore, if \\(X\\) and \\(X\\) conditioned on \\(C\\) are both discrete, we have \\[\nP_X(x) = \\sum_{i=1}^n P_{X|C_i}(x \\mid C_i) \\PR(C_i)\n\\] and if \\(X\\) and \\(X\\) conditioned on \\(C\\) are both continuous, we have \\[\n  f_X(x) = \\sum_{i=1}^n f_{X|C_i}(x \\mid C_i) \\PR(C_i).\n\\]\nIf \\(C_1, C_2, \\dots, C_n\\) is a partition of \\(Ω\\), then we can write \\[\n    X = \\sum_{i=1}^n X \\IND_{C_i}.\n\\] This gives us the law of total expectation \\[\n    \\EXP[X] = \\sum_{i=1}^n \\EXP[ X \\mid C_i ] \\PR(C_i).\n\\]\n\n\nExample 4.1 Consider the following experiment. A fair coin is tossed. If the outcome is heads, \\(X\\) is a uniform \\([0,1]\\) random variable. If the outcome is tails, \\(X\\) is \\(\\text{Bernoulli}(p)\\) random variable. Find \\(F_X(x)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nLet \\(C_H\\) denote the event that the coin shows heads and \\(C_T\\) the event that it shows tails. The coin is fair, so \\(\\PR(C_H) = \\PR(C_T) = \\frac{1}{2}\\).\n\nIf heads (\\(C_H\\)): \\(X \\sim \\text{Uniform}[0,1]\\), so the conditional CDF is \\[\nF_{X|C_H}(x) =\n\\begin{cases}\n  0, & x &lt; 0 \\\\\n  x, & 0 \\leq x \\leq 1 \\\\\n  1, & x &gt; 1\n\\end{cases}\n\\]\nIf tails (\\(C_T\\)): \\(X \\sim \\text{Bernoulli}(p)\\), so the conditional CDF is \\[\nF_{X|C_T}(x) =\n\\begin{cases}\n  0, & x &lt; 0 \\\\\n  1-p, & 0 \\leq x &lt; 1 \\\\\n  1, & x \\geq 1\n\\end{cases}\n\\]\n\nBy the law of total probability, \\[\\begin{align*}\nF_X(x) &= F_{X|C_H}(x)\\, \\PR(C_H) + F_{X|C_T}(x)\\, \\PR(C_T) \\\\\n&= \\frac{1}{2} F_{X|C_H}(x) + \\frac{1}{2} F_{X|C_T}(x)\n\\end{align*}\\]\nSo, \\[\nF_X(x) =\n\\begin{cases}\n0, & x &lt; 0 \\\\\\\\\n\\frac{1}{2}x + \\frac{1}{2}(1-p), & 0 \\leq x &lt; 1 \\\\\\\\\n\\frac{1}{2} \\cdot 1 + \\frac{1}{2} \\cdot 1 = 1, & x \\geq 1\n\\end{cases}\n=\n\\begin{cases}\n0, & x &lt; 0 \\\\\n\\frac{1}{2}x + \\frac{1-p}{2}, & 0 \\leq x &lt; 1 \\\\\n1, & x \\geq 1\n\\end{cases}\n\\] Thus, \\(X\\) is a mixed random variable.\n\n\n\n\nExample 4.2 (Memoryless property of geometric random variable) Let \\(X \\sim \\text{geometric}(p)\\) and \\(m,n\\) be positive integers. Compute \\[ \\PR(X &gt; n + m \\mid X &gt; m). \\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall that the PMF of a geometric random variable is \\[\nP_X(k) = p (1-p)^{k-1}, \\quad k \\in \\naturalnumbers.\n\\] Therefore, \\[\n\\PR(X &gt; m) = \\sum_{k = m + 1}^∞ P_X(k)\n= \\sum_{k=m+1}^{∞} p (1-p)^{k-1} = (1-p)^m.\n\\]\nNow consider \\[\\begin{align*}\n\\PR(X &gt; m + n \\mid X &gt; m) &=\n\\frac{ \\PR(\\{ X &gt; m + n \\} \\cap \\{X &gt; m \\}) } {\\PR(X &gt; m) }\n\\\\\n&=\n\\frac{ \\PR(X &gt; m + n ) } {\\PR(X &gt; m) } \\\\\n&= \\frac{(1-p)^{m+n}}{(1-p)^m} = (1-p)^n = \\PR(X &gt; n).\n\\end{align*}\\]\nThis is called the memoryless property of a geometric random variable.\n\n\n\n\nExample 4.3 (Memoryless property of exponential random variable) Let \\(X \\sim \\text{Exponential}(λ)\\) and \\(t,s\\) be positive reals. Compute \\[ \\PR(X &gt; t + s \\mid X &gt; t). \\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall that the PDF of an exponential random variable is \\[\nf_X(x) = λ e^{-λ x}, \\quad x \\ge 0.\n\\] Therefore, \\[\n\\PR(X &gt; t) = \\int_{t}^{∞} f_X(x)\\, dx = e^{-λ t}.\n\\]\nNow consider \\[\\begin{align*}\n\\PR(X &gt; t + s \\mid X &gt; t) &=\n\\frac{ \\PR(\\{ X &gt; t + s \\} \\cap \\{X &gt; t \\}) } {\\PR(X &gt; t) }\n\\\\\n&=\n\\frac{ \\PR(X &gt; t + s ) } {\\PR(X &gt; t) } \\\\\n&= \\frac{e^{-λ(t+s)}}{e^{-λt}} = e^{-λs} = \\PR(X &gt; s).\n\\end{align*}\\]\nThis is called the memoryless property of a exponential random variable.\n\n\n\n\nExample 4.4 Suppose \\(X\\) and \\(Y\\) are random variables that are uniformly distributed in the shaded region shown in Figure 4.1. Compute \\[\n  \\PR( X - Y &lt; 1 \\mid Y &lt; 2).\n\\]\n\nf_X_Y_2025_mid_term_1 = [\n  {x: 0, y: 0},\n  {x: 4, y: 0},\n  {x: 4, y: 4},\n  {x: 0, y: 0},\n];\n\nf_X_Y_2025_mid_term_1_A = [\n  {x: 0, y: 0},\n  {x: 4, y: 4},\n];\n\nf_X_Y_2025_mid_term_1_A_2 = [\n  {x: 1, y: 0},\n  {x: 4, y: 3},\n];\n\nf_X_Y_2025_mid_term_1_B = [\n  {x: 0, y: 0},\n  {x: 4, y: 0},\n  {x: 4, y: 2},\n  {x: 2, y: 2},\n  {x: 0, y: 0},\n];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  width: 300,\n  height: 300,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.areaY(f_X_Y_2025_mid_term_1, {x: \"x\", y: \"y\", fill: \"lightblue\"}),\n    Plot.line(f_X_Y_2025_mid_term_1, {x: \"x\", y: \"y\", stroke: \"black\"}),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 4.1: The joint PDF \\(f_{X,Y}\\) is uniform in the shaded region and zero outside.\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nDefine the events \\[\n  A = \\{ ω : X(ω) - Y(ω) &lt; 1 \\}\n  \\quad\\text{and}\\quad\n  B = \\{ ω : Y(ω) &lt; 2 \\}.\n\\] These are shown in Figure 4.2\nPlot.plot({\n  grid: true,\n  width: 300,\n  height: 300,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.areaY(f_X_Y_2025_mid_term_1_A, {x: \"x\", y: \"y\", fill: \"blue\"}),\n    Plot.areaY(f_X_Y_2025_mid_term_1_A_2, {x: \"x\", y: \"y\", fill: \"lightblue\"}),\n    Plot.line(f_X_Y_2025_mid_term_1, {x: \"x\", y: \"y\", stroke: \"black\"}),\n  ]\n})\nPlot.plot({\n  grid: true,\n  width: 300,\n  height: 300,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.areaY(f_X_Y_2025_mid_term_1, {x: \"x\", y: \"y\", fill: \"lightblue\"}),\n    Plot.areaY(f_X_Y_2025_mid_term_1_B, {x: \"x\", y: \"y\", fill: \"blue\"}),\n    Plot.line(f_X_Y_2025_mid_term_1, {x: \"x\", y: \"y\", stroke: \"black\"}),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(A = \\{ X - Y &lt; 1 \\}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(B = \\{ Y &lt; 2 \\}\\).\n\n\n\n\n\n\n\nFigure 4.2: The events \\(A\\) and \\(B\\)\n\n\n\nObserve that \\[\n  f_{X,Y}(x,y) = \\frac 18, \\quad 0 &lt; y &lt; x &lt; 4.\n\\] We are interested in computing \\[\n    \\PR(A \\mid B) = \\frac{\\PR(A \\cap B)}{\\PR(B)}.\n\\] We will compute the numerator and denominator separately. Note that since the joint distribution is a uniform distribution on the light red region, the probability of the dark blue regions is equal to their area, which can be either computed geometrically or analytically. We follow the latter approach here.\nObserve that \\[\n    \\PR(A\\cap B) = \\int_{0}^2 \\int_{y}^{y+1} \\frac 18 dx dy\n    = \\int_{0}^{2} \\frac 18 dy = \\frac 14.\n\\]\nSimilarly \\[\n    \\PR(B) =\n    \\int_{0}^2 \\int_{y}^{4} \\frac 18 dx dy\n    = \\int_{0}^2 \\frac{1}{8}(4 - y) dy\n    = \\frac 34.\n\\] Thus, \\[\n    \\PR(A \\mid B) = \\frac{\\PR(A \\cap B)}{\\PR(B)} = \\frac{1}{3}.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html#conditioning-on-random-variables",
    "href": "conditional-expectation.html#conditioning-on-random-variables",
    "title": "4  Conditional probability and conditional expectation",
    "section": "4.2 Conditioning on random variables",
    "text": "4.2 Conditioning on random variables\nWe first start with the case where we are conditioning on discrete random variables.\n\nIf \\(X\\) and \\(Y\\) are random variables defined on a common probability space and \\(Y\\) is discrete, then \\[F_{X|Y}(x \\mid y) = \\PR(X \\le x \\mid Y = y)\\] for any \\(y\\) such that \\(\\PR(Y = y) &gt; 0\\).\nIf \\(X\\) is also discrete, the conditional PMF \\(P_{X| Y}\\) is defined as \\[P_{X|Y}(x|y) = \\PR(X = x \\mid Y = y) = \\frac{P_{X,Y}(x,y)}{P_Y(y)}\\] for any \\(y\\) such that \\(\\PR(Y = y) &gt; 0\\).\nMoreover, we have that \\[ P_{X|Y}(x\\mid y) = F_{X|Y}(x\\mid y) - F_{X|Y}(x^{-}\\mid y). \\]\nThe above expression can be written differently to give the chain rule for random variables: \\[\n   P_{X,Y}(x,y) = P_{Y}(y) P_{X|Y}(x|y).\n\\]\nFor any event \\(B\\) in \\(\\mathscr B(\\reals)\\), the law of total probability may be written as \\[\\PR(X \\in B) = \\sum_{y \\in \\ALPHABET Y} \\PR(X \\in B \\mid Y = y) P_Y(y). \\]\nIf \\(X\\) is independent of \\(Y\\), we have \\[\n   P_{X|Y}(x\\mid y) = P_X(x), \\quad \\forall x,y \\in \\reals.\n\\]\n\nWe now consider the case when we are conditioning on a continuous random variable.\n\nIf \\(Y\\) is continuous, \\(\\PR(Y = y) = 0\\) for all \\(y\\). We may think of \\[ F_{X|Y}(x\\mid y) = \\lim_{δ \\downarrow 0} \\PR(X \\le x \\mid y \\le Y \\le y + δ).\\]\nWhen \\(X\\) and \\(Y\\) are jointly continuous, we define the conditional PDF \\[ f_{X|Y}(x \\mid y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}. \\]\nNote that the conditional PDF cannot be interpreted in the same manner as the conditional PMF because it gives the impression that we are conditioning on a zero-probability event. However, we can view it as a limit as follows:\n\\[\\begin{align*}\n   F_{X|Y}(x\\mid y) &= \\lim_{Δy \\downarrow 0} \\PR(X \\le x \\mid y \\le Y \\le y + Δy) \\\\\n   &= \\lim_{Δy \\downarrow 0} \\dfrac{\\PR(X \\le x, y \\le Y \\le y + Δy)}{\\PR(y \\le Y \\le y + Δy)} \\\\\n   &= \\lim_{Δy \\downarrow 0} \\dfrac{\\int_{-∞}^x \\int_{y}^{y + Δy} f_{X,Y}(u,v)\\, dv\\, du }{ \\int_{y}^{y+Δy} f_Y(v)\\, dv }\n\\end{align*}\\] If \\(Δy\\) is small, we can approximate \\[\n\\int_{y}^{y + Δy} f_Y(v)\\, dv \\approx f_Y(y) \\cdot Δy\n\\] and \\[\n\\int_{y}^{y + Δy} f_{X,Y}(u,v) dv \\approx f_{X,Y}(u,y) \\cdot Δy.\n\\] Substituting in the above equation, we get \\[\\begin{align*}\n   F_{X|Y}(x \\mid y) &\\approx \\lim_{Δy \\downarrow 0}\n   \\dfrac{ \\int_{-∞}^x f_{X,Y}(u,y) Δy\\, du }{ f_Y(y) Δy }\n   \\\\\n   &= \\int_{-∞}^x \\left[ \\frac{f_{X,Y}(u,y)}{f_Y(y)} \\right] du\n\\end{align*}\\]. Thus, when \\(X\\) and \\(Y\\) are jointly continuous, we have \\[\nf_{X|Y}(x\\mid y) = \\frac{d}{dx} F_{X|Y}(x \\mid y) =\n  \\dfrac{f_{X,Y}(x,y)}{f_Y(y)}.\n\\]\nThe formal definition of conditional densities requires some ideas from advanced probability theory, which we will not cover in this course. Nonetheless, I will try to explain the intuition behind the formal definitions in the next section.\nThe above expression may be written differently to give the chain rule for random variables: \\[ f_{X,Y}(x,y) = f_Y(y) f_{X|Y}(x \\mid y). \\]\nFor any event \\(B \\in \\mathscr B(\\reals)\\), the law of total probability may be written as \\[\n\\PR(X \\in B) = \\int_{-∞}^∞ \\PR(X \\in B \\mid Y = y) f_Y(y)\\, dy\n\\] An immediate implication of this is \\[\nF_X(x) = \\PR(X \\le x) = \\int_{-∞}^{∞} \\PR(X \\le x \\mid Y = y) f_Y(y)\\, dy\n= \\int_{-∞}^{∞} F_{X|Y}(x|y) f_Y(y)\\, dy.\n\\]\nIf \\(X\\) is independent of \\(Y\\), we have \\[\nf_{X|Y}(x\\mid y) = f_X(x), \\quad \\forall x, y \\in \\reals.\n\\]\nWe can show that conditional PMF and conditional PDF satisfy all the properties of PMFs and PDFs. Therefore, we can define conditional expectation \\(\\EXP[ g(X) \\mid Y = y ]\\) in terms of \\(P_{X|Y}\\) or \\(f_{X|Y}\\). We can similarly define conditional variance \\(\\VAR(X \\mid Y = y)\\) using the conditional probability law: \\[\n\\VAR(X \\mid Y = y) = \\EXP\\left[ (X - \\EXP[X \\mid Y = y])^2 \\mid Y = y \\right].\n\\]\n\n\nExample 4.5 Suppose \\(X\\) and \\(Y\\) are jointly continuous random variables with the joint PDF \\[\nf_{X,Y}(x,y) = \\frac{e^{-x/y} e^{-y}}{y}, \\quad 0 &lt; x &lt; ∞, 0 &lt; y &lt; ∞.\n\\] Find \\(f_{X|Y}\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe first compute the marginal \\(f_Y(y)\\).\n\\[\\begin{align*}\nf_Y(y) &= \\int_{-∞}^{∞} f_{X,Y}(x,y) \\, dx \\\\\n&= \\int_{0}^{∞} \\frac{e^{-x/y} e^{-y}}{y} dx \\\\\n&= \\frac{e^{-y}}{y} \\int_{0}^∞ e^{-x/y} dx \\\\\n&= e^{-y}.\n\\end{align*}\\] Thus, \\[ f_{X|Y}(x \\mid y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\n= \\frac{e^{-x/y}}{y}, \\quad 0 &lt; x &lt; ∞, 0 &lt; y &lt; ∞.\n\\]\n\n\n\n\nExample 4.6 Suppose \\(X \\sim \\text{Uniform}[0,1]\\) and given \\(X = x\\), \\(Y\\) is uniformly distributed on \\((0,x)\\). Find the PDF of \\(Y\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe will use the law of total probability. \\[\nF_Y(y) = \\int_{-∞}^{∞} F_{Y|X}(y \\mid x) f_X(x) \\, dx\n= \\int_{0}^1 F_{Y|X}(y \\mid x) \\, dx\n\\] where we have used the fact that \\(f_X(x) = 1\\) for \\(x \\in [0,1]\\). Now, we know that given \\(X = x\\), \\(Y \\sim \\text{uniform}[0,x]\\). Therefore, \\[\nf_{Y|X}(y\\mid x) = \\frac 1x, \\quad 0 &lt; y &lt; x.\n\\] Therefore, \\[\nF_{Y|X}(y \\mid x) = \\begin{cases}\n0 & y &lt; 0 \\\\\n\\dfrac{y}{x} & 0 &lt; y &lt; x \\\\\n1 & y \\ge x\n\\end{cases}\n\\]\nWe will compute \\(F_Y(y)\\) for the three cases separately.\n\nFor \\(y &lt; 0\\), \\[ F_Y(y) = \\int_{0}^1 F_{Y|X}(y|x) dx = 0.\\]\nFor \\(0 &lt; y &lt; 1\\), \\[ F_Y(y) = \\int_{0}^y 1\\, dx + \\int_{y}^1 \\frac{y}{x} \\, dx\n= y - y \\ln y. \\]\nFor \\(y &gt; 1\\), \\[ F_Y(y) = \\int_{0}^1 1 \\, dx = 1. \\]\n\nThus, \\[\nF_Y(y) = \\begin{cases}\n0 & y &lt; 0 \\\\\ny - y \\ln y & 0 \\le y &lt; 1 \\\\\n1 & y &gt; 1.\n\\end{cases}\n\\]\nHence, \\[\nf_Y(y) = \\frac{d F_{Y}(y)}{dy} = - \\ln y, \\quad 0 &lt; y &lt; 1.\n\\]\n\n\n\n\nExample 4.7 Let \\(X = [X_1, X_2]\\) be a bivariate Gaussian random variable with \\(μ_X = 0\\) and \\[\nΣ_X = \\MATRIX{ σ_1^2 & ρ σ_1 σ_2 \\\\ ρ σ_1 σ_2 & σ_2^2 }\n\\] where \\(\\ABS{ρ} &lt; 1\\). Compute the conditional PDF \\(f_{X_1 | X_2}\\)?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall that \\[\n  f_{X_1,X_2}(x_1, x_2) =\n  \\frac{1}{(2 π) σ_1 σ_2 \\sqrt{(1 - ρ^2)}}\n  \\exp\\left( \\frac{1}{1 - ρ^2}\n  \\left[\n    \\frac{x_1^2}{2 σ_1^2} - 2 ρ \\frac{x_1 x_2}{σ_1 σ_2}\n    + \\frac{x_2^2}{2 σ_2^2} \\right]\n  \\right).\n\\] Moreover, \\[\n  f_{X_2}(x_2) = \\frac{1}{\\sqrt{2 π} σ_2} \\exp\\left(\n  - \\frac{x_2^2}{2 σ_2^2 } \\right).\n\\] Thus, \\[\\begin{align*}\n  f_{X_1 | X_2}(x_1 | x_2) &= \\frac{f_{X_1, X_2}(x_1, x_2)}{f_{X_2}(x_2) }\n  \\\\\n  &= \\frac{1}{\\sqrt{2 π σ_2 (1 - ρ^2)}}\n  \\exp\\left(\n    -\\frac{(x_1 - ρ \\frac{σ_1}{σ_2} x_2)^2}{ 2 σ_1^2 (1 - ρ^2) }\n  \\right).\n\\end{align*}\\] Thus, \\(X_1 \\mid X_2 = x_2\\) is \\(\\mathcal{N}(ρ \\frac{σ_1}{σ_2} x_2, σ_1^2(1 - ρ^2))\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html#conditional-expectation",
    "href": "conditional-expectation.html#conditional-expectation",
    "title": "4  Conditional probability and conditional expectation",
    "section": "4.3 Conditional expectation",
    "text": "4.3 Conditional expectation\n\nWhen \\(X\\) and \\(Y\\) are jointly discrete with joint PMF \\(P_{X,Y}\\), then we can define conditional expectation as \\[\n\\EXP[X \\mid Y = y] = \\sum_{x \\in \\ALPHABET X} x P_{X|Y}(x|y)\n\\] and, more generally, for any function \\(g\\), \\[\n\\EXP[g(X) \\mid Y = y] = \\sum_{x \\in \\ALPHABET X} g(x) P_{X|Y}(x|y).\n\\]\nAnalogously, \\(X\\) and \\(Y\\) are jointly continuous with joint PDF \\(f_{X,Y}\\), we can define conditional expectation as \\[\n\\EXP[X|Y = y] = \\int_{-∞}^{∞} x f_{X|Y}(x|y)\\, dx\n\\] and, more generally, for any function \\(g\\), \\[\n\\EXP[g(X)|Y = y] = \\int_{-∞}^{∞} g(x) f_{X|Y}(x|y)\\, dx\n\\]\nNow consider a function \\(g(X,Y)\\). Then, \\[\n\\EXP[g(X,Y) \\mid Y = y] = \\int_{-∞}^{∞} g(x,y) f_{X|Y}(x|y)\\, dx\n= \\EXP[g(X,y) \\mid Y = y].\n\\]\nObserve that \\[\\begin{align*}\n\\int_{-∞}^{∞} \\EXP[g(X,Y) \\mid Y = y ] f_Y(y) dy\n&=\n\\int_{-∞}^{∞} \\int_{-∞}^{∞} g(x,y) f_{X|Y}(x|y) f_Y(y)\\, dx\\, dy \\\\\n&=\n\\int_{-∞}^{∞} \\int_{-∞}^{∞} g(x,y) f_{X,Y}(x,y)\\, dx\\, dy \\\\\n&= \\EXP[g(X,Y)].\n\\end{align*}\\] This is known as the smoothing property of conditional expectation.\nMore precisely, we may think of \\(\\EXP[g(X,Y) | Y = y]\\) as a function of \\(y\\), say \\(h(y)\\). Then \\(h(Y) = \\EXP[g(X,Y) | Y]\\) is a random variable! The smoothing property of conditional expectation says that \\[ \\EXP[ \\EXP[ g(X,Y) | Y ] ] = \\EXP[ g(X,Y) ]. \\] This is a subtle point, and we will spend some time to develop an intuition of what this means.\n\n\n4.3.1 Conditioning on a \\(σ\\)-algebra\nThe key idea is conditioning on a \\(σ\\)-algebra. To avoid technical subtleties, we restrict to the discrete case.\n\nConsider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) where \\(\\ALPHABET F\\) is a finite \\(σ\\)-algebra. Let \\(\\ALPHABET G\\) be a sub-\\(σ\\)-algebra of \\(\\ALPHABET F\\). In particular, we assume that there is a partition \\(\\{D_1, D_2, \\dots, D_m\\}\\) of \\(Ω\\) such that \\(\\ALPHABET G = 2^{\\{D_1, \\dots, D_m\\}}\\). The elements \\(D_1, \\dots, D_m\\) are called the atoms of the \\(σ\\)-algebra \\(\\ALPHABET G\\).\nTODO: Add example. 4x4 grid. partition for \\(\\ALPHABET G\\).\nWe define \\(\\PR(A \\mid \\ALPHABET G)\\) (which we will write as \\(\\EXP[\\IND_{A} \\mid \\ALPHABET G]\\) as \\[\n\\EXP[\\IND_{A} \\mid \\ALPHABET G](ω)\n= \\sum_{i=1}^m \\EXP[ \\IND_{A} \\mid D_i ] \\IND_{D_i}(ω).\n\\] Thus, on each \\(ω \\in D_i\\), the value of \\(\\EXP[I_{A} \\mid \\ALPHABET G]\\) is equal to \\(\\EXP[I_{A} \\mid D_i]\\).\nThis idea can be extended to any random variable instead of \\(\\IND_{A}\\), that is, for any random variable \\(X\\) \\[\n\\EXP[X \\mid \\ALPHABET G](ω)\n= \\sum_{i=1}^m \\EXP[ X \\mid D_i ] \\IND_{D_i}(ω).\n\\] Thus, on each \\(ω \\in D_i\\), the value of \\(\\EXP[X \\mid \\ALPHABET G]\\) is equal to \\(\\EXP[X \\mid D_i]\\).\nWhen \\(\\ALPHABET G = \\{\\emptyset, Ω\\}\\) is the trivial \\(σ\\)-algebra, \\[\\EXP[X \\mid \\{\\emptyset, Ω\\}] = \\EXP[X].\\]\nWhen \\(X = \\IND_{A}\\), \\(\\EXP[ \\IND_{A} \\mid \\ALPHABET G] = \\PR(A \\mid \\ALPHABET G)\\).\nIf \\(X_1\\) and \\(X_2\\) are joint random variables and \\(a_1\\) and \\(a_2\\) are constants, then \\[ \\EXP[ a_1 X_1 + a_2 X_2 \\mid \\ALPHABET G]\n= a_1 \\EXP[X_1 \\mid \\ALPHABET G] + a_2 \\EXP[X_2 \\mid \\ALPHABET G]. \\]\nIf \\(Y\\) is another random variable which is \\(\\ALPHABET G\\) measurable (i.e., \\(Y\\) takes constant values on the atoms of \\(\\ALPHABET G\\)), then \\[\n\\EXP[X Y \\mid \\ALPHABET G] = Y \\EXP[X \\mid \\ALPHABET G].\n\\]\n[The result can be proved pictorially.]\n\n\n\n4.3.2 Smoothing property of conditional expectation\nLet \\(\\ALPHABET H ⊂ \\ALPHABET G ⊂ \\ALPHABET F\\), where \\(⊂\\) means sub-\\(σ\\)-algebra. Let \\(\\{E_1, \\dots, E_k\\}\\) denote the partition corresponding to \\(\\ALPHABET H\\) and \\(\\{D_1, \\dots, D_m\\}\\) denote the partition corresponding to \\(\\ALPHABET G\\). The fact that \\(\\ALPHABET H\\) is a sub-\\(σ\\)-algebra of \\(\\ALPHABET G\\) means that each atom \\(E_i\\) of \\(\\ALPHABET H\\) is a union of atoms of \\(\\ALPHABET G\\) (i.e., \\(\\{D_1, \\dots, D_m\\}\\) is a refinement of the partition \\(\\{E_1, \\dots, E_k\\}\\)). Therefore, \\[\\begin{equation}\\tag{smoothing-property}\n\\bbox[5pt,border: 1px solid]\n{\\EXP[ \\EXP[ X \\mid \\ALPHABET G ] \\mid \\ALPHABET H ] =\n\\EXP[ X \\mid \\ALPHABET H].}\n\\end{equation}\\] This is known as the smoothing property of conditional expectation.\nA special case of the above property is that \\[\n\\EXP[ \\EXP[ X \\mid \\ALPHABET G] ] =\n\\EXP[ X ].\n\\] where we have taken \\(\\ALPHABET H = \\{\\emptyset, Ω\\}\\) as the trivial \\(σ\\)-algebra. Observe that the above definition is equivalent to the law of total probability!\n\n\n4.3.3 Conditioning on random variable\n\nNow suppose \\(Y\\) is a discrete random variable, then \\(\\PR(A \\mid Y)\\) and \\(\\EXP[X \\mid Y]\\) may be viewed as a short-hand notation for \\(\\PR(A \\mid σ(Y))\\) and \\(\\EXP[X \\mid σ(Y)]\\). Similar interpretations hold for conditioning on multiple random variables (or, equivalently, conditioning on random vectors).\nThe smoothing property of conditional expectation can then be stated as \\[\n\\EXP[ \\EXP[ X | Y_1, Y_2 ] Y_1 ] = \\EXP[ X | Y_1 ].\n\\]\nAn implication of the smoothing property is the following: for any (measurable) function \\(g \\colon \\reals \\to \\reals\\), \\[ \\EXP[ g(Y) \\EXP[ X \\mid Y] ] = \\EXP[ X g(Y) ]. \\]\n\n\n\nThis previous property is used for generalizing the definition of conditional expectation to continuous random variables. First, we consider conditioning wrt \\(σ\\)-algebra \\(\\ALPHABET G \\subset \\ALPHABET F\\), which is not necessarily finite (or countable).\nThen, for any non-negative1 random variable \\(X\\), \\(\\EXP[X \\mid \\ALPHABET G]\\) is defined as a \\(\\ALPHABET G\\)-measurable random variable that satisfies \\[ \\EXP[ \\IND_{A} \\EXP[ X \\mid \\ALPHABET G] ] = \\EXP[ X \\IND_{A} ] \\] for every \\(A \\in \\ALPHABET G\\).\n\n1 We start with non-negative random variables just to avoid the concerns with existence of expectation due to \\(∞ - ∞\\) nature. A similar definition works in general as long as we can rule out \\(∞ - ∞\\) case.\nIt can be shown that \\(\\EXP[X \\mid \\ALPHABET G]\\) exists and is unique up to sets of measure zero. Formally, one takes about a “version” of conditional expectation.\nThen \\(\\EXP[X \\mid Y]\\) for \\(Y\\) continuous may be viewed as \\(\\EXP[X \\mid σ(Y)]\\).\nThe formal definition of conditional expectation implies that if we take any Borel subsets \\(B_X\\) of \\(\\reals\\), then \\(\\PR(X \\in B_X \\mid Y)\\) is a (measurable) function \\(m(y)\\) that satisfies \\[\\begin{equation}\\label{eq:defn-cond}\n\\PR(X \\in B_X, Y \\in B_Y) = \\int_{B_Y} m(y) f_Y(y) dy\n\\end{equation}\\] for all Borel subsets \\(B_Y\\) of \\(\\reals\\).\nWe will show that \\[ m(y) = \\int_{B_X} \\frac{ f_{X,Y}(x,y) }{ f_{Y}(y) } \\, dx \\] satisfies \\(\\eqref{eq:defn-cond}\\). In particular, the RHS of \\(\\eqref{eq:defn-cond}\\) is \\[\n\\int_{B_Y} \\left[ \\int_{B_X} \\frac{ f_{X,Y}(x,y) }{ f_{Y}(y) } \\, dx \\right] f_Y(y) \\, dy\n= \\int_{B_Y} \\int_{B_X} f_{X,Y}(x,y) \\, dx \\, dy\n= \\PR(X \\in B_X, Y \\in B_Y)\n\\] which equals the LHS of \\(\\eqref{eq:defn-cond}\\). This is why the conditional density is defined the way it is defined!\nFinally, it can be shown that \\(\\PR(A \\mid Y) \\coloneqq \\EXP[\\IND_{A} \\mid σ(Y)]\\), \\(A \\in \\ALPHABET F\\), satisfies the axioms of probability.Therefore, conditional probability satisfies all the properties of probability (and consequently, conditional expectations satisfy all the properties of expectations).\nNote that the definition of conditional expectation generalizes Bayes rule. In particular, for any (measurable) function \\(g \\colon \\reals \\to \\reals\\) we have \\[\\begin{align*}\n\\EXP[ g(X) \\mid Y = y ] &= \\int_{-∞}^∞ g(x) f_{X|Y}(x\\mid y) \\, dx\n\\\\\n&= \\int_{-∞}^∞ g(x) \\frac{f_{X,Y}(x,y)}{f_Y(y)} \\, dx \\\\\n&= \\frac{ \\displaystyle \\int_{-∞}^{∞} g(x) f_{X,Y}(x,y)\\, dx}\n   { f_Y(y)} \\\\\n&= \\frac{ \\displaystyle \\int_{-∞}^{∞} g(x) f_{X,Y}(x,y)\\, dx}\n   {\\displaystyle \\int_{-∞}^{∞} f_{X,Y}(x,y)\\, dx } \\\\\n&= \\frac{ \\displaystyle \\int_{-∞}^{∞} g(x) f_{Y|X}(y|x) f_X(x) \\, dx}\n   {\\displaystyle \\int_{-∞}^{∞} f_{Y|X}(y|x) f_X(x) \\, dx }. \\\\\n  \\end{align*}\\]\n\n\nExample 4.8 Let \\(X\\) and \\(Y\\) be independent and identically distributed \\(\\text{Bernoulli}(p)\\) random variables.\n\nConsider the events \\(A_k = \\{ ω : X(ω) + Y(ω) = k\\}\\), \\(k \\in \\{0,1,2\\}\\). Find \\(\\PR(A_k \\mid X)\\).\nCompute \\(\\EXP[X + Y \\mid X]\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html#MMSE",
    "href": "conditional-expectation.html#MMSE",
    "title": "4  Conditional probability and conditional expectation",
    "section": "4.4 Conditional expectation as minimum mean square estimator",
    "text": "4.4 Conditional expectation as minimum mean square estimator\n\nSuppose we observe a random variable \\(Y\\) and wish to estimate another random variable \\(X\\). We seek a function \\(h(Y)\\) that minimizes the mean-squared error: \\[\n\\EXP[(X - h(Y))^2].\n\\] The function \\(h\\) that minimizes this quantity is called the minimum mean-squared error (MMSE) estimator of \\(X\\) given \\(Y\\).\nThe conditional expectation \\(\\EXP[X \\mid Y]\\) is the MMSE estimator of \\(X\\) given \\(Y\\)\n\n\n\n\n\n\nNoteProof\n\n\n\nRecall that in Example 2.23, we had shown that \\[\n   \\EXP[ (X-t)^2 ] = \\EXP[ (X - μ)^2] + (t - μ)^2.\n\\]\nBy the same argument, we have \\[\n   \\EXP[ (X - h(Y))^2 \\mid Y ] = \\EXP[ (X - \\EXP[X\\mid Y])^2 \\mid Y] + (h(Y) - \\EXP[X\\mid Y])^2\n\\] since \\(h(Y)\\) is \\(Y\\)-measurable.\nBy smoothing property of conditional expectation, we have \\[\\begin{equation}\\label{eq:MMSE-error}\n\\EXP[(X - h(Y))^2] = \\EXP[(X - \\EXP[X \\mid Y])^2] + \\EXP[(h(Y) - \\EXP[X \\mid Y])^2].\n\\end{equation}\\] which is minimized when \\(h(Y) = \\EXP[X\\mid Y]\\).\n\n\nThe error \\(X - \\EXP[X \\mid Y]\\) is orthogonal (i.e., uncorrelated) to all functions of \\(Y\\), i.e., for any (measurable) function \\(g(Y)\\), we have \\[ \\EXP[(X - \\EXP[X \\mid Y]) g(Y)] = 0. \\]\n\n\n\n\n\n\nNoteProof\n\n\n\nBy the smoothing property of conditional expectation, we have \\[\\begin{align*}\n\\EXP[(X - \\EXP[X \\mid Y]) g(Y)] &= \\EXP[\\EXP[(X - \\EXP[X \\mid Y]) g(Y) \\mid Y]] \\\\\n&= \\EXP[g(Y) \\EXP[X - \\EXP[X \\mid Y] \\mid Y]] \\\\\n&= \\EXP[g(Y) \\cdot 0] = 0\n\\end{align*}\\] where we used the fact that \\(\\EXP[X - \\EXP[X \\mid Y] \\mid Y] = 0\\).\n\n\nConversely, if a function \\(h(Y)\\) is such that the error \\(X - h(Y)\\) is orthogonal to all functions of \\(Y\\), i.e., \\(\\EXP[(X - h(Y)) g(Y)] = 0\\) for all functions \\(g(Y)\\), then \\(h(Y) = \\EXP[X \\mid Y]\\) (almost surely), and hence \\(h(Y)\\) is the MMSE estimator.\n\n\n\n\n\n\nNoteProof\n\n\n\nSuppose \\(\\EXP[(X - h(Y)) g(Y)] = 0\\) for all functions \\(g(Y)\\). In particular, taking \\(g(Y) = h(Y) - \\EXP[X \\mid Y]\\), we have: \\[\n\\EXP[(X - h(Y)) (h(Y) - \\EXP[X \\mid Y])] = 0.\n\\] Expanding this, we get \\[\\begin{align*}\n0 &= \\EXP[(X - h(Y)) (h(Y) - \\EXP[X \\mid Y])] \\\\\n  &= \\EXP{ (X - \\EXP[X \\mid Y] - (h(Y) - \\EXP[X \\mid Y])) (h(Y) - \\EXP[X \\mid Y]) } \\\\\n  &= \\EXP[(X - \\EXP[X \\mid Y]) (h(Y) - \\EXP[X \\mid Y])] - \\EXP[(h(Y) - \\EXP[X \\mid Y])^2]\n\\end{align*}\\] The first term is zero because \\(h(Y) - \\EXP[X \\mid Y]\\) is a function of \\(Y\\) and, by point 4, \\(X - \\EXP[X \\mid Y]\\) is orthogonal to all functions of \\(Y\\). Therefore, \\[\n\\EXP[(h(Y) - \\EXP[X \\mid Y])^2] = 0,\n\\] which implies \\(h(Y) = \\EXP[X \\mid Y]\\) almost surely. By point 2, this means \\(h(Y)\\) is the MMSE estimator.\n\n\nThe MMSE estimator is unbiased: \\(\\EXP[X - \\EXP[X \\mid Y]] = \\EXP[X] - \\EXP[\\EXP[X \\mid Y]] = 0\\) by the smoothing property.\nThe total variance of \\(X\\) can be decomposed as: \\[\n\\VAR(X) = \\EXP[(X - \\EXP[X])^2] = \\EXP[(X - \\EXP[X \\mid Y])^2] + \\EXP[(\\EXP[X \\mid Y] - \\EXP[X])^2].\n\\] This follows from point 3 by taking \\(h(Y) = \\EXP[X]\\) (a constant function) in \\(\\eqref{eq:MMSE-error}\\). The first term is the estimation error (the variance that cannot be reduced by observing \\(Y\\)), and the second term is the variance of the estimator itself.\nWhen \\(X\\) and \\(Y\\) are jointly Gaussian, the conditional expectation is an affine function of \\(Y\\). This is a special property of Gaussian distributions. We expanded on this in the next section.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation-gaussian.html",
    "href": "conditional-expectation-gaussian.html",
    "title": "5  Conditional expectation for Gaussians",
    "section": "",
    "text": "5.1 Review of Gaussian random variables",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional expectation for Gaussians</span>"
    ]
  },
  {
    "objectID": "conditional-expectation-gaussian.html#review-of-gaussian-random-variables",
    "href": "conditional-expectation-gaussian.html#review-of-gaussian-random-variables",
    "title": "5  Conditional expectation for Gaussians",
    "section": "",
    "text": "A multivariate Gaussian random vector \\(X = (X_1, \\dots, X_n) \\sim \\mathcal{N}(\\mu, \\Sigma)\\) has a PDF \\[\nf_X(x) = \\frac{1}{(2\\pi)^{n/2} \\det(\\Sigma)^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^\\TRANS \\Sigma^{-1} (x - \\mu) \\right), \\quad x \\in \\reals^n\n\\] and MGF \\[\nM_X(s) = \\EXP\\big[ e^{s^\\TRANS X} \\big] = \\exp\\left( s^\\TRANS \\mu + \\frac{1}{2} s^\\TRANS \\Sigma s \\right), \\qquad s \\in \\reals^n.\n\\]\nThus, all moments of a Gaussian random variable can be expressed in terms of its mean and variance. In particular, if \\(X \\sim \\mathcal{N}(0, \\sigma^2)\\), then all odd moments are zero; all even moments can be written in terms of the variance: \\[\n\\EXP[X^{2k}] =\n\\underbrace{(2k-1)\\cdot(2k-3)\\cdots3\\cdot1}_{\\eqqcolon (2k-1)!!}\\, \\sigma^{2k}\n\\]\nAny linear combination of jointly Gaussian random variables is also Gaussian.\nMore concretely, if \\(X = (X_1, \\dots, X_n) \\sim \\mathcal{N}(\\mu, \\Sigma)\\) and \\(A \\in \\reals^{m \\times n}\\) and \\(b \\in \\reals^m\\), then the affine transformation \\(A X + b\\) is also Gaussian: \\[\nA X + b \\sim \\mathcal{N}\\big(A\\mu + b,\\, A\\Sigma A^\\TRANS\\big).\n\\]\nAs a consequence, marginals of jointly Gassian random variables are also Gaussian.\nMore concretely, if \\(X = (X_1, \\dots, X_n) \\sim \\mathcal{N}(\\mu, \\Sigma)\\), then any subvector formed by selecting a subset of the indices is itself multivariate Gaussian, with the corresponding mean vector and covariance matrix.\nGaussian random variables are independent if and only if they are uncorrelated. In particular, if \\(X\\) and \\(Y\\) are (jointly) Gaussian random variables, then \\(X\\) and \\(Y\\) are independent if and only if \\(\\COV(X, Y) = 0\\).\nFor a jointly Gaussian random vector \\((X_1, \\dots, X_n)\\), the components are independent if and only if the covariance matrix is diagonal.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional expectation for Gaussians</span>"
    ]
  },
  {
    "objectID": "conditional-expectation-gaussian.html#conditional-expectation-for-gaussian-random-variables",
    "href": "conditional-expectation-gaussian.html#conditional-expectation-for-gaussian-random-variables",
    "title": "5  Conditional expectation for Gaussians",
    "section": "5.2 Conditional expectation for Gaussian random variables",
    "text": "5.2 Conditional expectation for Gaussian random variables\n\nIf \\(X\\) and \\(Y\\) are jointly Gaussian, then \\(X|Y\\) is also Gaussian. In particular, suppose\n\n\\(X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)\\)\n\n\\(Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y^2)\\)\n\n\\(\\COV(X, Y) = \\rho \\sigma_X \\sigma_Y\\), where \\(-1 \\leq \\rho \\leq 1\\)\n\nthen \\[\nX \\mid (Y = y) \\sim \\mathcal{N}(\\mu_{X|Y},\\, \\sigma^2_{X|Y})\n\\] where the conditional mean is \\[\n\\mu_{X|Y} = \\mu_X + \\rho \\frac{\\sigma_X}{\\sigma_Y}(y - \\mu_Y)\n\\] and the conditional variance \\[\n\\sigma^2_{X|Y} = \\sigma_X^2 (1 - \\rho^2)\n\\]\nIt is possible to derive the above result from the definition of conditional probability by using some tedious algebra. However, we present a different approach here that is based on interpreting conditional expectations as orthogonal projections.\nHilbert space of random variables: The key idea is to construct a Hilbert space of square-integrable random variables. For scalar random variables \\(X\\) and \\(Y\\) with \\(\\EXP[X^2] &lt; ∞\\) and \\(\\EXP[Y^2] &lt; ∞\\), we define the inner product as \\[\n     \\IP{X}{Y} = \\EXP[XY].\n\\] The corresponding norm is \\(\\NORM{X}^2 = \\EXP[X^2]\\). It can be shown that this space is complete (i.e., all Cauchy sequences converge), and is therefore a Hilbert space.\nFor \\(\\reals^n\\)-valued square-integrable random vectors \\(X_1\\) and \\(X_2\\) (i.e., \\(\\EXP[\\|X_1\\|^2] &lt; ∞\\) and \\(\\EXP[\\|X_2\\|^2] &lt; ∞\\)), we define the inner product as \\[\n     \\IP{X_1}{X_2} = \\EXP[X_1^\\TRANS X_2].\n\\] The corresponding norm is \\(\\NORM{X}^2 = \\EXP[\\NORM{X}^2]\\). As before, this space is complete and forms a Hilbert space.\nOrthogonal projection theorem: A fundamental property of Hilbert spaces is the orthogonal projection theorem.\n\nTheorem 5.1 (Orthogonal Projection Theorem) Let \\(\\ALPHABET M\\) be a proper linear subspace of a Hilbert space \\(\\ALPHABET H\\). Then any element \\(x \\in \\ALPHABET H\\) can be uniquely represented in the form \\[ x = \\hat x + e \\] where \\(\\hat x \\in \\ALPHABET M\\) and \\(e \\perp \\ALPHABET M\\) (i.e., for any \\(w \\in \\ALPHABET M\\), \\(\\IP{e}{w} = 0\\)). This unique element \\(\\hat x\\) is called the orthogonal projection of \\(x\\) onto \\(\\ALPHABET M\\) and has the property that it minimizes the distance: \\[\n    \\NORM{x - \\hat x} \\le \\NORM{x - w}, \\quad \\forall w \\in \\ALPHABET M.\n\\]\n\nWe also state a Lemma which will be useful later on.\n\nLemma 5.1 If \\(u \\in \\reals^n\\) and \\(w \\in \\reals^m\\) are vectors such that \\(u^\\TRANS K w = 0\\) for all matrices \\(K \\in \\reals^{n \\times m}\\), then \\(u w^\\TRANS = \\mathbf{0}_{n \\times m}\\).\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nNote that \\[u^\\TRANS K w = \\sum_{i=1}^n \\sum_{j=1}^m K_{ij} u_i w_j.\\]\nFor any \\((i', j')\\) with \\(1 \\le i' \\le n\\) and \\(1 \\le j' \\le m\\), choose \\(K\\) such that \\(K_{ij} = δ_{ii'} δ_{jj'}\\) (i.e., \\(K\\) has a \\(1\\) in position \\((i', j')\\) and zeros elsewhere). Then \\(u^\\TRANS K w = u_{i'} w_{j'} = 0\\).\nSince \\((i', j')\\) was arbitrary, we have \\(u_i w_j = 0\\) for all \\(i, j\\). Therefore, \\(u w^\\TRANS = \\mathbf{0}_{n \\times m}\\).\n\n\n\nApplication to Gaussian random vectors: Now consider \\(X \\in \\reals^n\\) and \\(Y \\in \\reals^m\\) to be jointly Gaussian random vectors, i.e., \\[\n     \\MATRIX{X \\\\ Y} \\sim\n     \\mathcal{N}\\left(\n         \\MATRIX{ μ_X \\\\ μ_Y},\n         \\MATRIX{ Σ_{XX} & Σ_{XY} \\\\ Σ_{XY}^\\TRANS & Σ_{YY}}\n         \\right).\n\\] Let \\(\\ALPHABET M\\) denote the proper linear subspace of all affine functions of \\(Y\\), i.e., \\[\n     \\ALPHABET M = \\{ K Y + c : K \\in \\reals^{n \\times m}, c \\in \\reals^n  \\}.\n\\] Let \\(\\hat{X}\\) denote the orthogonal projection of \\(X\\) onto \\(\\ALPHABET M\\). From Theorem 5.1, we know that \\(\\hat{X}\\) minimizes the distance, i.e., for any \\(W \\in \\ALPHABET M\\), \\[\n     \\NORM{X - \\hat{X}} \\le \\NORM{X - W}\n     \\iff\n     \\EXP[ \\|X - \\hat{X}\\|^2] \\le \\EXP[ \\|X - W\\|^2].\n\\] Thus, \\(\\hat{X}\\) minimizes the mean-squared error. Moreover, by the orthogonality property, the projection error is orthogonal to the subspace: \\[\n     \\IP{W}{X - \\hat{X}} = 0, \\quad \\forall W \\in \\ALPHABET M\n     \\iff\n     \\EXP[ W^\\TRANS (X - \\hat{X})] = 0, \\quad \\forall W \\in \\ALPHABET M.\n\\]\nAn immediate implication of the above is that \\(\\hat X\\) is an unbiased estimator of \\(X\\), i.e., \\[\n\\EXP[\\hat{X}] = \\EXP[X].\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\nSince \\(\\ALPHABET M\\) includes all affine functions of \\(Y\\), it includes \\(e_i\\), the unit vector in the \\(i\\)-th direction. Hence, \\[\n\\EXP[\\hat{X}] = \\EXP[X] + \\EXP[ \\IP{X - \\hat{X}}{e_i}] = \\EXP[X] + \\EXP[ \\IP{X - \\hat{X}}{e_i}] = \\EXP[X].\n\\]\n\n\nThe orthogonal projection is the conditional expectation, i.e., \\[\n\\hat{X} = \\EXP[X \\mid Y].\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\nSince \\(\\hat{X} \\in \\ALPHABET M\\), it is an affine function of \\(Y\\). Hence \\(X - \\hat X\\) and \\(Y\\) are jointly Gaussian. Moreover, since \\(K(Y - μ_Y) \\in \\ALPHABET M\\) for all \\(K \\in \\reals^{n \\times m}\\), we have \\[\n\\EXP[ (X - \\hat{X})^\\TRANS K(Y - μ_Y)] = 0, \\quad \\forall K \\in \\reals^{n \\times m}.\n\\] By Lemma 5.1, this implies that \\[\n\\EXP[(X - \\hat{X})(Y - μ_Y)^\\TRANS] = \\mathbf{0}_{n \\times m}.\n\\] Since \\(X - \\hat X\\) and \\(Y\\) are jointly Gaussian and uncorrelated (because \\(\\EXP[X - \\hat X] = 0\\)), they are independent. Therefore, for any (measurable) function \\(g\\), the independence gives: \\[\n\\EXP[\\IP{X - \\hat{X}}{g(Y)}] = \\EXP[X - \\hat{X}]^\\TRANS \\EXP[g(Y)] = 0,\n\\] where the last equality follows from the fact that \\(\\hat X\\) is an unbiased estimator of \\(X\\).\nNow, from last section, we know that if a function \\(h(Y)\\) is such that \\(X - h(Y)\\) is orthogonal to all functions of \\(Y\\), then \\(h(Y)\\) is the conditional expectation \\(\\EXP[X \\mid Y]\\). Hence, \\(\\hat X = \\EXP[X \\mid Y]\\).\n\n\nDeriving the formula: To find the explicit formula for \\(\\EXP[X \\mid Y]\\), we write \\(\\hat{X} = K_{\\circ} Y + c_\\circ\\) and find \\(K_\\circ\\) and \\(c_\\circ\\) from properties of orthogonal projection. First note that from the smoothing property of conditional expectation, we have \\[\n   \\EXP[ \\hat X] = \\EXP[ \\EXP[ X \\mid Y] ] = \\EXP[X]\n   \\implies\n   K_\\circ μ_Y + c_\\circ = μ_X\n   \\implies\n   c_\\circ = μ_X - K_\\circ μ_Y.\n\\]\nBy orthogonal projection theorem, the error \\(X - \\hat{X} = (X - μ_X) - K_{\\circ}(Y - μ_Y)\\) must be orthogonal to all elements of \\(\\ALPHABET M\\). In particular, it must be orthogonal to \\(Y - μ_Y\\) and to constant vectors.\nThe orthogonality condition with \\(K(Y - μ_Y)\\) gives: \\[\\begin{equation}\\label{eq:orthogonal}\n\\EXP[( (X - μ_X) - K_{\\circ}(Y - μ_Y) )^\\TRANS K(Y - μ_Y)] = 0,\n  \\quad \\forall K \\in \\reals^{n \\times m}.\n\\end{equation}\\] By Lemma 5.1, \\(\\eqref{eq:orthogonal}\\) is equivalent to \\[\\begin{equation}\\label{eq:claim}\n\\EXP[( (X - μ_X) - K_{\\circ}(Y - μ_Y) )(Y - μ_Y)^\\TRANS ] = \\mathbf{0}_{n \\times m}.\n\\end{equation}\\]\nNow this implies that \\[\n   Σ_{XY} = \\EXP[ (X - μ_X) (Y - μ_Y)^\\TRANS ]\n   = K_{\\circ} \\EXP[ (Y - μ_Y) (Y - μ_Y)^\\TRANS]\n   = K_{\\circ} Σ_{YY}\n\\] Hence, \\[\n   K_{\\circ} = Σ_{XY} Σ_{YY}^{-1}\n   \\quad\\text{and}\\quad\n   c_{\\circ} = μ_X - Σ_{XY} Σ_{YY}^{-1} μ_Y.\n\\]\nThus, \\[\n\\bbox[5pt,border: 1px solid]{\n    \\EXP[ X \\mid Y] = μ_X + Σ_{XY} Σ_{YY}^{-1}(Y - μ_Y)\n}\n\\]\nSince the conditional mean is an affine function of \\(Y\\), it is a Gaussian random variable.\nThe conditional covariance \\(\\COV(X \\mid Y)\\) is defined as \\[\n   \\COV(X \\mid Y) = \\EXP[ (X - \\EXP[X \\mid Y])(X - \\EXP[X \\mid Y])^\\TRANS \\mid Y]\n\\] Note that we have already shown that \\(X - \\hat X\\) is orthogonal to every random variable in \\(\\ALPHABET M\\). Thus, \\(X - \\hat X\\) is orthogonal to \\(Y\\). Hence, \\[\n   \\EXP[ (X - \\hat X)(X - \\hat X)^\\TRANS \\mid Y]\n   = \\EXP[ (X - \\hat X) (X - \\hat X)^\\TRANS].\n\\] To simplify the calculation, we assume that \\(X\\) and \\(Y\\) are zero mean. Then, \\(\\hat{X} = K_{\\circ}Y\\), and we have \\[\\begin{align*}\n\\COV(X \\mid Y) &= \\EXP[ (X - \\hat X) (X - \\hat X)^\\TRANS] \\\\\n&= \\EXP[ X X^\\TRANS + K_{\\circ} Y Y^\\TRANS K_{\\circ}^\\TRANS - 2 X Y^\\TRANS K_{\\circ}^\\TRANS] \\\\\n&= Σ_{XX} + K_{\\circ} Σ_{YY} K_{\\circ}^\\TRANS - 2 Σ_{XY} K_{\\circ}^\\TRANS \\\\\n&= Σ_{XX} - Σ_{XY} Σ_{YY}^{-1} Σ_{XY}^\\TRANS.\n\\end{align*}\\]\nSummary: For jointly Gaussian random vectors \\((X, Y)\\) with the joint distribution given above, the conditional distribution is Gaussian and is given by: \\[\n   \\bbox[5pt,border: 1px solid]{\n   X \\mid Y  \\sim \\mathcal{N}\\left(μ_X + Σ_{XY} Σ_{YY}^{-1} (Y - μ_Y), \\, Σ_{XX} - Σ_{XY} Σ_{YY}^{-1} Σ_{XY}^\\TRANS\\right)\n   }\n   \\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional expectation for Gaussians</span>"
    ]
  },
  {
    "objectID": "conditional-expectation-gaussian.html#exercises",
    "href": "conditional-expectation-gaussian.html#exercises",
    "title": "5  Conditional expectation for Gaussians",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 5.1 Let \\(X \\sim \\mathcal{N}(μ_X, Σ_X)\\) denote the state of nature. We make an observation \\[ Y = X + W \\] where \\(W \\sim \\mathcal{N}(0, Σ_W)\\). Find \\(\\EXP[X \\mid Y]\\).\n\n\nExercise 5.2 A robot is trying to estimate its position \\(X \\sim \\mathcal{N}(μ_X, Σ_X)\\) using two independent sensors. Sensor 1 provides measurement \\[ Y_1 = C_1 X + W_1 \\] where \\(W_1 \\sim \\mathcal{N}(0, Σ_{W_1})\\) is measurement noise and \\(C_1\\) is a known measurement matrix. Sensor 2 provides measurement \\[ Y_2 = C_2 X + W_2 \\] where \\(W_2 \\sim \\mathcal{N}(0, Σ_{W_2})\\) is independent of \\(W_1\\) and \\(C_2\\) is another known measurement matrix.\nFind \\(\\EXP[X \\mid Y_1, Y_2]\\) and show how the conditional covariance compares to using a single sensor.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional expectation for Gaussians</span>"
    ]
  },
  {
    "objectID": "conditional-expectation-gaussian.html#further-reading",
    "href": "conditional-expectation-gaussian.html#further-reading",
    "title": "5  Conditional expectation for Gaussians",
    "section": "Further reading",
    "text": "Further reading\n\nGubner, Chapter 8.5, 9",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional expectation for Gaussians</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html",
    "href": "moment-generating-functions.html",
    "title": "6  Moment Generating Functions",
    "section": "",
    "text": "6.1 Moment Generating Functions\nThe moment generating function (MGF) of a random variable \\(X\\) is defined as \\[\nM_X(s) = \\EXP[e^{sX}]\n\\] provided the expectation exists.\nThe MGF of common random variables is shown in Table 6.1.\nIf there exist a neighborhood around origin where \\(M_X(s)\\) is well-defined. Then, we can use the MGF to “generate the moments” of \\(X\\) as follows:",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html#moment-generating-functions",
    "href": "moment-generating-functions.html#moment-generating-functions",
    "title": "6  Moment Generating Functions",
    "section": "",
    "text": "When \\(X\\) is discrete, we have \\[ M_X(s) = \\sum_{x \\in \\ALPHABET X} e^{sx} p_X(x). \\]\nWhen \\(X\\) is continuous, we have \\[ M_X(s) = \\int_{-∞}^∞ e^{sx} f_X(x) \\, dx. \\]\n\n\n\n\n\n\n\nImportantRelationship to Laplace transforms\n\n\n\nAlthough most texts (including the textbook) restrict \\(s\\) to be real, my personal view is that one should really interpret \\(s\\) as a complex number. If we do so, then we have the following:\n\n\\(M_X(-s)\\) is the Laplace transform of the PDF.\n\\(M_X(-j ω)\\) is the Fourier transform of the PDF, which is called the characteristic function of \\(X\\).\n\nTherefore, we can recover the PDF by taking the inverse Laplace transform of MGF. Thus, specifying the MGF of a random variable is equivalent to specifying the PDF.\nHistorically, MGF is defined for \\(s \\in \\reals\\) and there are distributions (e.g., Cauchy) for which MGF does not exist for any \\(s \\neq 0\\). In avoid such situations, one uses the characteristic function because the characteristic function always exists. However, if we view the domain of MGF to be \\(\\mathbb{C}\\), then there is no need for a distinction between MGF and characteristic function.\n\n\n\nExample 6.1 Suppose \\(X\\) is a random variable which takes values \\(\\{0, 1, 2\\}\\) with probabilities \\(\\{\\frac 12, \\frac 13, \\frac 16\\}\\). Then, \\[\\begin{align*}\nM_X(s) &= \\EXP[e^{sX}] \\\\\n&= \\frac 12 e^{s 0} + \\frac 13 e^{s 1} + \\frac 16 e^{s 2} \\\\\n&= \\frac 12 + \\frac 13 e^{s} + \\frac 16 e^{2s}.\n\\end{align*}\\]\n\n\nExample 6.2 Find the MGF of a Poisson random variable with parameter \\(λ\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\[\\begin{align*}\nM_X(s) &= \\EXP[e^{sX}] = \\sum_{k=0}^{∞}e^{ks} \\frac{λ^k e^{-λ}}{k!} \\\\\n&= e^{-λ} \\sum_{k=0}^{∞} \\frac{(λe^s)^k}{k!} \\\\\n&= e^{-λ} e^{λ e^{s}}.\n\\end{align*}\\]\n\n\n\n\nExample 6.3 Find the MGF of an exponential random variable with parameter \\(λ\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\[\\begin{align*}\nM_X(s) &= \\EXP[e^{sX}] \\\\\n&= \\int_{0}^∞ e^{sx} λ e^{-λx} \\, dx \\\\\n&= λ \\int_{0}^∞ e^{(s-λ)x} \\, dx \\\\\n&= \\frac{λ}{λ-s}.\n\\end{align*}\\]\nNote that we could have looked up this result from the Laplace transform tables which show that \\[\ne^{at} \\xleftrightarrow{\\hskip 0.5em \\mathcal{L}\\hskip 0.5em } \\frac{1}{s-a}\n\\]\n\n\n\n\n\n\n\nTable 6.1: MGF of common random vables\n\n\n\n\n\n\n\n\n\n\nRandom variable\nParameter(s)\nMGF\n\n\n\n\nBernoulli\n\\(p\\)\n\\(1 - p + p e^s\\)\n\n\nBinomial\n\\((n,p)\\)\n\\((1-p + p e^s)^n\\)\n\n\nGeometric\n\\(p\\)\n\\(\\dfrac{p e^s}{1 - (1-p)e^s}\\)\n\n\nPoisson\n\\(λ\\)\n\\(\\exp(λ (e^s - 1))\\)\n\n\nUniform\n\\((a,b)\\)\n\\(\\dfrac{e^{sb} - e^{sa}}{s(b-a)}\\)\n\n\nExponential\n\\(λ\\)\n\\(\\dfrac{λ}{λ-s}\\)\n\n\nGaussian\n\\((μ,σ)\\)\n\\(\\exp\\bigl(μ s + \\frac 12 σ^2 s^2 \\bigr)\\)\n\n\n\n\n\n\n\n\n\\(M_X(0) = 1\\)\n\\(\\dfrac{d}{ds} M_X(s) \\biggr|_{s=0} = \\EXP[X]\\).\n\\(\\dfrac{d^2}{ds^2} M_X(s) \\biggr|_{s=0} = \\EXP[X^2]\\).\nand in general \\(\\dfrac{d^k}{ds^k} M_X(s) \\biggr|_{s=0} = \\EXP[X^k]\\).\nHence, by Taylor series expansion of \\(M_X(s)\\) within the radius of convergence, we get \\[\nM_X(s) = \\sum_{k=0}^∞ \\frac{\\EXP[X^k]}{k!} s^k.\n\\] Thus, when if the MGF is well defined in a neighborhood around origin, knowing all the moments of a distribution is sufficient to construct the MGF. We already saw that the MGF is sufficient to construct the PDF/PMF of the distribution. Thus, the distribution of a random variable is completely characterized by its moments.\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe first property follows from definition: \\[\nM_X(0) = \\EXP[e^{0 X}] = \\EXP[1] = 1.\n\\]\nFor the general derivative, we have \\[\\begin{align*}\n\\frac{d^k}{ds^k} M_X(s)\n&= \\int_{-∞}^∞ \\frac{d^k}{ds^k} e^{sx} f_X(x) \\, dx \\\\\n&= \\int_{-∞}^∞ x^k e^{sx} f_X(x) \\, dx.\n\\end{align*}\\]\nTherefore \\[\n\\frac{d^k}{ds^k} M_X(s) \\biggr|_{s=0} = \\int_{-∞}^∞ x^k f_X(x) \\, dx.\n\\]\n\n\n\n\nExample 6.4 Use the MGF of Bernoulli to find its first all the moments of \\(X\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFrom Table 6.1, we see that \\[ M_X(s) = 1 - p + p e^{s}. \\] Therefore,\n\n\\(\\dfrac{d}{ds} M_X(s) = p e^s\\).\n\\(\\dfrac{d^2}{ds^2} M_X(s) = p e^s\\).\nand in general \\(\\dfrac{d^k}{ds^k} M_X(s) = p e^s\\).\n\nThus,\n\n\\(\\EXP[X] = \\dfrac{d}{ds} M_X(s) \\biggr|_{s=0} = p\\).\n\\(\\EXP[X^2] = \\dfrac{d^2}{ds^2} M_X(s) \\biggr|_{s=0} = p\\).\nand in general \\(\\EXP[X^k] = \\dfrac{d^k}{ds^k} M_X(s) \\biggr|_{s=0} = p\\).\n\n\n\n\n\n6.1.1 Moment generating functions and sums of independent random variables\n\nTheorem 6.1 Suppose \\(X_1, X_2, \\dots, X_n\\) are independent random variables defined on the same probability space. Let \\[\n  Z = X_1 + X_2 + \\dots + X_n\n\\] Then, \\[\n  M_Z(s) = M_{X_1}(s) M_{X_2}(s) \\cdots M_{X_n}(s).\n\\]\nFurthermore, if the variables are identically distributed, then \\[\n  M_Z(s) = (M_{X_1}(s))^n.\n\\]\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof follows immediately from properties of independent random variables. We prove the result for \\(n=2\\). \\[\nM_Z(s) = \\EXP[e^{sX_1} e^{sX_2}]\n= \\EXP[e^{sX_1} e^{sX_2}] = M_{X_1}(s) M_{X_2}(s).\n\\]\n\n\n\nTheorem 6.1 is a very useful result. An immediate implication of the result is that following:\n\nSum of i.i.d. Bernoulli random variables is a Binomial random variable\nLet \\(X_i \\sim \\text{Ber}(p)\\). Then \\(M_{X_i}(s) = (1 - p + pe^s)\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = (1 - p + pe^s)^n\\).\nSum of independent Binomial random variables with the same \\(p\\) is a Binomial random variable.\nLet \\(X_i \\sim \\text{Binom}(m_i,p)\\). Then \\(M_{X_i}(s) = (1 - p + p e^s)^{m_i}\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = (1 - p + p e^s)^M\\), where \\(M = \\sum_{i=1}^n m_i\\).\nSum of independent Poisson random variables is Poisson.\nLet \\(X_i \\sim \\text{Pois}(λ_i)\\). Then \\(M_{X_i}(s) = e^{λ_i(e^s - 1)}\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = e^{λ(e^s - 1)}\\), where \\(λ = \\sum_{i=1}^n λ_i\\).\nSum of independent Gaussian random variables is Gaussian.\nLet \\(X_i \\sim \\mathcal N(μ_i, σ_i)\\). Then, \\(M_{X_i}(s) = \\exp( μ_i s + \\frac 12 σ_i^2 s^2)\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = \\exp(μ s + \\frac 12 σ^2 s^2)\\), where \\[\n   μ = \\sum_{i=1}^n μ_i\n   \\quad\\text{and}\\quad\n   σ^2 = \\sum_{i=1}^n σ_i^2.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html#the-central-limit-theorem",
    "href": "moment-generating-functions.html#the-central-limit-theorem",
    "title": "6  Moment Generating Functions",
    "section": "6.2 The Central Limit Theorem",
    "text": "6.2 The Central Limit Theorem\nOne of the ways in which MGFs are useful is that they allow us to understand the limiting behavior of sum of i.i.d. random variables.\n\n\n\n\n\n\nTipConvergence in distribution\n\n\n\nA sequence of random variables \\(\\{X_n\\}_{n \\ge 1}\\) is said to converge in distribution to a random variable \\(X\\) (denoted by \\(X_n \\xrightarrow{D} X\\)) if \\[\n\\lim_{n \\to \\infty} F_{X_n}(x) = F_X(x),\n\\] for all \\(x\\) where \\(F_X\\) is continuous.\n\n\n\nExample 6.5 Consider a sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables where \\(X_n \\sim \\mathcal{N}(0,1/n)\\). Show that \\(X_n \\xrightarrow{D} 0\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nLet \\(F\\) denote the CDF of the constant random variable \\(X=0\\), i.e., \\[\nF(x) = \\begin{cases}\n0, & x &lt; 0 \\\\\n1, & x \\ge 0\n\\end{cases}\n\\]\nLet \\(Z\\) denote the standard Gaussian random variable. Then \\[\nF_{X_n}(x) = \\PR(X_n \\le x) = \\PR(Z \\le \\sqrt{n} x)\n\\to \\begin{cases}\n1, & x &gt; 0 \\\\\n0, & x &lt; 0\n\\end{cases}\n\\] Thus, \\(F_{X_n}(x)\\) converges to \\(F(x)\\) for all \\(x \\neq 0\\). Recall that the definition of convergence in distribution, does not require convergence of \\(F_{X_n}(x)\\) at points of discontinuity of \\(F\\). So, \\(X_n \\xrightarrow{D} 0\\).\n\n\n\nAn important implication of convergence in distribution is that for any continuous bounded function \\(g\\) \\[\n\\EXP[g(X_n)] \\to \\EXP[g(X)].\n\\] For this reason, convergence in distribution is sometimes called weak convergence.\nThe relationship between PDFs and MGFs implies the following continuity theorem:\n\n\n\n\n\n\nImportantContinuity Theorem\n\n\n\nConsider a sequence of random variables \\(\\{X_n\\}_{n \\ge 1}\\). For ease of notation, we use \\((F_n, M_n)\\) to denote the CDF and MGF of \\(X_n\\).\n\nIf \\(F_n \\to F\\) for some CDF \\(F\\) with MGF \\(M\\), then \\(M_n(s) \\to M(s)\\) for all \\(s\\).\nConversely, if \\(M(s) = \\lim_{n\\to ∞} M_n(s)\\) exists and is continuous at \\(s = 0\\), then \\(M\\) is the MGF of some CDF \\(F\\) and \\(F_n \\to F\\).\n\n\n\nThe proof is a bit involved and technical but makes sense from a signal processing point of view: we know that if a sequence of signals \\(x_n \\to x\\), then \\(\\mathcal L(x_n) \\to \\mathcal L(x)\\).\n\n6.2.1 Two limit theorems\nWe now prove two limit theorems using the moment generating function.1\n1 Traditionally, these results are proved via the characteristic function and not the MGF. However, as discussed above, my personal view is that we can do everything with MGFs if we define them over complex numbers (as is done in Signals and Systems) rather than real numbers (as is done in probability theory).\nTheorem 6.2 (A law of large numbers) Let \\(\\{X_n\\}_{n \\ge 1}\\) be a sequence of i.i.d. random variables with finite mean \\(μ\\). Then, their partial sums \\(S_n = X_1 + \\cdots + X_n\\) satisfy \\[\n\\frac{1}{n} S_n \\xrightarrow{D} μ\n\\]\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe theorem asserts that as \\(n \\to ∞\\), \\[\n\\PR(n^{-1}S_n \\le x) =\n\\begin{cases}\n  0 & \\hbox{if } x &lt; μ, \\\\\n  1 & \\hbox{if } x \\ge μ.\n\\end{cases}\n\\]\nLet \\(M_X\\) denote the MGF of \\(X_n\\). From Theorem 6.1, we know that \\[\nM_{n^{-1} S_n}(s) = M_X(s/n)^n.\n\\]\nFrom the Taylor series expansion of \\(M_X(s)\\), we know that \\[\nM_X(s/n) = 1 + μ s + o(s).\n\\] Therefore, \\[M_{n^{-1} S_n}(s) = \\left(1 + \\frac{μs}{n} + o\\left(\\frac{s}{n}\\right) \\right)^n \\to \\exp(μ s)\n\\quad \\text{as} \\quad n \\to ∞,\n\\] which is the MGF of a constant random variable.\n\n\n\nWe will show later that convergence in distribution to a constant implies converge in probability. Therefore, the above implies the weak law of large numbers.\nThe above result shows that when \\(n\\) is large, the sum \\(S_n\\) is approximately the same as \\(n μ\\). The answer to this is provided by the Central Limit Theorem, which asserts that when \\(X_n\\) has finite variance \\(σ\\):\n\n\\(S_n - n μ\\) is about as big as \\(\\sqrt{n}\\).\nIrrespective of the distribution of \\(X_n\\), \\((S_n - n μ)/\\sqrt{n}\\) converges in distribution to a normal distribution with variance \\(σ\\).\n\nWe prove the result below.\n\nTheorem 6.3 (Central Limit Theorem) Let \\(\\{X_n\\}_{n \\ge 1}\\) be i.i.d. random variables with mean \\(μ\\) and finite non-zero variance \\(σ^2\\). Then, their partial sums \\(S_n = X_1 + \\cdots + X_n\\) satisfy \\[\n\\frac{S_n - n μ}{\\sqrt{n}\\, σ}\n\\xrightarrow{D}\n{\\cal N}(0,1).\n\\]\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof idea is similar to that of Theorem 6.2. Consider the “normalized” sequence of random variables \\(Y_n = (X_n - μ)/σ\\), which have mean zero and unit variance. Let \\(M_Y\\) denote their MGF. Then, the Taylor series expansion of \\(M_Y\\) gives \\[ M_Y(s) = 1 + \\frac{1}{2} s^2 + o(s^2). \\] Moreover, observe that \\[\nZ_n \\coloneqq \\frac{S_n - n μ }{\\sqrt{n}\\, σ}\n= \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n Y_i.\n\\] Therefore, by Theorem 6.1, we have \\[\nM_{Z_n}(s) = M_Y(s/\\sqrt{n})^n\n= \\left(1 + \\frac {s^2}{2n} + o\\left( \\frac {s^2}{n} \\right) \\right)^n\n\\to \\exp(-\\tfrac 12 s^2),\n\\] which is the MGF of \\({\\cal N}(0,1)\\).\n\n\n\nThe central limit theorem is one of the cornerstones of probability theory. The earliest statement of the result goes back to de Moivre (1773), but there was little follow up on that until Laplace Théorie analytique des probabilités, (1812). The term “central limit theorem” is due to Poyla (1920) who called is such because the limit theorem plays a “central role in probability theory”.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html#moment-generating-function-of-random-vectors",
    "href": "moment-generating-functions.html#moment-generating-function-of-random-vectors",
    "title": "6  Moment Generating Functions",
    "section": "6.3 Moment generating function of random vectors",
    "text": "6.3 Moment generating function of random vectors\nFor random vectors, \\(X \\in \\reals^n\\), the MGF is a function \\(M_X \\colon \\mathbb{C}^n \\to \\mathbb{C}\\) and for any \\(s \\in \\mathbb{C}^n\\) is given by\n\\[M_X(s) = \\EXP[e^{ \\langle s, X \\rangle }]. \\]\nTODO: Multivariate Gaussian. Relationship with correlation and covariance.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "inequalities.html",
    "href": "inequalities.html",
    "title": "7  Probability inequalities",
    "section": "",
    "text": "7.1 Union Bound\nWe already proved this when talking about probability spaces.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#union-bound",
    "href": "inequalities.html#union-bound",
    "title": "7  Probability inequalities",
    "section": "",
    "text": "TipUnion bound\n\n\n\nLet \\(A_1, \\dots, A_n\\) be a collection of events. Then, \\[\n\\PR\\biggl( \\bigcup_{i=1}^n A_i \\biggr)\n\\le\n\\sum_{i=1}^n \\PR(A_n).\n\\]\n\n\n\n\nExample 7.1 Let \\(X_1, \\dots, X_n\\) be i.i.d. random variables with CDF \\(F_X(x)\\). Find an upper bound on the CDF of \\[\nZ_n = \\min(X_1, \\dots, X_n).\n\\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe know that \\[Z_n = \\min(X_1, \\dots, X_n) \\le z\\] if and only if \\[ X_i \\le Z \\quad \\forall i \\in \\{1, \\dots, n\\}.\\] Therefore, \\[\\PR(Z_n \\le z) = \\PR\\biggl( \\bigcup_{i=1}^n \\{X_i \\le z \\} \\biggr)\n\\le \\sum_{i=1}^n \\PR(X_i \\le z) = n F_X(z).\n\\]\n\n\n\n\nExample 7.2 Let \\(\\{A_i\\}_{i = 1}^{∞}\\) be a countably infinite sequence of events. Show that\n\nIf \\(\\PR(A_i) = 0\\) for all \\(i \\in \\naturalnumbers\\), then \\[ \\PR\\Bigl( \\bigcup_{i=1}^{∞} A_i \\Bigr) = 0. \\]\nIf \\(\\PR(A_i) = 1\\) for all \\(i \\in \\naturalnumbers\\), then \\[ \\PR\\Bigl( \\bigcap_{i=1}^{∞} A_i \\Bigr) = 1. \\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#cauchy-schwartz-inequality",
    "href": "inequalities.html#cauchy-schwartz-inequality",
    "title": "7  Probability inequalities",
    "section": "7.2 Cauchy-Schwartz inequality",
    "text": "7.2 Cauchy-Schwartz inequality\nThis is a generalization of :Cauchy-Schwartz inequality on inner products to random variables.\n\n\n\n\n\n\nTipCauchy-Schwartz inequality\n\n\n\nLet \\(X\\) and \\(Y\\) be real-valued random variables. Then, \\[\n\\EXP[XY]^2 \\le \\EXP[X^2] \\EXP[Y^2].\n\\] or equivalently, \\[\n\\COV(X,Y)^2 \\le \\VAR(X) \\VAR(Y).\n\\]\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nDefine \\(f(s) = \\EXP[ (sX + Y)^2\\) for any \\(s \\in \\reals\\). Then, by linearity of expectation, we have \\[\nf(s) = \\EXP[ (sX + Y)^2 ] = s^2 \\EXP[X^2] + 2s \\EXP[XY] + \\EXP[Y^2].\n\\] We know that \\(f(s) \\ge 0\\). Recall that a quadratic form \\[\nA s^2 + B s + C \\ge 0\n\\] for all values of \\(s\\) if and only it has repeated or complex roots. Thus, the determinant must be non-positive, that is \\[\nΔ = B^2 - 4AC \\le 0.\n\\] The result follows by taking \\(A = \\EXP[X^2]\\), \\(B = 2 \\EXP[XY]\\), and \\(C = \\EXP[Y^2]\\).\n\n\n\n\nExample 7.3 Let \\(X \\ge 0\\) be a random variable with mean \\(μ\\) and finite variance \\(σ^2\\). Given \\(λ \\in (0,1)\\), define \\[A = \\{ ω : X(ω) \\ge λ μ \\}.\\]\n\nShow that \\[\\EXP[ X \\IND_{A} ] \\ge (1 -λ) μ. \\]\nHint: Note that \\(X = X (\\IND_A + \\IND_{A^c})\\). What do we know about \\(X\\) when \\(ω \\in A^c\\)?\nUsing the result of part (a), show that \\[\n  \\PR(X \\ge λ μ) \\ge (1-λ)^2 \\frac{ \\EXP[X]^2 }{\\EXP[X^2]}\n  = (1-λ)^2 \\frac{μ^2}{μ^2 + σ^2}.   \n\\]\nHint: Use Cauchy Schwartz to bound \\(\\EXP[X \\IND_A]\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#jensens-inequality",
    "href": "inequalities.html#jensens-inequality",
    "title": "7  Probability inequalities",
    "section": "7.3 Jensen’s inequality",
    "text": "7.3 Jensen’s inequality\nIf we take \\(Y = 1\\) in Cauchy-Schwartz inequality, we get that \\[\n\\EXP[X^2] \\ge \\EXP[X]^2.\n\\] This also follows from the fact that \\(\\VAR(X) \\ge 0\\). Jensen’s inequality may be thought as a generalization of this to convex functions.\n\n\n\n\n\n\nTipJensen’s inequality\n\n\n\nSuppose \\(X\\) is a real-valued random variable. Then for any convex \\(g \\colon \\reals \\to \\reals\\), we have \\[\n\\EXP[ g(X) ] \\ge g(\\EXP[X]).\n\\]\nMoreover, for any concave \\(g \\colon \\reals \\to \\reals\\), we have \\[\n\\EXP[ g(X) ] \\le g(\\EXP[X]).\n\\]\n\n\nFor example, since \\(g(x) = 1/x\\) is convex, we have \\[\n\\EXP\\left[\\frac 1x\\right] \\ge \\frac{1}{\\EXP[X]}.\n\\] Moreover, since \\(g(x) = \\log(x)\\) is concave, we have \\[\n\\EXP[ \\log(X)] \\le \\log(\\EXP[X]).\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Tangent at the mean of \\(X\\)\n\n\n\nLet \\(μ = \\EXP[X]\\). Consider the function \\(L\\) defined in Figure 7.1, which is the tangent at \\(μ\\) on \\(g(x)\\). By construction, \\(g(x) \\ge L(x)\\). Therefore, \\[\n  \\EXP[g(X)] \\ge \\EXP[L(X)]\n  = \\EXP[aX + b] = a μ + b = L(μ) = g(μ).\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#markov-inequality",
    "href": "inequalities.html#markov-inequality",
    "title": "7  Probability inequalities",
    "section": "7.4 Markov inequality",
    "text": "7.4 Markov inequality\n\n\n\n\n\n\nTipMarkov inequality\n\n\n\nFor any non-negative random variable \\(X\\) and a number \\(a &gt; 0\\), \\[\n\\PR(X \\ge a) \\le \\frac{\\EXP[X]}{a}.\n\\]\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nDefine \\(Z = \\IND\\{X \\ge a\\} a\\), which moves all the “mass” of \\(X\\) to the left.\n\nThus, \\[\n\\EXP[X] \\ge \\EXP[Z] = a \\PR(X \\ge a).\n\\]\n\n\n\n\nExample 7.4 Suppose \\(X \\sim \\text{Unif}(0,1)\\). Verify Markov inequality for \\(\\PR(X \\ge 0.2)\\), \\(\\PR(X \\ge 0.5)\\), and \\(\\PR(X \\ge 0.8)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe table below compares the actual tail probability with the bound obtained from Markov inequality.\n\n\n\n\\(a\\)\n\\(\\PR(X \\ge a)\\)\n\\(\\EXP[X]/a\\)\n\n\n\n\n\\(0.2\\)\n\\(0.8\\)\n\\(0.5/0.2 = 2.5\\)\n\n\n\\(0.5\\)\n\\(0.5\\)\n\\(0.5/0.5 = 1\\)\n\n\n\\(0.8\\)\n\\(0.2\\)\n\\(0.5/0.8 = .625\\)\n\n\n\n\n\n\nExample 7.4 shows that the Markov inequality is not tight. Moreover, it gives a vacuous bound for \\(a &lt; μ\\). Thus, it only makes sense to apply the Markov inequality when \\(a \\ge μ\\).\n\n7.4.1 Union bound as a special case of Markov inequality\nNote that it is possible to derive the union bound as a corollary of Markov inequality. Given events \\(A_1, \\dots, A_n\\), define the random variables \\(X_1, \\dots, X_n\\) as \\[\nX_i(ω) = \\IND_{A_i}(ω) = \\begin{cases}\n1, & ω \\in A_i \\\\\n0, & ω \\not\\in A_i\n\\end{cases}.\n\\] Let \\[ X = X_1 + \\dots + X_n. \\] \\(X_i\\) takes the value \\(1\\) when event \\(A_i\\) occurs. Therefore, the event \\(\\bigcup_{i=1}^n A_i\\) is equal to the event \\(X \\ge 1\\), i.e., \\[\n\\PR\\left(\\bigcup_{i=1}^n A_i \\right)\n= \\PR(X \\ge 1).\n\\] Now, by Markov inequality, we have \\[\n\\PR(X \\ge 1) \\le \\EXP[X] = \\PR(A_1) + \\cdots + \\PR(A_n).\n\\] The union bound follows by combining the above equations.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#chebyshev-inequality",
    "href": "inequalities.html#chebyshev-inequality",
    "title": "7  Probability inequalities",
    "section": "7.5 Chebyshev inequality",
    "text": "7.5 Chebyshev inequality\n\n\n\n\n\n\n\nTipChebyshev inequality\n\n\n\nLet \\(X\\) be a real-valued random variable with mean \\(μ\\). Then for any \\(a &gt; 0\\), we have \\[\n\\PR( \\ABS{X - μ} \\ge a) \\le \\frac{ \\VAR(X) }{a^2}.\n\\]\nAn alternative form of is as follows. Let \\(a = k σ\\). Then, \\[\n\\PR(\\ABS{X - μ} \\ge k σ) \\le \\frac{1}{k^2}.\n\\]\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nObserve that \\[\\begin{align*}\n\\PR( \\ABS{X - μ} \\ge a) &= \\PR((X - μ)^2 \\ge a^2)\n\\\\\n&\\stackrel{(a)}\\le \\frac{\\EXP[ (X-μ)^2 ]}{a^2} = \\frac{\\VAR(X)}{a^2}.\n\\end{align*}\\] where \\((a)\\) follows from Markov inequality.\n\n\n\nIn the following example, we compare the strength of Chebyshev inequality compared to that of Markov inequality.\n\nExample 7.5 Let \\(X \\sim \\text{Bin}(n,p)\\) and consider any \\(q \\in (p,1)\\). Use both Markov and Chebyshev inequalities to bound \\(\\PR(X \\ge nq)\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nRecall that \\(\\EXP[X] = np\\) and \\(\\VAR(X) = n p(1-p)\\). Therefore, from Markov inequality we have \\[\n\\PR(X \\ge nq) \\le \\frac{np}{nq} = \\frac{p}{q}.\n\\] Therefore, Markov inequality does not suggest any form of concentration with \\(n\\).\nWe now consider Chebyshev inequality. To do so, we first massage the expression a bit \\[\\begin{align*}\n\\PR(X \\ge nq) &= \\PR( X - np \\ge n(q-p) ) \\\\\n&\\le \\PR( \\ABS{X - np} \\ge n(q-p) )\n\\le \\frac{np(1-p)}{n^2(q-p)^2} = \\frac{1}{n} \\cdot \\frac{p(1-p)}{(q-p)^2}\n\\end{align*}\\] which shows that \\(\\PR(X \\ge nq)\\) gets smaller as \\(n\\) increases.\n\n\n\nWe can use Chebyshev inequality to prove the weak law of large numbers.\n\n\n\n\n\n\nTipWeak law of large numbers\n\n\n\nLet \\(X_1, X_2, \\dots\\) be independent random variables with \\(\\EXP[X_n] = μ\\) and \\(\\VAR(X_n) = σ^2\\). Let \\[\n\\bar X_n = \\frac 1n \\sum_{i=1}^n X_i\n\\] be the sample mean. Then, \\[\\PR(\\ABS{X_n - μ} &gt; ε) \\le \\frac{σ^2}{n ε^2}. \\]\nTherefore, \\(X_n \\xrightarrow{p} μ\\) (which reads as \\(X_n\\) converges in probability to \\(μ\\); we will study convergence in probability in next lecture).\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nObserve that \\[\\begin{align*}\n\\EXP[\\bar X_n] &= \\frac{1}{n} \\sum_{i=1}^n \\EXP[X_i] = μ \\\\\n\\VAR(\\bar X_n) &= \\frac{1}{n^2} \\sum_{i=1}^n \\VAR(X_i) = \\frac{σ^2}{n}.\n\\end{align*}\\]\nThen, by Chebyshev inequality, we have \\[\n\\PR(\\ABS{X_n - μ} &gt; ε) \\le \\frac{\\VAR(\\bar X_n)}{ε^2} = \\frac{σ^2}{n ε^2}.\n\\]\n\n\n\nIf we do not know \\(\\VAR(X)\\), we can still use Chebyshev inequality with an upper bound on \\(\\VAR(X)\\). For example, if \\(X \\in [a,b]\\), then we can show that \\[\n\\VAR(X) \\le \\frac{(b-a)^2}{4}.\n\\] So, for any random variable \\(X \\in [a,b]\\), we have that \\[\n\\PR(\\ABS{X - μ} &gt; ε) \\le \\frac{(b-a)^2}{4 ε^2}.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#chernoff-bound",
    "href": "inequalities.html#chernoff-bound",
    "title": "7  Probability inequalities",
    "section": "7.6 Chernoff bound",
    "text": "7.6 Chernoff bound\n\n\n\n\n\n\nTipChernoff Bound\n\n\n\nLet \\(X\\) be a real-valued random variable. Then, for any \\(a &gt; 0\\), we have \\[\n\\PR(X \\ge a) \\le e^{-φ(a)}\n\\] where \\[\nφ(a) = \\max_{s &gt; 0} \\{ s a - \\log M_X(s) \\}\n\\] is the Legendre-Fenchel transform of \\(\\log M_X(s)\\).\n\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof relies on two observations. First, for any \\(s &gt; 0\\), \\(e^s x\\) is an increasing function of \\(x\\). Therefore, \\[\n\\{ ω : X(ω) &gt; a \\} = \\{ ω : e^{sX(ω)} &gt; e^{sa} \\}.\n\\] Hence, \\[\n\\PR(X &gt; a) = \\PR(e^{sX} \\ge e^{sa})\n\\le \\frac{\\EXP[e^{sX}}{e^{sa}}\n= e^{-( sa - \\log M_X(s) ) }\n\\] where the inequality follows from Markov inequality\nSecond, observe that the above inequality holds for every \\(s &gt; 0\\). So, to get the tightest bound, we minimize the RHS over all \\(s &gt; 0\\), i.e., \\[\n\\PR(X &gt; a) = \\PR(e^{sX} \\ge e^{sa})\n\\le \\min_{s &gt; 0} e^{-( sa - \\log M_X(s) ) }\n\\] Since \\(e^x\\) is increasing in \\(x\\), the minimizer above is the same as the maximizer for \\[\nφ(a) = \\max_{s &gt; 0} \\{ sa - \\log M_X(s) \\}.\n\\]\n\n\n\nChernoff bound is much stronger than Markov and Chebyshev inequalities. To see that, let’s revisit the bound of Example 7.5. For a Binomial random variable, \\[\nM_X(s) = (1 - p + p e^s)^n.\n\\] Then, \\[\\PR(X \\ge nq) \\le \\exp\\left( - \\max_{s \\ge 0} \\Big[ sn q - n \\log(1-p + p e^s) \\Big] \\right)\n= \\exp\\left( - n \\max_{s \\ge 0} \\Big[ s q - \\log(1 - p + p^s) \\Big] \\right).\n\\] Even before we go ahead and compute the value in maximum of the term in square brackets, we observe that the result here is qualitatively different from that in Example 7.5. As for Chebyshev inequality, we get that the probability is going to zero, but Chernoff bound shows that the probability is going to zero exponentially fast. A bit of algebra shows that the exact bound is \\[\n\\PR(X \\ge nq) \\le \\exp\\bigl( - n D(p \\| q) \\bigr),\n\\] where \\[\nD(p \\| q) = q \\log \\left( \\frac{q}{p} \\right) + (1-q) \\log \\left( \\frac{1-q}{1 - p} \\right).\n\\]\nWe now present another example to show the tightness of the Chernoff bound.\n\nExample 7.6 Use the Chernoff bound to compute a tail bound on Gaussian random variable, i.e., for any \\(ε &gt; 0\\), bound \\(\\PR(X \\ge 0)\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#azuma-hoeffding-inequality",
    "href": "inequalities.html#azuma-hoeffding-inequality",
    "title": "7  Probability inequalities",
    "section": "7.7 Azuma-Hoeffding inequality",
    "text": "7.7 Azuma-Hoeffding inequality\nAlthough Chernoff bound is fairly tight, one of the drawbacks is that it requires the knowledge of the MGF of \\(X\\). This is in contrast to Markov and Chebyshev inequalities which only require knowledge of the mean and variance. For sums of i.i.d. random variables, it is possible to get a tight bound that only depends on the mean (and a proxy for variance).\n\n\n\n\n\n\nTipAzuma-Hoeffding inequality\n\n\n\nLet \\(X_1, \\dots, X_n\\) be i.i.d. random variables such that \\(X_i \\in [a,b]\\) and \\(\\EXP[X_i] = μ\\). Define \\[\n\\bar X_n = \\frac 1n \\sum_{i=1}^n X_i.\n\\] Then, \\[ \\PR( \\bar X_n - μ  &gt; ε ) \\le e ^ {- 2 ε^2 n } \\] and \\[ \\PR( \\ABS{\\bar X_n - μ}  &gt; ε ) \\le 2 e ^ {- 2 ε^2 n } \\]\n\n\nWe will not provide a proof of this inequality. The Azuma-Hoeffding inequality can also be interpreted as follows. For any \\(δ &gt; 0\\), we have that the true mean lies in the interval \\[\n\\left[ \\bar X_n - \\sqrt{ \\frac{1}{2n} \\log \\frac 2 {δ} },\n       \\bar X_n + \\sqrt{ \\frac{1}{2n} \\log \\frac 2 {δ} } \\right]\n\\]\nWe can revisit Example 7.5 using Hoeffding inequality. Recall that a Binomial random variable is the sum of i.i.d. Bernoulli random variables. Therefore, we have \\[\n\\PR( X - np \\ge n (q-p) ) = \\PR( \\bar X_n - p \\ge (q - p) )\n\\le e^{-2 (q - p)^2 n}.\n\\]\nThe bound is weaker than what we obtained via Chernoff bound, but it only requires the first moment and the fact that the random variables are bounded.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html",
    "href": "convergence-of-random-variables.html",
    "title": "8  Convergence of random variables",
    "section": "",
    "text": "8.1 Almost sure convergence\nSuppose we have an infinite sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\). Thus, for every \\(ω \\in Ω\\), there is an infinite sequence \\[\n  X_1(ω), X_2(ω), X_3(ω), \\dots\n\\] A sequence of random variables, also called a stochastic process, can be thought of a generalization of random vectors. When does this sequence converge?",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#almost-sure-convergence",
    "href": "convergence-of-random-variables.html#almost-sure-convergence",
    "title": "8  Convergence of random variables",
    "section": "",
    "text": "Recall that a sequence \\(\\{x_n\\}_{n \\ge 1}\\) of real numbers converges to a limit \\(x\\) if for every \\(ε &gt; 0\\), there exists a \\(N\\) such that for all \\(n \\ge N\\), we have that \\[ \\ABS{ x_n - x } &lt; ε. \\]\nThe simplest way to define convergence of a sequence of random variables as follows: a sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to a limit \\(X\\) surely if for every \\(ω \\in Ω\\), the sequence \\(\\{X_n(ω)\\}_{n \\ge 1}\\) of real numbers converges to \\(X(ω)\\).\nSure convergence can be too strong, as is illustrated by the following example.\n\n\nExample 8.1 Consider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) where \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr{B}(0,1)\\), and \\(\\PR\\) is the uniform distribution on \\(Ω\\). Define \\(X_n(ω) = \\IND_{A_n}(ω)\\) where \\(A_n = [0, \\frac 1n]\\), i.e., \\[\nX_n(ω) = \\begin{cases}\n1, & ω \\in [0, \\frac{1}{n}] \\\\\n0, & ω \\in (\\frac{1}{n}, 1]\n\\end{cases}\n\\]\n\nviewof ω_convergence_1 = Inputs.range([0.0, 1], {label: \"ω\", value:0.05, step: 0.01})\n\npoints_convergence_1 = {\n    const N = 120\n    var points = new Array(N)\n\n    for(var n=1; n &lt;= N; n++) {\n      var val = (ω_convergence_1*n &lt;= 1) ? 1 : 0 ;\n      points[n-1] = { n: n, X_n: val }\n    }\n    return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  y : {ticks: 1 },\n  height: 100,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.ruleX(points_convergence_1, {x: \"n\", y: \"X_n\", strokeWidth: 1}),\n    Plot.dot(points_convergence_1, {x: \"n\", y: \"X_n\", fill: \"currentColor\", r: 2}),\n  ]\n})\n\n\n\n\n\n\nShow that \\(\\{X_n\\}_{n \\ge 1}\\) does not converge in the sure sense.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFor any \\(ω \\in (0,1]\\), \\(X_n(ω) = 1\\) for \\(n \\le \\lfloor 1/ω \\rfloor\\), and \\(0\\) afterwards. Thus, \\(X_n(ω) \\to 0\\).\nHowever, for \\(ω = 0\\), \\(X_n(ω) = 1\\) for all \\(n\\). Thus, \\(X_n(ω) \\to 1\\).\n\n\n\n\nFrom a practical point of view, we do not care about not converging at \\(ω = 0\\), because that is an event of zero probability. Stated differently, the set \\[\n  \\{ ω : X_n(ω) \\to 0 \\}\n\\] has probability 1.\nBased on the previous discussion, we can relax the notion of sure convergence to almost sure convergence. A sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to a random variable \\(X\\) almost surely if \\[\n  \\PR\\left( \\left\\{ ω : \\lim_{n \\to ∞}\n   X_n(ω) = X(ω) \\right\\} \\right) = 1\n\\] Or, equivalently, for any \\(ε &gt; 0\\), \\[\n  \\PR\\left( \\limsup_{n \\to ∞} \\{ ω : | X_n(ω) - X(ω) | &gt; ε \\}  \\right) = 0\n\\] We denote such convergence as \\(X_n \\xrightarrow{a.s.} X\\).\nAnother way to generalize the definition of convergence of a sequence of real numbers to that of random variables is called convergence in probability. A sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to a random variable \\(X\\) in probability if \\[\n  \\lim_{n \\to ∞} \\PR( \\ABS{ X_n - X } &gt; ε ) = 0.\n\\] Or, equivalently, for any \\(ε &gt; 0\\) and \\(δ &gt; 0\\), there exists a \\(N\\) such that for all \\(n \\ge N\\), we have \\[\n  \\PR( \\ABS{ X_n - X} &gt; ε ) \\le δ.\n\\] We denote such convergence as \\(X_n \\xrightarrow{p} X\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#convergence-in-probability",
    "href": "convergence-of-random-variables.html#convergence-in-probability",
    "title": "8  Convergence of random variables",
    "section": "8.2 Convergence in probability",
    "text": "8.2 Convergence in probability\n\nAlmost sure convergence implies convergence in probability, i.e., \\[\n  [X_n \\xrightarrow{a.s.} X]\n  \\implies\n  [X_n \\xrightarrow{p} X]\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nFix \\(ε &gt; 0\\). Define \\[\nA_n = \\{ ω : \\exists m \\ge n, \\ABS{X_m(ω) - X} \\ge ε \\}.\n\\] Then, \\(\\{A_n\\}_{n \\ge 1}\\) is a decreasing sequence of events. If \\(ω \\in \\bigcap_{n \\ge 1} A_n\\), then \\(X_n(ω) \\not\\xrightarrow{a.s} X(ω)\\). This implies \\[\\PR\\Bigl( \\bigcap_{n \\ge 1} A_n \\Bigr) \\le\n  \\PR\\Bigl( \\Bigl\\{ ω : \\lim_{n \\to ∞}X_n(ω) \\neq X(ω) \\Bigr\\}\\Bigr)\n= 0. \\] By continuity of probability, \\[\n\\lim_{n \\to ∞} \\PR(A_n) = \\PR\\Bigl( \\lim_{n \\to ∞} A_n \\Bigr) = 0.\n\\]\n\n\n\nHowever, the converse is not true. Convergence in probability does not imply almost sure convergence, as is illustrated by the following example. An easier to understand example is presented in Example 8.5, part (b).\n\n\nExample 8.2 Consider a probability space as in Example 8.1. Define \\(X_n(ω) = \\IND_{A_n}(ω)\\), where \\(A_n = \\{ ω : n ω - \\lfloor n ω \\rfloor \\in [0, \\frac 1n] \\}\\).\n\nviewof ω_convergence_2 = Inputs.range([0.0, 1], {label: \"ω\", value:0.05, step: 0.01})\n\npoints_convergence_2 = {\n    const N = 120\n    var points = new Array(N)\n\n    for(var n=1; n &lt;= N; n++) {\n      var val = ((ω_convergence_2*n % 1)*n &lt;= 1) ? 1 : 0 ;\n      points[n-1] = { n: n, X_n: val }\n    }\n    return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  y : {ticks: 1 },\n  height: 100,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.ruleX(points_convergence_2, {x: \"n\", y: \"X_n\", strokeWidth: 1}),\n    Plot.dot(points_convergence_2, {x: \"n\", y: \"X_n\", fill: \"currentColor\", r: 2}),\n  ]\n})\n\n\n\n\n\n\nShow that \\(\\{X_n\\}_{n \\ge 1}\\) converges in probability but not almost surely.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nNote that \\(X_n(ω) \\in \\{0, 1\\}\\). Therefore, the sequence \\(\\{X_n(ω)\\}_{n \\ge 1}\\) of real numbers converges if and only if there exists an \\(N(ω)\\) such that \\(X_n(ω)\\) takes a constant value for all \\(n \\ge N(ω)\\). However, as can be seen from the above figure, for any value of \\(ω \\neq 0\\), \\(X_n(ω)\\) takes both values \\(0\\) and \\(1\\) infinitely often. Therefore, \\(X_n(ω)\\) does not converge to a limit.\nFor \\(ω = 0\\), \\(X_n(ω) = 1\\), and therefore \\(X_n(ω) \\to 1\\). However, \\(\\PR(\\{ω = 0 \\}) = 0\\). Hence, the sequence \\(\\{X_n(ω)\\}_{n \\ge 1}\\) does not converge almost surely.\nNow fix an \\(ε &gt; 0\\) and consider the set \\[\n  E_n = \\{ ω : |X_n(ω) - 0| &gt; ε \\}.\n\\] If \\(ε &gt; 1\\), then \\(E_n = \\emptyset\\), and \\(\\PR(E_n) = 0\\) for all \\(n\\). So, we assume that \\(ε &lt; 1\\). Then, \\[\n  E_n = \\{ ω : X_n(ω) = 1 \\}.\n\\] Note that \\(X_n(ω) = 1\\) for roughly \\(1/n\\) fraction of points (One can make this argument formal by showing the sequence \\(X_n(ω)\\) is equidistributed sequence modulo \\(1\\) by using Weyl’s criterion). So, \\[\n  \\PR(E_n) = \\PR(X_n = 1) = \\frac 1n.\n\\] Thus, \\[\n  \\lim_{n \\to ∞} \\PR(E_n) = 0.\n\\] Hence, \\(\\{X_n\\}\\) converges in probability.\n\n\n\n\nThere is however a partial converse. If \\(\\{X_n\\}_{n \\ge 1}\\) is a monotone sequence of random variables (i.e., either \\(X_1 \\le X_2 \\le \\cdots\\) almost surely or \\(X_1 \\ge X_2 \\ge \\cdots\\) almost surely) and \\(c\\) is a constant, then \\[\n  [X_n \\xrightarrow{p} c]\n  \\implies\n  [X_n \\xrightarrow{a.s.} c].\n  \\]\nIf \\(X_n \\xrightarrow{p} X\\), then there exists a subsequence \\(\\{n_k : k \\in \\naturalnumbers \\}\\) such that \\(\\{X_{n_k}\\}_{k \\ge 1}\\) converges almost surely to \\(X\\).\n\\(X_n \\xrightarrow{p} X\\) if and only if every subsequence \\(\\{n_k : k \\in \\naturalnumbers \\}\\) has a sub-subsequence \\(\\{n_{k_m} : m \\in \\naturalnumbers \\}\\) such that \\(\\{X_{n_{k_m}} \\}_{m \\ge 1}\\) that converges to \\(X\\) almost surely.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#convergence-in-distribution",
    "href": "convergence-of-random-variables.html#convergence-in-distribution",
    "title": "8  Convergence of random variables",
    "section": "8.3 Convergence in Distribution",
    "text": "8.3 Convergence in Distribution\n\nWe have already seen the notion of convergence in distribution (also called weak-convergence). A sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to a random variable \\(X\\) in distribution if \\[\n    \\lim_{n \\to ∞} \\PR(X_n \\le x) = \\PR(X \\le x)\n\\] for every \\(x\\) where \\(F(x) = \\PR(X \\le x)\\) is continuous.\nNote that the limiting object \\(\\lim_{n \\to ∞} \\PR(X_n \\le x)\\) need not be a valid CDF. For example, consider the deterministic sequence, \\(X_n = n\\). The corresponding CDF is \\[\n    F_{X_n}(x) = \\begin{cases}\n      0 & x &lt; n \\\\\n      1 & x \\ge n\n    \\end{cases}\n\\] Thus, for any \\(x\\), we have \\(\\lim_{n \\to ∞} F_{X_n}(x) = 0\\) which is not a valid CDF.\nConvergence in probability implies convergence in distribution. \\[\n  [X_n \\xrightarrow{p} X]\n  \\implies\n  [X_n \\xrightarrow{D} X]\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nLet \\(F_n\\) and \\(F\\) denote the CDFs of \\(X_n\\) and \\(X\\), respectively. Fix \\(ε &gt; 0\\), pick \\(x\\) such that \\(F\\) is continuous at \\(x\\), and consider \\[\\begin{align*}\n  F_n(x) &= \\PR(X_n \\le x) = \\PR(X_n \\le x, X \\le x + ε) + \\PR(X_n \\le x, X &gt; x + ε)\n\\\\\n&\\le \\PR(X \\le x + ε) + \\PR(X - X_n &gt; ε)\n\\\\\n&\\le F(x + ε) + \\PR(\\ABS{X_n - X} &gt; ε).\n\\end{align*}\\] Similarly, \\[\\begin{align*}\n  F(x-ε) &= \\PR(X \\le x-ε) = \\PR(X \\le x-ε, X_n \\le x ) + \\PR(X \\le x - ε, X_n &gt; x )\n\\\\\n&\\le \\PR(X_n \\le x) + \\PR(X_n - X &gt; ε)\n\\\\\n&\\le F_n(x) + \\PR(\\ABS{X_n - X} &gt; ε).\n\\end{align*}\\]\nThus, \\[\nF(x-ε) - \\PR(\\ABS{X_n - X} &gt; ε) \\le F_n(x) \\le F(x+ε) + \\PR(\\ABS{X_n - X} &gt; ε).\n\\] Taking \\(n \\to ∞\\), we have \\[\nF(x-ε) \\le \\liminf_{n \\to ∞} F_n(x) \\le \\limsup_{n \\to ∞} F_n(x) \\le F(x + ε).\n\\] The result is true for all \\(ε &gt; 0\\). Since \\(F\\) is continuous at \\(x\\), when we take \\(ε \\downarrow 0\\), we have \\[\nF(x - ε) \\uparrow F(x)\n\\quad\\text{and}\\quad\nF(x + ε) \\downarrow F(x)\n\\] which implies that \\(F_n(x) \\to F(x)\\).\n\n\n\nThe converse is not true. For example, let \\(X_n\\) be a sequence of i.i.d. \\(\\text{Uniform}(0,1)\\) random variables. Clearly, \\(X_n \\xrightarrow{D} X\\), where \\(X\\) is also \\(\\text{Uniform}(0,1)\\). However, for any \\(ε \\in (0,1)\\), \\[\n   \\PR( \\ABS{X_n - X} &gt; ε ) = (1-ε)^2\n\\]\n[Think of the area of the unit square corresponding to the event \\(|X_n - X| &gt; ε\\)]. Thus, \\[\n   \\lim_{n \\to ∞} \\PR(\\ABS{X_n - X} &gt; ε) \\neq 0.\n\\] So, \\(X_n\\) does not converge in probability to \\(X\\).\nThere is a partial converse. If \\(c\\) is a constant then \\[\n   [X_n \\xrightarrow{D} c]\n   \\implies\n   [X_n \\xrightarrow{p} c].\n\\]\nConvergence in distribution does not imply convergence of moments! For example, consider the sequence of random variables where \\[\n     X_n = \\begin{cases}\n       0 & \\text{w.p. } 1 - \\frac 1n \\\\\n       n & \\text{w.p. } \\frac 1n\n     \\end{cases}\n\\] Then, \\[\n   F_{X_n}(x) = \\begin{cases}\n     0, & x &lt; 0\n     1 - \\frac 1n, & 0 \\le x &lt; n \\\\\n     1 & x \\ge n\n   \\end{cases}\n\\] Thus, we have \\[\n   \\lim_{n \\to ∞} F_{X_n}(x) =\n   \\begin{cases}\n     0 & x &lt; 0 \\\\\n     1 & x \\ge 0\n   \\end{cases}\n\\] which corresponds to a random variable \\(X = 0\\).\nHowever, for any \\(k \\ge 1\\), we have \\(\\EXP[X_n^k] = n^{k-1}\\) but \\(\\EXP[X^k] = 0\\). Thus, the moments do not converge!\nSkorokhod’s representation theorem. If \\(X_n \\xrightarrow{D} X\\), then there exists a sequence \\(\\{Y_n\\}_{n \\ge 1}\\) that is identically distributed to \\(\\{X_n\\}_{n \\ge 1}\\) such that \\(Y_n \\xrightarrow{a.s.} Y\\), where \\(Y\\) is identically distributed to \\(X\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#convergence-in-r-th-mean",
    "href": "convergence-of-random-variables.html#convergence-in-r-th-mean",
    "title": "8  Convergence of random variables",
    "section": "8.4 Convergence in \\(r\\)-th mean",
    "text": "8.4 Convergence in \\(r\\)-th mean\n\nThere is another form of convergence: convergence in the \\(r\\)-th mean. We say that a sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to \\(X\\) in \\(r\\)-th mean, \\(r \\ge 1\\), if \\(\\EXP[\\ABS{X_n}^r] &lt; ∞\\) for all \\(n\\), \\(\\EXP[\\ABS{X}^r] &lt; ∞\\), and \\[\n   \\lim_{n \\to ∞} \\EXP[ \\ABS{X_n - X}^r ] = 0.\n\\]\nWhen \\(r = 1\\), we say that \\(\\{X_n\\}_{n \\ge 1}\\) converges in the mean. When \\(r=2\\), we say that \\(\\{X_n\\}_{n \\ge 1}\\) converges in the mean-square. We will mainly focus on mean-square convergence and denote it by \\(X_n \\xrightarrow{m.s.} X\\).\nIf \\(\\{X_n\\}_{n \\ge 1}\\) converges to \\(X\\) in the \\(r\\)-th mean, then it also converges in the \\(s\\)-th mean, for all \\(s &lt; r\\).\nConvergence in \\(r\\)-th mean implies convergence in probability.\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nBy Markov inequality, we have (for any \\(r \\ge 1\\)) \\[\n  \\PR(\\ABS{X_n - X} &gt; ε) = \\PR(\\ABS{X_n - X}^r &gt; ε^r)\n  \\le \\frac{ \\EXP[ \\ABS{X_n - X}^r ] }{ε^r}.\n\\] So, if the RHS goes to zero as \\(n \\to ∞\\), then so does the LHS.\n\n\n\nHowever, the converse is not true as is illustrated by Example 8.3, part (c).\nConvergence in mean-square and almost surely are not comparable. Example 8.3, part (c) gives an example that converges almost surely but not in mean square. Example 8.5, part (a) gives an example that converges in mean square but not almost surely.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#continuity-properties-of-convergence",
    "href": "convergence-of-random-variables.html#continuity-properties-of-convergence",
    "title": "8  Convergence of random variables",
    "section": "8.5 Continuity properties of convergence",
    "text": "8.5 Continuity properties of convergence\n\nContinuity preserves convergence almost surely, in probability, and in distribution. Thus, if \\(X_n\\) converges to \\(X\\) in any of these modes and \\(g\\) is a continuous function, then \\(g(X_n)\\) converges in the same mode to \\(g(X)\\).\nThis is not the case for mean square convergence. For example, \\(X_n = 0\\) with probability \\(1 - \\frac 1{n^3}\\) and \\(X_n = 1\\) with probability \\(\\frac 1{n^3}\\). Then, \\[ \\EXP[|X_n - 0|^2] = n^2 \\frac{1}{n^3} \\to 0. \\] Thus, \\(X_n \\xrightarrow{m.s.} 0\\). Now consider the function \\(g(x) = x^2\\). \\[ \\EXP[ | g(X_n) - g(0)|^2 ] = n^4 \\frac{1}{n^3} = n \\to ∞. \\] Thus, \\(g(X_n)\\) does not converge in mean square to \\(g(0)\\).\nIf \\(X_n \\xrightarrow{a.s.} X\\) and \\(Y_n \\xrightarrow{a.s.} Y\\), then \\((X_n, Y_n) \\xrightarrow{a.s.} (X,Y)\\). Therefore, for any continuous function \\(g\\), we have \\(g(X_n, Y_n) \\xrightarrow{a.s.} g(X,Y)\\).\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nDefine \\[\\begin{align*}\n    Ω_X &= \\{ ω : \\lim_{n \\to ∞} X_n(ω) = X(ω) \\} \\\\\n    Ω_Y &= \\{ ω : \\lim_{n \\to ∞} Y_n(ω) = Y(ω) \\}\n  \\end{align*}\\] We know that \\(\\PR(Ω_X) = 1\\) and \\(\\PR(Ω_Y) = 1\\). From Example 7.2, we have that \\(\\PR(Ω_X \\cap Ω_Y) = 1\\). Thus, \\((X_n, Y_n) \\xrightarrow{a.s} (X,Y)\\).\nThe second part follows from continuity property of convergence.\n\n\n\nThe above property is also true for convergence in probability. The proof is left as a homework exercise.\nHowever, this property is not true for convergence in distribution as is illustrated by the following example. Suppose \\(X_n \\sim \\text{Unif}(-1,1)\\) and \\(X \\sim \\text{Unif}(-1,1)\\) be independent random variables. Define \\(Y_n = X_n\\) and \\(Y = - X\\). Then, \\(X_n \\xrightarrow{D} X\\), \\(Y_n \\xrightarrow{D} Y\\), but \\(X_n + Y_n = 2X_n\\) does not converge in distribution to \\(X + Y = 0\\).\nSlutsky’s theorem: Suppose \\(X_n \\xrightarrow{D} X\\) and \\(Y_n \\xrightarrow{D} c\\), where \\(c\\) is a constant. Then, \\((X_n, Y_n) \\xrightarrow{D} (X, c)\\). Therefore, for any continuous function \\(g(X_n, Y_n) \\xrightarrow{D} g(X, c)\\). In particular, \\(X_n + Y_n \\xrightarrow{D} X + c\\) and \\(X_n Y_n \\xrightarrow{D} cX\\).\nNote that we have already argued that \\(Y_n \\xrightarrow{D} c\\) is equivalent to \\(Y_n \\xrightarrow{p} c\\). So, one can replace \\(Y_n \\xrightarrow{D} c\\) in Slutsky’s theorem by \\(Y_n \\xrightarrow{p} c\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#some-examples",
    "href": "convergence-of-random-variables.html#some-examples",
    "title": "8  Convergence of random variables",
    "section": "8.6 Some examples",
    "text": "8.6 Some examples\n\nExample 8.3 Consider the probability space \\((Ω, \\ALPHABET F, \\PR)\\) as in Example 8.1, and consider different choices of \\(X_n\\) shown below. For each case, determine whether \\(X_n\\) converges almost surely, in probability, or in mean-square.\n\n\\(X_n(ω) = \\IND_{A_n}(ω)\\), where \\(A_n = [0, \\frac 1n]\\).\n\\(X_n(ω) = \\IND_{A_n}(ω)\\), where \\(A_n = [0, \\frac 1{n^2}]\\).\n\\(X_n(ω) = \\textcolor{red}{n}\\IND_{A_n}(ω)\\), where \\(A_n = [0, \\frac 1n]\\).\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe only prove part (a). The proof of the other two parts is similar.\nFor \\(ω \\neq 0\\), the sequence \\(\\{X_n(ω)\\}_{n \\ge 1}\\) is a finite sequence of ones followed by an infinite sequence of zeros. Then, \\(\\lim_{n \\to ∞} X_n(ω) = 0\\). Thus, \\[\n  \\PR\\Bigl( \\Bigl\\{ ω : \\lim_{n \\to ∞} X_n(ω) = 0 \\Bigr\\} \\Bigr)\n  = \\PR( ω \\in (0,1]) = 1.\n\\] Hence, \\(X_n \\xrightarrow{a.s} 0\\).\nFix an \\(ε &gt; 0\\) and consider the set \\[\n  E_n = \\{ ω : |X_n(ω) - 0| &gt; ε \\}.\n\\] If \\(ε &gt; 1\\), then \\(E_n = \\emptyset\\), and \\(\\PR(E_n) = 0\\) for all \\(n\\). So, we assume that \\(ε &lt; 1\\). Then, \\[\n  E_n = \\{ ω : X_n(ω) = 1 \\}.\n\\] Hence, \\[\n  \\PR(E_n) = \\PR(X_n = 1) = \\frac 1n\n\\] which converges to \\(0\\) as \\(n \\to ∞\\). Therefore, \\(X_n \\xrightarrow{p} 0\\).\nConsider \\[\n  \\EXP[\\ABS{X_n}^2] = \\int_{A_n} 1\\, d ω = \\frac{1}{n}\n\\] which converges to \\(0\\) as \\(n \\to ∞\\). Thus, \\(X_n \\xrightarrow{m.s.} 0\\).\nHowever, observe that for part (c) \\[\n  \\EXP[\\ABS{X_n}^2] = \\int_{A_n} n^2\\, d ω = n\n\\] which goes to \\(∞\\) as \\(n \\to ∞\\). Thus, \\(\\{X_n\\}_{n \\ge 1}\\) does not converge in mean square.\n\n\n\n\nExample 8.4 Consider an i.i.d. sequence \\(\\{X_n\\}_{n \\ge 1}\\), where \\(X_n \\sim \\text{Uniform}(0,1)\\). Define \\[\n  Y_n = \\min\\{X_1, \\dots, X_n\\}.\n\\] Determine whether \\(Y_n\\) converges almost surely, in probability, or in mean-square.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nConvergence in probability\nFix an \\(ε &gt; 0\\) and consider the set \\[\n  E_n = \\{ ω : |Y_n(ω) - 0| &gt; ε \\}.\n\\] As in Example 8.3, if \\(ε &gt; 1\\), then \\(\\PR(E_n) = 0\\). So, we assume that \\(ε &lt; 1\\). Then, \\[\\begin{align*}\n\\PR(E_n) &= \\PR(Y_n &gt; ε) = \\PR\\bigl( \\min\\{ X_1, \\dots, X_n \\} \\ge ε ) \\\\\n&= \\PR( X_1 \\ge ε, X_2 \\ge ε, \\dots, X_n \\ge ε ) \\\\\n&= (1-ε)^n\n\\end{align*}\\] which goes to zero as \\(n \\to ∞\\). Thus, \\(Y_n \\xrightarrow{p} 0\\).\n\n\nAlmost sure convergence\nWe now consider almost sure convergence. Note that for a fixed \\(ω\\), the sequence \\(\\{Y_n(ω)\\}_{n \\ge 1}\\) is a decreasing sequence. Hence, it must have a limit. Denote that limit by \\(Y(ω)\\), i.e., \\[\n  Y(ω) = \\lim_{n \\to ∞} Y_n(ω).\n\\]\nSince \\(\\{Y_n\\}_{n \\ge 1}\\) is a decreasing sequence, we have that \\(Y(ω) \\le Y_n(ω)\\). Hence, for any \\(ε &gt; 0\\), \\[\n  \\PR(Y &gt; ε) \\le \\PR(Y_n &gt; ε) = (1 - ε)^n\n\\] where the last inequality follows from the calculations done above.\nThe above inequality holds for every \\(n\\), so we have \\[\n  \\PR(Y &gt; ε) \\le \\lim_{n \\to ∞} (1-ε)^n = 0.\n\\] Recall that \\(ε &gt; 0\\) was arbitrary. Therefore, we have shown that \\[\n  \\PR\\Bigl( \\lim_{n\\to ∞} Y_n &gt; ε \\Bigr) = 0.\n\\] Thus, the only possibility is that \\[\n  \\PR\\Bigl( \\lim_{n\\to ∞} Y_n = 0 \\Bigr) = 1.\n\\] Hence \\(Y_n \\xrightarrow{a.s.} 0\\).\n\n\nMean square convergence\nLet \\(F_n\\) denote the CDF of \\(Y_n\\) and \\(f_n\\) denote the PDF. As computed above, \\[\n  F_n(y) = 1 - (1-y)^n, \\quad y \\in (0,1).\n\\] Therefore, \\[\n  f_n(y) = \\frac{d F_n(y)}{dy} = n(1-y)^{n-1}, \\quad y \\in (0,1).\n\\] Since \\(Y_n \\in [0, 1]\\), we have \\[\n  \\EXP[ \\ABS{Y_n - 0}^2 ] = \\EXP[ Y_n^2 ] \\le \\EXP[ Y_n]\n\\] where the last inequality uses the fact that \\(y^2 \\le y\\) for \\(y \\in [0,1]\\). We now compute \\(\\EXP[Y_n]\\). \\[\\begin{align*}\n  \\EXP[Y_n] &= \\int_{0}^{1} y f_n(y)\\, dy \\\\\n  &= \\int_{0}^1 y n (1-y)^{n-1}\\, dy \\\\\n  &= n \\int_{0}^1 y (1-y)^{n-1}\\, dy.\n\\end{align*}\\] We could compute the integral using integration by parts, but here we simply use the fact that the above is a :Beta integral. Thus, \\[\n  \\EXP[Y_n] = n \\int_{0}^1 y (1-y)^{n-1}\\, dy = n B(2,n)\n  = n\\frac{1!(n-1)!}{(n+1)!}\n  = \\frac{1}{n+1}.\n\\] Thus, \\[\n  \\lim_{n \\to ∞} \\EXP[Y_n^2 ]\n  \\le\n  \\lim_{n \\to ∞} \\EXP[Y_n ]\n  = 0.\n\\] Hence, \\(Y_n \\xrightarrow{m.s.} 0\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#almost-sure-convergence-from-convergence-in-probability.",
    "href": "convergence-of-random-variables.html#almost-sure-convergence-from-convergence-in-probability.",
    "title": "8  Convergence of random variables",
    "section": "8.7 Almost sure convergence from convergence in probability.",
    "text": "8.7 Almost sure convergence from convergence in probability.\nExample 8.4 shows that verifying almost sure convergence can be a bit tricky. In this section, we show that sometimes it is possible to infer almost sure convergence from convergence in probability.\n\n\n\n\n\n\nWarningLim inf and lim sup of sets\n\n\n\n\n\nExplanation adapted from math.stackexchange [1] and [2]\nLimits of sets is easy to describe when we have weakly increasing or weakly decreasing sequence of sets. In particular, if \\(\\{C_n\\}_{n \\ge 1}\\) is a weakly increasing sequence of sets, then \\[\n\\lim_{n \\to ∞} C_n = \\bigcup_{n=1}^{∞} C_n.\n\\] Thus, the limit is the union of all sets. Moreover, if \\(\\{D_n\\}_{n \\ge 1}\\) is weakly decreasing sequence of sets, then \\[\n\\lim_{n \\to ∞} D_n = \\bigcap_{n=1}^{∞} D_n.\n\\] Thus, the limit is the intersection of all sets.\nWhat happens when a sequence \\(\\{A_n\\}_{n \\ge 1}\\) is neither increasing nor decreasing? We can sandwich it between an increasing sequence \\(\\{C_n\\}_{n \\ge 1}\\) and a decreasing sequence \\(\\{D_n\\}\\) as follows:\n\\[\n\\begin{array}{lcl}\nC_1 = A_1 \\cap A_2 \\cap A_3 \\cap \\dotsb\n&\\quad\\subseteq\\qquad\nA_1\n\\qquad\n&\\subseteq \\qquad\nD_1 = A_1 \\cup A_2 \\cup A_3 \\cup \\dotsb\n\\\\\nC_2 = \\phantom{A_1 \\cap{}} A_2 \\cap A_3 \\cap \\dotsb\n&\\quad\\subseteq\\qquad\nA_2\n\\qquad\n&\\subseteq \\qquad\nD_2 = \\phantom{A_1 \\cup{}} A_2 \\cup A_3 \\cup \\dotsb\n\\\\\nC_3 = \\phantom{A_1 \\cap A_2 \\cap{}} A_3 \\cap \\dotsb\n&\\quad\\subseteq\\qquad\nA_3\n\\qquad\n&\\subseteq \\qquad\nD_3 = \\phantom{A_1 \\cup A_2 \\cup{}} A_3 \\cup \\dotsb\n\\\\\n& \\qquad\\vdots &\n\\end{array}\n\\]\nThe limit of \\(\\{C_n\\}_{n \\ge 1}\\) is called the lim inf of \\(\\{A_n\\}_{n \\ge 1}\\), i.e., \\[\n  \\liminf_{n \\to ∞} A_n = \\lim_{n \\to ∞} C_n = \\bigcup_{n=1}^∞ C_n\n  = \\bigcup_{n=1}^{∞} \\bigcap_{i=n}^{∞} A_i.\n\\] Similarly, the limit of \\(\\{D_n\\}_{n \\ge 1}\\) is called the lim sup of \\(\\{A_n\\}_{n \\ge 1}\\), i.e., \\[\n  \\limsup_{n \\to ∞} A_n = \\lim_{n \\to ∞} D_n = \\bigcap_{n=1}^∞ D_n\n  = \\bigcap_{n=1}^{∞} \\bigcup_{i=n}^{∞} A_i.\n\\] When the two limits are equal, we say that the sequence \\(\\{A_n\\}_{n \\ge 1}\\) has a limit.\n\nAnother way to think about these definitions is as follows. \\[\n  ω \\in \\limsup_{n \\to ∞} A_n \\iff\n  \\limsup_{n \\to ∞} \\IND_{A_n}(ω) = 1\n\\] which holds if and only if the binary sequence \\(\\{\\IND_{A_n}(ω)\\}\\) has infinitely many ones, i.e., \\(ω\\) is a member of infinitely many \\(A_n\\).\nSimilarly, \\[\n  ω \\in \\liminf_{n \\to ∞} A_n \\iff\n  \\liminf_{n \\to ∞} \\IND_{A_n}(ω) = 1\n\\] which holds if and only if the binary sequence \\(\\{\\IND_{A_n}(ω)\\}\\) eventually becomes \\(1\\) forever, i.e., \\(ω\\) is eventually a member of \\(A_n\\) forever.\nFor example, suppose we toss a coin infinite number of coins. Let \\((Ω, \\ALPHABET F, \\PR)\\) denote the corresponding probability space, and let \\(A_n\\) denote the event that the \\(n\\)-th toss is a heads. Then,\n\n\\(\\limsup_{n \\to ∞} A_n\\) is the event that there are infinitely many heads.\n\\(\\liminf_{n \\to ∞} A_n\\) is the event that all but a finite number of the coins were heads, i.e., there were only finitely many tails.\n\n\n\n\nWe now state two fundamental results. The proofs are not difficult but are omitted due to time.\n\nLemma 8.1 (Borel Cantelli Lemma) Let \\(\\{A_n\\}_{n \\ge 1}\\) be a sequence of events defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\). If the sum of the probability of the events is finite, i.e., \\[\n\\sum_{n=1}^∞ \\PR(A_n) &lt; ∞,\n\\] then the probability that infinitely many of them occur is zero, i.e., \\[\n  \\PR\\Bigl(\\limsup_{n \\to ∞} A_n \\Bigr) = 0.\n\\]\n\nThere is a partial converse of Borel-Cantelli lemma.\n\nLemma 8.2 (Second Borel Cantelli Lemma) Let \\(\\{A_n\\}_{n \\ge 1}\\) be a sequence of independent events defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\). If the sum of the probability of the events is infinite, i.e., \\[\n\\sum_{n=1}^∞ \\PR(A_n) = ∞,\n\\] then the probability that infinitely many of them occur is one, i.e., \\[\n  \\PR\\Bigl(\\limsup_{n \\to ∞} A_n \\Bigr) = 1.\n\\]\n\nAn immediate implication of Borel-Cantelli lemma is the following:\n\nLemma 8.3 Suppose \\(X_n \\xrightarrow{p} X\\) and for any \\(ε &gt; 0\\), we have \\[\n\\sum_{n=1}^{∞} \\PR(\\ABS{X_n - X} &gt; ε) &lt; ∞\n\\] then \\(X_n \\xrightarrow{a.s.} X\\).\n\nIn light of the above result, we revisit some variations of the examples of the previous section.\n\nConsider Example 8.3, part a, we have \\[\n    \\PR(E_n) = \\frac 1n.\n\\] Since \\(\\sum_{n \\ge 1} \\PR(E_n) = ∞\\), we cannot use the above theorem to infer that \\(X_n \\xrightarrow{a.s.} 0\\). This is not a contradiction because Lemma 8.3 is a sufficient condition, not a necessary condition.\nConsider Example 8.3, part b, we have \\[\n    \\PR(E_n) = \\frac 1{n^2}.\n\\] Since \\(\\sum_{n \\ge 1} \\PR(E_n) &lt; ∞\\), we can use Lemma 8.3 to infer that \\(X_n \\xrightarrow{a.s.} 0\\).\nIn Example 8.4, we have argued that \\(\\PR(Y_n &gt; ε) = (1-ε)^n\\). Therefore, \\(\\sum_{n \\ge 1} \\PR(Y_n &gt; ε) = \\sum_{n=1}^∞ (1-ε)^n = \\frac{1-ε}{ε} &lt; ∞\\) (for \\(0 &lt; ε &lt; 1\\)). Hence, by Lemma 8.3, \\(Y_n \\xrightarrow{a.s.} 0\\).\n\nWe can also consider a variation of the above.\n\nExample 8.5 Consider a variation of Example 8.3, where we no longer specify \\(X_n\\) as a function of \\(ω\\) but simply assume that \\[\n   X_n = \\begin{cases}\n     0 & \\text{with probability } 1 - p_n \\\\\n     1 & \\text{with probability } p_n\n   \\end{cases}\n\\] and \\(\\{X_n\\}_{n \\ge 1}\\) are independent.\nDetermine whether \\(\\{X_n\\}_{n \\ge 1}\\) converges almost surely, in probability, or in mean square\n\n\\(p_n = \\frac 1n\\)\n\\(p_n = \\frac 1{n^2}\\).\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nConvergence in probability: Fix \\(ε \\in (0,1)\\). As before define \\(E_n = \\{ ω : \\ABS{X_n(ω)} &gt; ε \\} = \\{ ω : X_n(ω) = 1 \\}\\). Thus, \\(\\PR(E_n) = p_n\\) and for both cases, \\(p_n \\to 0\\) as \\(n \\to ∞\\). Therefore, \\(X_n \\xrightarrow{p} 0\\).\nAlmost sure convergence: For part (a), \\(\\sum_{n \\ge 1} p_n = ∞\\); hence by the Second Borel-Cantelli lemma, \\[ \\PR(\\limsup_{n \\to ∞} \\{ \\ABS{X_n} &gt; ε \\}) = 1. \\] So, \\(X_n\\) does not converge almost surely!\nFor part (b), \\(\\sum_{n \\ge 1} p_n &lt; ∞\\); hence by Lemma 8.3, \\(X_n \\xrightarrow{a.s.} 0\\).\nMean square convergence: We have \\(\\EXP[\\ABS{X_n}^2] = p_n\\). For both cases \\(p_n \\to 0\\). Hence, \\(X_n \\xrightarrow{m.s.} 0\\).\n\nNote that part (a) converges in probability and mean square but not almost surely.\n\n\n\nA variation of Lemma 8.4 is the following:\n\nLemma 8.4 Let \\(\\{X_n\\}_{n \\ge 1}\\) be a sequence of random variables with finite expectations and let \\(X\\) be another random variable. If \\[\n\\sum_{n=1}^{∞} \\EXP[ \\ABS{X_n - X} ]  &lt; ∞\n\\] then \\[\nX_n \\xrightarrow{a.s.} X.\n\\]\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nTo simplify the notation, we assume that \\(X = 0\\).\nPick any \\(ε &gt; 0\\) and define the sequence of events \\[\nA_n = \\bigl\\{ ω : \\ABS{X_n} &gt; ε \\bigr\\},\n\\quad n \\in \\naturalnumbers.\n\\]\nFrom Markov inequality, we have \\[\\PR(A_n) = \\PR\\bigl( \\ABS{X_n} &gt; ε \\bigr)\n\\le \\dfrac{\\EXP[\\ABS{X_n}]}{ε}. \\] Therefore, \\[ \\sum_{n=1}^{∞} \\PR(A_n)\n\\le \\frac{1}{ε} \\sum_{n=1}^{∞} \\EXP[\\ABS{X_n}]\n&lt; ∞\n\\] by the hypothesis of the result. Therefore, by Borel-Cantelli lemma, we have \\[\n\\PR\\Bigl( \\limsup_{n \\to ∞} A_n \\Bigr) = 0.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#strong-law-of-large-numbers",
    "href": "convergence-of-random-variables.html#strong-law-of-large-numbers",
    "title": "8  Convergence of random variables",
    "section": "8.8 Strong law of large numbers",
    "text": "8.8 Strong law of large numbers\n\nTheorem 8.1 Let \\(\\{X_n\\}_{n \\ge 1}\\) be an i.i.d. sequence of random variables with mean \\(μ\\) and variance \\(σ^2\\). Let \\[\n  \\bar X_n = \\frac 1n \\sum_{i=1}^n X_i\n\\] be the sample average. Then, \\(\\bar X_n \\xrightarrow{a.s.} μ\\), i.e., \\[\n\\PR\\Bigl( ω : \\lim_{n \\to ∞} \\bar X_n(ω) = μ \\Bigr) = 1.\n\\]\n\nWe provide a proof under the assumption that the fourth moment exists.\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nWe assume that \\(μ = 0\\) (this is just for notational simplicity) and \\(\\EXP[X^4] = γ &lt; ∞\\) (this is a strong assumption).\nSince we know that the fourth moment exists, we can use a fourth moment version of Markov inequality: \\[ \\PR(\\ABS{\\bar X_n} \\ge ε) = \\PR(\\ABS{\\bar X_n}^4 \\ge ε^4) \\le \\frac{ \\EXP[ \\bar X_n^4]}{ε^4}. \\]\nThen, by the multinomial theorem, we have \\[ \\EXP[\\bar X_n^4] = \\frac{1}{n^4} \\EXP\\biggl[\n\\sum_{i} X_i^4 + \\binom{4}{1,3} \\sum_{i \\neq j} X_i X_j^3 + \\binom{4}{2,2} \\sum_{i \\neq j} X_i^2 X_j^2\n+ \\binom{4}{1,1,2} \\sum_{i \\neq j \\neq k} X_i X_j X_k^2 + \\sum_{i \\neq j \\neq k \\neq \\ell} X_i X_j X_k X_{\\ell} \\biggr].\n\\]\nSince the \\(\\{X_i\\}_{i \\ge 1}\\) are independent and zero mean, we have\n\n\\(\\EXP[X_i X_j^3] = \\EXP[X_i] \\EXP[X_j^3] = 0\\).\n\\(\\EXP[X_i X_j X_k^2] = \\EXP[X_i] \\EXP[X_j] \\EXP[X_k^2] = 0\\).\n\\(\\EXP[X_i X_j X_k X_{\\ell}] = \\EXP[X_i] \\EXP[X_j] \\EXP[X_k] \\EXP[X_{\\ell}] = 0\\).\n\nTherefore, \\[\n\\EXP[\\bar X_n^4] = \\frac{1}{n^4} \\bigl[ n \\EXP[X_i^4] + 3 n(n-1) \\EXP[ X_i^2 X_j^2] \\bigr]\n\\le \\frac{M_4}{n^3} + \\frac{3 σ^4}{n^2}\n\\] where \\(M_4\\) is the fourth moment. Now, from the fourth moment version of Markov inequality, we have \\[ \\PR(\\ABS{\\bar X_n} \\ge ε) \\le \\frac{ \\frac{M_4}{n^3} + \\frac{3 σ^4}{n^2} }{ε^4}. \\] This implies that \\[\n\\sum_{n=1}^{∞} \\PR(\\ABS{\\bar X_n} \\ge ε) &lt; ∞.\n\\] Thus, from Lemma 8.3, we have that \\(\\bar X_n \\xrightarrow{a.s.} 0\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#further-reading",
    "href": "convergence-of-random-variables.html#further-reading",
    "title": "8  Convergence of random variables",
    "section": "8.9 Further Reading",
    "text": "8.9 Further Reading\n\nGubner: Sec. 13.1, 14.1, 14.2, 14.3 (and corresponding problems at the end of the chapters)\nGrimmett and Stirzaker: Sec. 7.1, 7.2, 7.3, 7.4, 7.5 (and corresponding problems at the end of the sections and the end of the chapters)",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "stochastic-processes.html",
    "href": "stochastic-processes.html",
    "title": "9  Stochastic processes",
    "section": "",
    "text": "A stochastic process is a family of random variables, indexed by time. You may think of it as a random signal.\nA stochastic process may be described in discrete-time, where we have a family of random variables \\(\\{X_n : n \\in N\\}\\) where index set \\(N\\) is either \\(\\{0, 1, \\dots, \\}\\) or \\(\\{1, 2, \\dots\\}\\) or in some cases \\(\\integers\\).\nNote that all random variables must be defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\). For a fixed \\(ω \\in Ω\\), we get a sequence of real numbers \\(\\{X_n(ω)\\}_{n \\ge 0}\\). Such a sequence is called a realization or sample path.\nAs an example, consider an infinite sequence of bits, where each bit is i.i.d. \\(\\text{Bernoulli}(p)\\). A sample path is shown below.\n\n\nviewof rerun_bits= Inputs.button(\"Generate sample path\")\n\npoints_bits = {\n\n    rerun_bits\n\n    const N = 120\n    const p = 0.5\n    var points = new Array(N)\n\n    for(var n=0; n &lt; N; n++) {\n      var val = (Math.random() &lt;= p) ? 0 : 1\n      points[n-1] = { n: n, X_n: val }\n    }\n    return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  y : {ticks: 1 },\n  height: 100,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.ruleX(points_bits, {x: \"n\", y: \"X_n\", strokeWidth: 1}),\n    Plot.dot(points_bits, {x: \"n\", y: \"X_n\", fill: \"currentColor\", r: 2}),\n  ]\n})\n\n\n\n\n\n\n\nAs an other example, consider a stochastic process \\(\\{Y_n\\}_{n \\ge 0}\\) defined as \\[ Y_n = x_n + W_n \\] where \\(\\{x_n\\}_{n \\ge 0}\\) is a deterministic discrete-time signal given by \\(x_n = \\sin(2πn/25)\\) and \\(\\{W_n\\}_{n \\ge 0}\\) is a Gaussian noise process, where \\(W_n \\sim \\mathcal{N}(0,0.2)\\). A sample path of \\(\\{Y_n\\}_{n \\ge 0}\\) is shown below.\n\n\nfunction gaussianRandom(mean=0, stdev=1) {\n    const u = 1 - Math.random(); // Converting [0,1) to (0,1]\n    const v = Math.random();\n    const z = Math.sqrt( -2.0 * Math.log( u ) ) * Math.cos( 2.0 * Math.PI * v );\n    // Transform to the desired mean and standard deviation:\n    return z * stdev + mean;\n}\n\nviewof rerun_noise = Inputs.button(\"Generate sample path\")\n\npoints_noise = {\n\n    rerun_noise\n\n    const N = 120\n    const sigma = 0.2\n    const omega = 2*Math.PI*0.04\n    var points = new Array(N)\n\n    for(var n=0; n &lt; N; n++) {\n      var signal = Math.sin(omega*n)\n      var noise = gaussianRandom(0, sigma)\n\n      points[n-1] = { n: n, X_n: signal,  W_n: noise, Y_n: signal + noise }\n    }\n    return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  y : {ticks: 1 },\n  height: 100,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.ruleX(points_noise, {x: \"n\", y: \"Y_n\", strokeWidth: 1}),\n    Plot.dot(points_noise, {x: \"n\", y: \"Y_n\", fill: \"currentColor\", r: 2}),\n  ]\n})\n\n\n\n\n\n\n\nIt is also possible for a stochastic process to be continuous time, where we have an uncountable collection of random variables \\(\\{X_t : t \\in T \\}\\), where the index set \\(T\\) is either \\((-∞, ∞)\\) or \\([0, ∞)\\). Continuous-time stochastic processes tend to be a bit more technical than discrete-time stochastic processes.\nAs the above examples illustrate, the random variables \\(\\{X_n\\}_{n \\ge 0}\\) need not be independent. So, we need a way to specify the joint distribution of a countable collection of random variables. This means that for any collection of time indices \\(\\{n_1, \\dots, n_k\\}\\), we must be able to specify the joint CDF of the random variables \\((X_{n_1}, X_{n_2}, \\dots, X_{n_k})\\).\nA discrete time stochastic process \\(\\{X_n\\}_{n \\ge 0}\\) is called strong-sense stationary if for any \\(\\{n_1, \\dots, n_k\\}\\) and \\(m &gt; 0\\), the random vectors \\[\n  (X_{n_1}, X_{n_2}, \\dots, X_{n_k})\n  \\quad \\text{and}\\quad\n  (X_{n_1 + m}, X_{n_2 + m}, \\dots, X_{n_k + m})\n\\] have the same joint distributions.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Stochastic processes</span>"
    ]
  },
  {
    "objectID": "markov-chains.html",
    "href": "markov-chains.html",
    "title": "10  Markov chains",
    "section": "",
    "text": "10.1 Time-homogeneous Markov chains\nLet \\(\\ALPHABET X\\) be a finite set. A stochastic process \\(\\{X_n\\}_{n \\ge 0}\\), \\(X_n \\in \\ALPHABET X\\), is called a Markov chain if it satisfies the Markov property: for any \\(n \\in \\integers_{\\ge 0}\\) and any \\(x_{1:n+1} \\in \\ALPHABET X^{n+1}\\), we have \\[\\begin{equation}\\tag{Markov property}\\label{eq:Markov}\n  \\PR(X_{n+1} = x_{n+1} \\mid X_{1:n} = x_{1:n})\n  = \\PR(X_{n+1} = x_{n+1} \\mid X_n = x_n).\n\\end{equation}\\]\nThe variable \\(X_n\\) is called the state of the Markov chain at time \\(n\\); the set \\(\\ALPHABET X\\) is called state space. The \\(\\eqref{eq:Markov}\\) implies that the current state captures all the information from the past that is relevant for the future. Stated differently, conditioned on the present, the past is independent of the future.\nIndependence is a symmetric relationship. Thus, we expect the Markov property to also hold if time is reversed! Exercise 10.1 asks you to formally prove that.\nWe now present some examples of Markov chains arising in different applications.\nSee the following video from Veritasium for an excellent history of Markov chains and its applications\nIn this course, we will focus on time-homogeneous Markov chains. The Markov chain is called time-homogeneous if the right hand side of \\(\\eqref{eq:Markov}\\) does not depend on \\(n\\). In this case, we describe the Markov chain by a state transition matrix \\(P\\), where \\(P_{ij} = \\PR(X_{n+1} = j | X_n = i)\\). Such Markov chains can also be visualized using state transition diagrams as we illustrate in the examples below.\nIn spite of its simplicity, such simple on-off Markov chains are used in various applications including telecommunication systems, network traffic modeling, machine failure-repair models, gene activation, and others. Some properties of the on-off Markov chain are as follows:\nNote that the gambler’s fortune in Example 10.3 is a random walk on non-negative integers \\(\\{0,1,2, \\dots\\}\\) with absorption at \\(0\\). In the variation where the gambler stops when his fortune reaches $\\(K\\), it is a random walk over \\(\\{0,1,\\dots,K\\}\\) with absorption at both ends: \\(0\\) and \\(K\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#time-homogeneous-markov-chains",
    "href": "markov-chains.html#time-homogeneous-markov-chains",
    "title": "10  Markov chains",
    "section": "",
    "text": "Example 10.4 (On-Off Markov chain) The On-Off Markov chain discussed earlier can be modelled with \\(\\ALPHABET X = \\{0, 1\\}\\) and a general transition probability matrix of the form. \\[\n  P = \\MATRIX{ 1 - a & a \\\\ b & 1 - b }.\n\\] The transition matrix can be visualized as follows.\n\n\n\nOn-Off Markov chain\n\n\n\n\n\nIf \\(a + b = 1\\), then both rows of the transition matrix are identical. Therefore, the Markov chain has no memory and is equivalent to a Bernoulli process with success probability \\(a = 1-b\\).\nWhen \\(a\\) or \\(b\\) are small, the corresponding state is “sticky”, i.e., when the Markov chain enters a sticky state, it stays there for a long time.\n\n\nExample 10.5 The Ehrenfest model of diffusion presented in Example 10.2 can be modelled as a Markov chain with state space \\(\\{0, 1, \\dots, K\\}\\) and transition probability \\[\n  P_{ij} = \\begin{cases}\n    i/K     & j = i-1 \\\\\n    (K-i)/K & j = i + 1 \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] The transition matrix can be visualized as follows.\n\n\n\nEhrenfest model of diffusion\n\n\n\n\nExample 10.6 (Random walk in one dimension) Imagine a particle which moves in a straight line in unit steps. Each step is one unit to the right with probability \\(p\\) or one unit to the left with probability \\(q = 1-p\\). It moves until it reaches one of two extreme points, which are called boundary points. The behavior of the particle at the boundary determines several different possibilities.\nWe will consider the case where the state space is \\(\\ALPHABET X = \\{-2, -1, 0, 1, 2\\}\\) and the process starts in state \\(0\\).\n\nAbsorbing random walk: Assume that when the particle reaches a boundary state, it stays there from that time on. We may visualize the Markov chain as follows.\n\n\n\nAbsorbing random walk\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 1 & 0 & 0 & 0 & 0 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                0 & 0 & 0 & 0 & 1}.\n\\]\nReflected random walk: Assume that when the particle reaches a boundary states, it is reflected and returns to the point it came from. We may visualize the Markov chain as follows.\n\n\n\nReflected random walk\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 0 & 1 & 0 & 0 & 0 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                0 & 0 & 0 & 1 & 0}.\n\\]\nRandom walk with restart: Assume that when the particle reaches a boundary state, it restarts in the initial state. We may visualize the Markov chain as follows.\n\n\n\nRandom walk with restart\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 0 & 0 & 1 & 0 & 0 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                0 & 0 & 1 & 0 & 0}.\n\\]\nRandom walk with periodic boundary: Assume that when the particle reaches a boundary state, it moves to the other boundary. We may visualize the Markov chain as follows.\n\n\n\nRandom walk with periodic boundary\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 0 & 0 & 0 & 0 & 1 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                1 & 0 & 0 & 0 & 0}.\n\\]\n\n\n\n\n10.1.1 Properties of interest\nDepending on the application, we are typically interested in the following properties of a Markov chain:\n\nIf the chain starts in state \\(i\\), what is the probability that after \\(n\\) steps it is in state \\(j\\)?\nIf the chain starts in state \\(i\\), what is the expected number of visits to state \\(j\\) in \\(n\\) steps?\nWhat is the expected number of steps that it takes for a chain starting in state \\(i\\) to visit state \\(j\\) for the first time?\nWhat is the average number of times that the chain is in state \\(i\\)? How does this depend on the initial state?\n\nIn the rest of this section, we will present results in Markov chain theory that answer the above questions.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#state-occupancy-probabilities",
    "href": "markov-chains.html#state-occupancy-probabilities",
    "title": "10  Markov chains",
    "section": "10.2 State occupancy probabilities",
    "text": "10.2 State occupancy probabilities\n\nLet \\(μ^{(n)}\\) denote the PMF of the state of the Markov chain at time \\(n\\). This is also called the state occupancy probabilities. We will think of \\(μ^{(n)}\\) as a row vector.\nBy the law of total probability, we have \\[\n  \\PR(X_n = j) = \\sum_{i \\in \\ALPHABET X} \\PR(X_{n-1} = i) \\PR(X_n = j | X_{n-1} = i)\n\\] or, equivalently, \\[\n  μ^{(n)}_j = \\sum_{i \\in \\ALPHABET X} μ^{(n-1)}_i P_{ij}\n\\] which can be written in matrix form as \\[ μ^{(n)} = μ^{(n-1)} P \\] and, by recursively expanding the right hand side, we have \\[ μ^{(n)} = μ^{(0)} P^n. \\]\nWe will abbreviate \\([P^n]_{ij}\\) as \\(P^{(n)}_{ij}\\). Note that \\[\n   P^{(n)}_{ij} = \\PR(X_n = j \\mid X_0 = i)\n\\] denotes the \\(n\\)-step transition probability.\nChapman-Kolmogorov equations: The multi-step transition probabilities satisfy the following: for any positive integers \\(m\\) and \\(n\\) and \\(i,j \\in \\ALPHABET X\\), we have \\[\n   P^{(n+m)}_{ij} = \\sum_{k \\in \\ALPHABET X} P^{(n)}_{ik} P^{(m)}_{kj}.\n\\] Thus, the probability of going from \\(i\\) to \\(j\\) in \\(n+m\\) steps equals the probability of going from \\(i\\) to somewhere in \\(n\\) steps and then from there to \\(j\\) in \\(m\\) steps.\n\n\nExample 10.7 Numerically compute the state occupancy probabilities for the different examples of random walk presented in Example 11.3 when \\(p = q = \\tfrac 12\\) for \\(n \\in \\{1, \\dots, 8\\}\\). In all cases, the initial probability \\[\n  μ^{(0)} = \\MATRIX{0 & 0 & 1 & 0 & 0 }.\n\\]\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nAbsorbing random walk: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & 0 & \\frac{1}{4} & 0 & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & \\frac{1}{8} & 0 & \\frac{1}{8} & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & 0 & \\frac{1}{8} & 0 & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & \\frac{1}{16} & 0 & \\frac{1}{16} & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{15}{32} & 0 & \\frac{1}{16} & 0 & \\frac{15}{32} \\\\\n\\end{array}\n\\right]\\)\n\nReflected random walk: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\nRandom walk with restart: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{8} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{8} & \\frac{1}{8} & \\frac{1}{2} & \\frac{1}{8} & \\frac{1}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{16} & \\frac{1}{4} & \\frac{3}{8} & \\frac{1}{4} & \\frac{1}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{8} & \\frac{3}{16} & \\frac{3}{8} & \\frac{3}{16} & \\frac{1}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{32} & \\frac{3}{16} & \\frac{7}{16} & \\frac{3}{16} & \\frac{3}{32} \\\\\n\\end{array}\n\\right]\\)\n\nRandom walk with periodic boundary: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & 0 & \\frac{1}{4} & 0 & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & \\frac{1}{8} & 0 & \\frac{1}{8} & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & 0 & \\frac{1}{8} & 0 & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & \\frac{1}{16} & 0 & \\frac{1}{16} & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{15}{32} & 0 & \\frac{1}{16} & 0 & \\frac{15}{32} \\\\\n\\end{array}\n\\right]\\)\n\n\n\n\n\n\nExample 10.8 Analytically compute the state occupancy probabilities for the on-off Markov chain of Example 10.4 when it starts from the initial probability distribution \\(μ^{(0)}\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nSince \\(μ^{(n+1)} = μ^{(n)} P\\), we have \\[\\begin{align*}\n  μ^{(n+1)}_0 &= μ^{(n)}_0 (1-a) + μ^{(n)}_1 b\n  \\\\\n  &= μ^{(n)}_0 (1-a) + (1 - μ^{(n)}_0) b\n  \\\\\n  &= μ^{(n)}_0 (1-a-b) + b.\n\\end{align*}\\] If \\(a = b = 0\\), then \\(μ^{(n+1)}_0 = μ^{(n)}_0 = \\cdots = μ^{(0)}_0\\). If not, we exploit the fact that \\[\n  b = \\frac{b}{a+b} - \\frac{b}{a+b}(1-a-b)  \n\\] to recursively write \\[\\begin{align*}\n  μ^{(1)}_0 &= μ^{(0)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b) + \\frac{b}{a+b}\n\\end{align*}\\] and \\[\\begin{align*}\n  μ^{(2)}_0 &= μ^{(1)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^2 + \\frac{b}{a+b}(1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^2 + \\frac{b}{a+b}\n\\end{align*}\\] and, so on, to get \\[\\begin{align*}\n  μ^{(n)}_0 &= μ^{(n-1)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^n + \\frac{b}{a+b}(1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^n + \\frac{b}{a+b}.\n\\end{align*}\\] Therefore, \\[\n  μ^{(n)}_1 = 1 - μ^{(n)}_0\n  = \\left(μ^{(0)}_1 - \\frac{a}{a+b}\\right)(1-a-b)^n + \\frac{a}{a+b}.\n\\]\n\n\n\n\n\n\n\n\n\nWarningHow did we figure out the above calculation?\n\n\n\n\n\nThe above analysis appears to be a bit of black magic. To understand what is going on, note that we are interested in computing \\(μ^{(0)} P^n\\). What is an efficient way to compute \\(P^n\\)? Eigen decomposition!. Since \\(P\\) is a row stochastic matrix, we have \\(P \\mathbf{1} = \\mathbf{1}\\), where \\(\\mathbf{1}\\) is the all-ones vector. Thus, \\(λ_1 = 1\\) is always an eigenvalue of any transition matrix, with corresponding eigenvector \\(\\mathbf{1}\\).\nFor Example 10.4, we can explicitly compute all eigenvalues by finding the roots of the characteristic equation \\[\\begin{align*}\n\\det(λI - P) &= \\DET{λ - 1 + a & - a \\\\ -b & λ - 1 + b} \\\\\n&= (λ-1 +a)(λ-1+b) - ab \\\\\n&= (λ-1)^2 + (a+b)(λ-1) = (λ-1)(λ-1 + a + b).\n\\end{align*}\\] Thus, the eigenvalues are \\(λ_1 = 1\\) and \\(λ_2 = 1 - a - b\\).\nFor the special case of \\(2 × 2\\) transition matrices, we can find the second eigenvalue by observing that \\(λ_1 = 1\\) is always an eigenvalue and \\(\\TR(P) = λ_1 + λ_2 = 1 + λ_2\\) or \\(\\det(P) = λ_1 λ_2 = λ_2\\).\nTo find the eigenvector, we find a vector \\(v\\) such that \\((λI - P)v = 0\\).\n\nFor \\(λ_1 = 1\\), we already know that \\(v_1 = [1; 1]\\) is an eigenvector.\nFor \\(λ_2 = (1-a-b)\\), we have \\[ λ_2I - P = \\MATRIX{-b & -a \\\\ -b & -a}. \\] Therefore, one possible eigenvector is \\([a; -b]\\).\n\nThen, from spectral decomposition, we know that \\[\n  P = V Λ V^{-1}\n\\] where \\(V = [v_1 v_2] = [1, a; 1, -b]\\) and \\(Λ = \\diag(1, 1-a-b)\\). Therefore, \\[\\begin{align*}\n  μ^{(n)} &= μ^{(0)} P^n = μ^{(0)} V Λ^n V^{-1} \\\\\n  &= \\MATRIX{ μ^{(0)}_0 & 1 - μ^{(0)}_0 }\n     \\MATRIX{1 & a \\\\ 1 & -b }\n     \\MATRIX{1 & 0 \\\\ 0 & 1 - a -b}\n     \\frac{1}{a+b}\n     \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &= \\MATRIX{\\dfrac{1}{a+b} & μ^{(0)}_0 - \\dfrac{b}{a+b} }\n     \\MATRIX{1 & 0 \\\\ 0 & 1 - a -b}\n     \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &=\\MATRIX{\\dfrac{1}{a+b} & 0 \\\\ 0 & \\left(μ^{(0)}_0 - \\dfrac{b}{a+b}\\right)(1-a-b)^n }\n   \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &= \\frac{b}{a+b} + \\left(μ^{(0)}_0 - \\dfrac{b}{a+b}\\right)(1-a-b)^n\n\\end{align*}\\]\n\n\n\nTODO: Add more examples!",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#class-structure",
    "href": "markov-chains.html#class-structure",
    "title": "10  Markov chains",
    "section": "10.3 Class structure",
    "text": "10.3 Class structure\n\nWe say that a state \\(j\\) is accessible from state \\(i\\) (abbreviated as \\(i \\rightsquigarrow j\\)) if there exists an \\(m \\in \\integers_{\\ge 0}\\) (which may depend on \\(i\\) and \\(j\\)) such that \\([P^m]_{ij} &gt; 0\\). The fact that \\(P^{(m)}_{ij} &gt; 0\\) implies that there exists an ordered sequence of states \\((i_0, \\dots, i_m)\\) such that \\(i_0 = i\\) and \\(i_m = j\\) such that \\(P_{i_k i_{k+1}} &gt; 0\\); thus, there is a path of positive probability from state \\(i\\) to state \\(j\\).\nAccessibility is a transitive relationship, i.e., if \\(i \\rightsquigarrow j\\) and \\(j \\rightsquigarrow k\\), then \\(i \\rightsquigarrow k\\).\n\nExample 10.9 Consider the Markov chain shown below.\n\n\nIdentify all states that are accessible from state \\(1\\).\nIdentify all states from which state \\(1\\) is accessible.\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nStates accessible from state \\(1\\) are \\(\\{1,2,3\\}\\).\nStates from which state \\(1\\) is accessible are \\(\\{1,2,3,4,5,6\\}\\).\n\n\n\n\nTwo distinct states \\(i\\) and \\(j\\) are said to communicate (abbreviated to \\(i \\leftrightsquigarrow j\\)) if \\(i\\) is accessible from \\(j\\) (i.e., \\(j \\rightsquigarrow i\\)) and \\(j\\) is accessible from \\(i\\) (\\(i \\rightsquigarrow j\\)). Alternatively, we say that \\(i\\) and \\(j\\) communicate if there exist \\(m, m' \\in \\integers_{\\ge 0}\\) such that \\(P^{(m)}_{ij} &gt; 0\\) and \\(P^{(m')}_{ji} &gt; 0\\).\nFor instance, in Example 10.9, state \\(1\\) communicates with state \\(2\\) but does not communicate with state \\(5\\).\nCommunication is an equivalence relationship, i.e., it is reflexive (\\(i \\leftrightsquigarrow i\\)), symmetric (\\(i \\leftrightsquigarrow j\\) if and only if \\(j \\leftrightsquigarrow i\\)), and transitive (\\(i \\leftrightsquigarrow j\\) and \\(j \\leftrightsquigarrow k\\) implies \\(i \\leftrightsquigarrow k\\)).\nThe states in a finite-state Markov chain can be partitioned into two sets: recurrent states and transient states. A state is recurrent if it is accessible from all states that are accessible from it (i.e., \\(i\\) is recurrent if \\(i \\rightsquigarrow j\\) implies that \\(j \\rightsquigarrow i\\)). States that are not recurrent are transient.\nIt can be shown that a state \\(i\\) is recurrent if and only if \\[\\sum_{m=1}^{\\infty} P^{(m)}_{ii} = \\infty.\\] This is a consequence of the second Borel-Cantelli lemma (Lemma 8.2) and strong Markov property (to be discussed later).\nStates \\(i\\) and \\(j\\) are said to belong to the same communicating class if \\(i\\) and \\(j\\) communicate. Communicating classes form a partition of the state space. Within a communicating class, all states are of the same type, i.e., either all states are recurrent (in which case the class is called a recurrent class) or all states are transient (in which case the class is called a transient class).\nFor example, in Example 10.9, there are two communication classes: \\(\\{1,2,3\\}\\) and \\(\\{4,5,6\\}\\). The communication class \\(\\{4,5,6\\}\\) is transient while the communication class \\(\\{1,2,3\\}\\) is recurrent.\nA communicating class \\(C\\) is said to be closed if \\[\n    i \\in C \\text{ and } i \\rightsquigarrow j \\implies j \\in C.\n\\] Thus, there is no escape from a closed class. For finite state spaces, a recurrent class is always closed and a transient class is never closed. But this is not the case for countable state Markov chains.\nA state \\(i\\) is called absorbing if \\(\\{i\\}\\) is a closed class, i.e., if \\(P_{ii} = 1\\).\nA Markov chain with a single communicating class (thus, all states communicate with each other and are, therefore, recurrent) is called irreducible.\nIf \\(C\\) is a finite irreducible closed set of states, then every state in \\(C\\) is recurrent. This is an important result for finite-state Markov chains: any finite closed communicating class must be recurrent.\nThe period of a state \\(i\\), denoted by \\(d(i)\\), is defined as \\[d(i) = \\gcd\\{ t \\in \\integers_{\\ge 1} : [P^t]_{ii} &gt; 0 \\}.\\] If the period is \\(1\\), the state is aperiodic, and if the period is \\(2\\) or more, the state is periodic. It can be shown that all states in the same class have the same period.\nA Markov chain is aperiodic, if all states are aperiodic. A simple sufficient (but not necessary) condition for an irreducible Markov chain to be aperiodic is that there exists a state \\(i\\) such that \\(P_{ii} &gt; 0\\). In general, for a finite and aperiodic Markov chain, there exists a positive integer \\(M\\) such that \\[ P^{(m)}_{ii} &gt; 0,\n       \\quad \\forall m \\ge M, i \\in \\ALPHABET X.\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#hitting-times-and-absorption-probabilities",
    "href": "markov-chains.html#hitting-times-and-absorption-probabilities",
    "title": "10  Markov chains",
    "section": "10.4 Hitting times and absorption probabilities",
    "text": "10.4 Hitting times and absorption probabilities\n\nWe use the following notation:\n\nFor any event \\(E\\), \\(\\PR_i(E)\\) denotes \\(\\PR(E \\mid X_0 = i)\\)\nFor any random variable \\(Y\\), \\(\\EXP_i[Y]\\) denotes \\(\\EXP[Y \\mid X_0 = i]\\).\n\nLet \\(A\\) be a subset of \\(\\ALPHABET X\\). The hitting time \\(H^A\\) of \\(A\\) is a random variable \\(H^A \\colon \\ALPHABET X \\to \\{0, 1, \\dots \\} \\cup \\{∞\\}\\) given by \\[\n  H^A(ω) = \\min\\{n \\ge 0 : X_n(ω) \\in A\\}.\n\\] The standard convention is that \\(H^A\\) is taken to be \\(∞\\) if \\(X_n \\neq A\\) for any \\(n &gt; 0\\). For a state \\(j \\in \\ALPHABET X\\), we use the short-hand \\(H^j\\) to denote \\(H^{\\{j\\}}\\).\nThe probability that starting from state \\(i\\) the Markov chain ever hits \\(A\\) is then given by \\[\n   h^A_i = \\PR_i(H^A &lt; ∞).\n\\] This is called hitting probability. When \\(A\\) is a closed class, \\(h^A_i\\) is called the absorption probability.\nThe mean-time taken for the Markov chain to reach \\(A\\) is given by \\[\n  m^A_i = \\EXP_i[H^A] = \\sum_{n=0}^{∞} n \\PR_i(H^A = n).\n\\] This is called the mean hitting time.\nA remarkable property of Markov chain is that these quantities can be computed by solving a system of linear equations associated with the transition probability matrix \\(P\\). We start with some examples to illustrate the main idea.\n\nExample 10.10 Suppose we toss a coin multiple times. What is the probability of getting a head before getting a tail?\nWe model this using a Markov chain where the state represents the outcome we’re waiting for. Let \\(p\\) denote the probability of heads and \\(q = 1-p\\) denote the probability of tails. The Markov chain has three states:\n\nState \\(-1\\): tail has occurred (absorbing state, we lose)\nState \\(0\\): initial state (no outcome yet)\nState \\(1\\): head has occurred (absorbing state, we win)\n\n\n\n\nMarkov chain for coin tossing until head or tail\n\n\nLet \\(h^1_i\\) denote the probability of hitting state \\(1\\) (i.e., getting a head) when starting at state \\(i\\). Then, we have \\[\\begin{align*}\n  h^1_{-1} &= 0, \\\\\n  h^1_0 &= p h^1_1 + q h^1_{-1} = p \\cdot 1 + q \\cdot 0 = p, \\\\\n  h^1_1 &= 1.\n\\end{align*}\\] Therefore, starting from state \\(0\\), the probability of getting a head before a tail is \\(h^1_0 = p\\).\n\n\nExample 10.11 Two players are playing a game. They repeatedly toss a coin. Player 1 wins if two consecutive heads occur before two consecutive tails. Player 2 wins if two consecutive tails occur before two consecutive heads. What is the probability that Player 1 wins?\nWe model this using a Markov chain where the state tracks the pattern of recent coin tosses. Let \\(p\\) denote the probability of heads and \\(q = 1-p\\) denote the probability of tails. The Markov chain has five states:\n\nState \\(-2\\): two consecutive tails (Player 2 wins, absorbing state)\nState \\(-1\\): one tail (last toss was a tail)\nState \\(0\\): initial state (or after alternating outcomes like HT or TH)\nState \\(1\\): one head (last toss was a head)\nState \\(2\\): two consecutive heads (Player 1 wins, absorbing state)\n\n\n\n\nMarkov chain for two heads before two tails\n\n\nLet \\(h^2_i\\) denote the probability of hitting state \\(2\\) (i.e., Player 1 wins) when starting at state \\(i\\). Then, we have \\[\\begin{align*}\n  h^2_{-2} &= 0, \\\\\n  h^2_{-1} &= p h^2_0 + q h^2_{-2} = p h^2_0 + q \\cdot 0 = p h^2_0, \\\\\n  h^2_0 &= p h^2_1 + q h^2_{-1}, \\\\\n  h^2_1 &= p h^2_2 + q h^2_0 = p \\cdot 1 + q h^2_0 = p + q h^2_0, \\\\\n  h^2_2 &= 1.\n\\end{align*}\\] From the second equation, we get \\(h^2_{-1} = p h^2_0\\). From the fourth equation, we get \\(h^2_1 = p + q h^2_0\\). Substituting into the third equation: \\[\n  h^2_0 = p h^2_1 + q h^2_{-1} = p(p + q h^2_0) + q(p h^2_0) = p^2 + pq h^2_0 + pq h^2_0 = p^2 + 2pq h^2_0.\n\\] Solving for \\(h^2_0\\), we get \\(h^2_0(1 - 2pq) = p^2\\), so \\[\n  h^2_0 = \\frac{p^2}{1 - 2pq} = \\frac{p^2}{1 - 2p(1-p)} = \\frac{p^2}{1 - 2p + 2p^2} = \\frac{p^2}{2p^2 - 2p + 1}.\n\\] For a fair coin (\\(p = q = \\frac{1}{2}\\)), we get \\(h^2_0 = \\frac{(1/2)^2}{1 - 2(1/2)(1/2)} = \\frac{1/4}{1 - 1/2} = \\frac{1/4}{1/2} = \\frac{1}{2}\\).\n\nWe now provide a general formula for computing hitting probabilities.\n\nTheorem 10.1 (Hitting probabilities) The hitting probabilities \\(\\{h^A_i\\}_{i \\in \\ALPHABET X}\\) satisfies the following system of linear equations: \\[\n  h^A_i = \\begin{cases}\n    1, & i \\in A \\\\\n    \\sum_{j \\in \\ALPHABET X} P_{ij} h^A_j, & i \\not\\in A\n  \\end{cases}\n\\] When \\(\\ALPHABET X\\) is finite, the above system has a unique solution; when \\(\\ALPHABET X\\) is countable, the above system may have multiple solutions and the hitting probabilities correspond to the minimal non-negative solution.\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nWhen \\(X_0 = i \\in A\\), the hitting time \\(H^A = 0\\), so \\(h^A_i = 1\\). This proves the first part of the formula.\nFor the second part, consider \\(X_0 = i \\not\\in A\\). Then \\(H^A_i \\ge 1\\). By the Markov property, we have \\[\n\\PR_i(H^A &lt; ∞ \\mid X_1 = j) = \\PR_j(H^A &lt; ∞) = h^A_j.\n\\] Moreover, by the law of total probability, we have \\[\\begin{align*}\nh^A_i &= \\PR_i(H^A &lt; ∞) = \\sum_{j \\in \\ALPHABET X} \\PR_i(H^A &lt; ∞, X_1 = j) \\\\\n&= \\sum_{j \\in \\ALPHABET X} \\PR_i(H^A &lt; ∞ \\mid X_1 = j) \\PR_i(X_1 = j) \\\\\n&= \\sum_{j \\in \\ALPHABET X} P_{ij} h^A_j.\n\\end{align*}\\]\n\n\n\n\nExample 10.12 Consider a gambler’s ruin problem, where the gambler starts with $\\(1\\) and stops either when he is ruined or when his fortune reaches $\\(K\\).\nFind the probability of ruin (i.e., the fortune gets absorbed in state \\(0\\) rather than state \\(K\\)).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFor the ease of notation, we use \\(h_i\\) as a short-form for \\(h^{\\{0\\}}_i\\). The hitting probabilities satisy the linear system of equations:\n\\[\\begin{align*}\n  h_0 &= 1 \\\\\n  h_i &= ph_{i+1} + q h_{i-1}, \\quad i \\in \\{1,\\dots,K-1\\} \\\\\n  h_K &= 0,\n\\end{align*}\\] where \\(q = 1-p\\).\nThe characteristic equation associated with the linear recurrence relationship is \\[\n  λ = p λ^2 + q\n\\] which has two distinct roots, \\(λ_1 = 1\\) and \\(λ_2 = q/p\\) if \\(p \\neq q\\) and a double root at \\(λ_1 = 1\\) if \\(p = q = \\frac 12\\). Therefore, the general solution is of the form \\[\n  h_i = a λ_1^i + b λ_2^i = a + b\\Bigl(\\tfrac {q}{p} \\Bigr)^i\n\\] where we determine the coefficients \\(a\\) and \\(b\\) from the boundary conditions \\(h_0 = 1\\) and \\(h_K = 0\\). Solving for \\(a\\) and \\(b\\), we get that for \\(p \\neq q\\), we have \\[\n  h_i = \\frac{1 - \\Bigl(\\frac qp\\Bigr)^i}{1 - \\Bigl(\\frac qp\\Bigr)^K}\n\\] and for \\(p = q = \\frac 12\\), we have \\[\n  h_i = \\frac{i}{K}.\n\\]\n\n\n\nSimilarly as above, we can derive a formula for computing mean hitting times. We start with a couple of examples to illustrate the main idea.\n\nExample 10.13 Suppose we toss a coin multiple times and stop at the first head. What is the expected number of coin tosses until stopping?\nFrom elementary probability we know that the number of tosses until stopping is a geometric random variable. However, we will model this using a Markov chain where the state denotes the number of consecutive heads so far. Let \\(p\\) denote the probability of heads and \\(q = 1-p\\) denote the probability of tails. Then, the Markov chain model is as follows.\n\n\n\nMarkov chain for coin tossing until one head\n\n\nLet \\(m^1_i\\) denote the expected number of tosses until stopping (i.e., hitting state \\(1\\)) when starting at state \\(i\\). Then, we have \\[\\begin{align*}\n  m^1_0 &= 1 + q m^1_0 + p m^1_1, \\\\\n  m^1_1 &= 0.\n\\end{align*}\\] Solving this system of equations, we get \\(m^1_0 = 1/(1-q) = 1/p\\).\n\n\nExample 10.14 Suppose we toss a coin multiple times and stop at two consecutive heads. What is the expected number of coin tosses until stopping?\nWe can model this in the same manner as before, where the state denotes the number of consecutive heads so far. The Markov chain is as follows:\n\n\n\nMarkov chain for coin tossing until two heads\n\n\nLet \\(m^2_i\\) denote the expected number of tosses until stopping (i.e., hitting state \\(2\\)) when starting at state \\(i\\). Then, we have \\[\\begin{align*}\n  m^2_0 &= 1 + q m^2_0 + p m^2_1, \\\\\n  m^2_1 &= 1 + q m^2_0 + p m^2_2, \\\\\n  m^2_2 &= 0.\n\\end{align*}\\] Solving this system of equations, we get \\(m^2_0 = 1/(1-p)\\).\n\nWe now provide a general formula for computing mean hitting times.\n\nTheorem 10.2 (Mean hitting times) The mean hitting times \\(\\{m^A_i\\}_{i \\in \\ALPHABET X}\\) satisfies the following system of linear equations: \\[\n  m^A_i = \\begin{cases}\n    0, & i \\in A \\\\\n    1 + \\sum_{j \\not\\in A} P_{ij} m^A_j, & i \\not\\in A\n  \\end{cases}\n\\] When \\(\\ALPHABET X\\) is finite, the above system has a unique solution; when \\(\\ALPHABET X\\) is countable, the above system may have multiple solutions and the mean hitting times correspond to the minimal non-negative solution.\n\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe proof is similar to the proof of Theorem 10.1. When \\(X_0 = i \\in A\\), the hitting time \\(H^A = 0\\), so \\(m^A_i = 0\\). This proves the first part of the formula.\nFor the second part, consider \\(X_0 = i \\not\\in A\\). Then \\(H^A_i \\ge 1\\). By the Markov property, we have \\[\n  \\EXP_i[ H^A \\mid X_1 = j] = 1 + \\EXP_j[ H^A ] = 1 + m^A_j.\n\\] Moreover, by the law of total probability, we have \\[\\begin{align*}\n  m^A_i &= \\EXP_i[ H^A ]\n  = \\sum_{j \\in \\ALPHABET X} \\EXP_i[ H^A \\mid X_1 = j ] \\PR_i(X_1 = j) \\\\\n  &= \\sum_{j \\in \\ALPHABET X} \\EXP_i[ H^A \\mid X_1 = j ] \\PR_i(X_1 = j) \\\\\n  &= \\sum_{j \\in \\ALPHABET X} P_{ij} \\bigl[ 1 + m^A_j \\bigr]\n  \\\\\n  &= 1 + \\sum_{j \\not\\in A} P_{ij} m^A_j.\n\\end{align*}\\]\n\n\n\nFor absorbing Markov chains, we can use matrix methods to efficiently compute hitting probabilities and mean hitting times. The key idea is to rearrange the transition matrix into a canonical form where absorbing states are grouped together.\nSuppose the state space \\(\\ALPHABET X\\) can be partitioned into transient states \\(\\ALPHABET T\\) and absorbing states \\(\\ALPHABET C\\). By reordering the states so that all transient states come first, followed by all absorbing states, the transition matrix \\(P\\) can be written in the canonical form: \\[\n  P = \\MATRIX{ Q & R \\\\ 0 & I}.\n\\] where:\n\n\\(Q\\) is a \\(|\\ALPHABET T| \\times |\\ALPHABET T|\\) matrix containing transition probabilities between transient states,\n\\(R\\) is a \\(|\\ALPHABET T| \\times |\\ALPHABET C|\\) matrix containing transition probabilities from transient states to absorbing states,\n\\(0\\) is a \\(|\\ALPHABET C| \\times |\\ALPHABET T|\\) zero matrix (since absorbing states cannot transition to transient states),\n\\(I\\) is a \\(|\\ALPHABET C| \\times |\\ALPHABET C|\\) identity matrix (since absorbing states stay in themselves).\n\nMulti-step transitions: For the canonical form, we can show by induction that \\[\nP^n = \\MATRIX{ Q^n & (I + Q + Q^2 + \\cdots + Q^{n-1}) R \\\\ 0 & I}.\n  \\] This follows from the fact that matrix multiplication preserves the block structure, and the lower-left block remains zero while the lower-right block remains \\(I\\) for all powers.\nFundamental Matrix: The fundamental matrix \\(N\\) is defined as \\[\n  N = (I - Q)^{-1} = I + Q + Q^2 + Q^3 + \\cdots.\n\\] The series expansion is valid because for transient states, the spectral radius of \\(Q\\) is less than 1, ensuring convergence. The fundamental matrix exists and is well-defined for any Markov chain with at least one absorbing state.\nInterpretation of \\(N_{ij}\\): The entry \\(N_{ij}\\) represents the expected number of visits to transient state \\(j\\) before absorption, starting from transient state \\(i\\). This can be seen from the series expansion: \\[\nN_{ij} = \\sum_{n=0}^∞ [Q^n]_{ij} = \\sum_{n=0}^∞ \\PR_i(X_n = j, \\text{ chain hasn't been absorbed by time } n).\n  \\]\nMean hitting times: Let \\(\\ONES\\) denote a column vector of ones with dimension \\(|\\ALPHABET T|\\). Then the mean hitting times (expected time until absorption) starting from each transient state are given by \\[\n  m = N \\ONES,\n\\] where \\(m_i\\) is the mean time until absorption starting from transient state \\(i\\).\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe mean hitting time \\(m_i\\) equals the expected number of steps spent in transient states before absorption. Since \\(N_{ij}\\) represents the expected number of visits to transient state \\(j\\) starting from transient state \\(i\\), we have \\[\n  m_i = \\sum_{j \\in \\ALPHABET T} N_{ij} = [N \\ONES]_i,\n\\] which gives the desired result in matrix form.\n\n\n\nHitting probabilities: The hitting probabilities (absorption probabilities) are given by \\[\n  h = N R,\n\\] where \\(h_{ij}\\) is the probability of being absorbed in absorbing state \\(j\\) when starting from transient state \\(i\\).\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nStarting from transient state \\(i\\), the chain is absorbed in absorbing state \\(j\\) if and only if it takes some number \\(n \\ge 0\\) of steps within transient states, and then transitions to \\(j\\) in step \\(n+1\\). The probability of being in transient state \\(k\\) after \\(n\\) steps (without being absorbed) is \\([Q^n]_{ik}\\), and the probability of transitioning from \\(k\\) to \\(j\\) is \\(R_{kj}\\). Therefore, \\[\n  h_{ij} = \\sum_{n=0}^∞ \\sum_{k \\in \\ALPHABET T} [Q^n]_{ik} R_{kj} = \\sum_{n=0}^∞ [Q^n R]_{ij} = [N R]_{ij},\n\\] where the last equality follows from \\(N = I + Q + Q^2 + \\cdots\\).\n\n\n\n\nExample 10.15 Consider the gambler’s ruin problem from Example 10.12 with \\(K = 3\\) (i.e., the gambler starts with $\\(1\\) and stops at either $\\(0\\) or $\\(3\\)). Write the transition matrix in canonical form and use the fundamental matrix to compute the probability of ruin and the expected duration of play.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe state space is \\(\\{0, 1, 2, 3\\}\\) where states \\(0\\) and \\(3\\) are absorbing. In the natural ordering \\((0, 1, 2, 3)\\), the transition matrix is: \\[\nP = \\MATRIX{\n  1 & 0 & 0 & 0 \\\\\n  q & 0 & p & 0 \\\\\n  0 & q & 0 & p \\\\\n  0 & 0 & 0 & 1\n}\n\\] where \\(q = 1-p\\). Reordering to put transient states first, we use the ordering \\((1, 2, 0, 3)\\): \\[\nP = \\MATRIX{\n  0 & p & q & 0 \\\\\n  q & 0 & 0 & p \\\\\n  0 & 0 & 1 & 0 \\\\\n  0 & 0 & 0 & 1\n}\n= \\MATRIX{ Q & R \\\\ 0 & I }\n\\] where \\[\nQ = \\MATRIX{ 0 & p \\\\ q & 0 }, \\quad\nR = \\MATRIX{ q & 0 \\\\ 0 & p }.\n\\]\nThe fundamental matrix is: \\[\nN = (I - Q)^{-1} = \\MATRIX{ 1 & -p \\\\ -q & 1 }^{-1}\n= \\frac{1}{1-pq} \\MATRIX{ 1 & p \\\\ q & 1 }.\n\\]\nThe hitting probabilities are: \\[\nh = N R = \\frac{1}{1-pq} \\MATRIX{ 1 & p \\\\ q & 1 } \\MATRIX{ q & 0 \\\\ 0 & p }\n= \\frac{1}{1-pq} \\MATRIX{ q & p^2 \\\\ q^2 & p }.\n\\]\nThus, starting from state \\(1\\), the probability of ruin (absorption in state \\(0\\)) is \\(h_{1,0} = q/(1-pq)\\) and the probability of winning (absorption in state \\(3\\)) is \\(h_{1,3} = p^2/(1-pq)\\).\nThe mean hitting times are: \\[\nm = N \\ONES = \\frac{1}{1-pq} \\MATRIX{ 1 & p \\\\ q & 1 } \\MATRIX{ 1 \\\\ 1 }\n= \\frac{1}{1-pq} \\MATRIX{ 1+p \\\\ q+1 }.\n\\]\nThus, starting from state \\(1\\), the expected duration is \\(m_1 = (1+p)/(1-pq)\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#invariant-distribution",
    "href": "markov-chains.html#invariant-distribution",
    "title": "10  Markov chains",
    "section": "10.5 Invariant Distribution",
    "text": "10.5 Invariant Distribution\n\nA probability distribution \\(π\\) on the state space \\(\\ALPHABET X\\) is called an invariant distribution if it satisfies the equation \\[\n  π = π P.\n\\]\nAn invariant distribution is also called a stationary distribution. If the initial distribution of the Markov chain is \\(π\\), then the distribution remains \\(π\\) at all future times. In other words, if \\(μ^{(0)} = π\\) then \\(μ^{(n)} = π\\) for all \\(n \\ge 0\\).\nConsider the on-off Markov chain from Example 10.4 with transition matrix \\[\n  P = \\MATRIX{ 1 - a & a \\\\ b & 1 - b }.\n\\] This chain has an invariant distribution given by \\(π = [b/(a+b), a/(a+b)]\\) (assuming \\(a+b &gt; 0\\)). One can verify that \\(π = π P\\), which shows that an invariant distribution exists for this chain.\nA finite Markov chain with only one communication class has a unique invariant distribution. Thus, irreducible Markov chains have a unique invariant distribution. Note that aperiodicity is not required. For instance, consider on-off Markov chain from Example 10.4 with \\(a = b = 1\\). This chain is periodic, but has a unique invariant distribution \\(π = [\\frac 12, \\frac 12]\\).\nA Markov chain with more than one closed communicating class has multiple invariant distributions. For example, consider a Markov chain with two absorbing states \\(0\\) and \\(1\\). The transition matrix is \\[\n  P = \\MATRIX{ 1 & 0 \\\\ 0 & 1 }.\n\\]\n\n\n\nMarkov chain with two absorbing states\n\n\nIn this case, any distribution of the form \\(π = [α, 1-α]\\) with \\(α \\in [0,1]\\) is an invariant distribution, since \\[\nπ P = [α, 1-α]\n\\MATRIX{ 1 & 0 \\\\ 0 & 1 }\n= [α, 1-α] = π.\n\\] Thus, there are uncountably many invariant distributions. This happens because the chain consists of two disjoint recurrent classes (the absorbing states \\(0\\) and \\(1\\)), and any probability mass can be put on either class and be preserved.\nMore generally, if a Markov chain has multiple irreducible communicating (closed) classes, each class can have its own invariant distribution (supported only on that class). Any convex combination of these distributions is also an invariant distribution for the whole chain.\nA Markov chain that is irreducible and aperiodic has a special prorperty: for any initial distribution \\(μ^{(0)}\\), we have \\[\\lim_{n \\to ∞} μ^{(n)}_j = π_j, \\quad \\forall j \\in \\ALPHABET X.\\]\nIrreducible and aperiodic Markov chains are also called ergodic. Thus, an ergodic Markov chain converges to its invariant distribution. For this reason, the invariant distribution is also called the limiting distribution or the steady-state distribution.\nComputing invariant distributions. The equation \\(π = π P\\) can be written component-wise as \\[\n  π_j = \\sum_{i \\in \\ALPHABET X} π_i P_{ij}, \\quad \\forall j \\in \\ALPHABET X.\n\\] These equations are called balance equations and can be used to find invariant distributions. Together with the constraint that \\(π\\) is a probability distribution (i.e., \\(\\sum_{j \\in \\ALPHABET X} π_j = 1\\) and \\(π_j \\ge 0\\) for all \\(j\\)), they form a system of linear equations that can be solved to determine the invariant distribution.\nAs an example, let’s compute the invariant distribution of the on-off Markov chain from Example 10.4. The transition matrix is \\[\n  P = \\MATRIX{ 1 - a & a \\\\ b & 1 - b }.\n\\] To find the invariant distribution \\(π = [π_0, π_1]\\), we solve the balance equations \\(π = π P\\): \\[\\begin{align*}\n  π_0 &= π_0 (1-a) + π_1 b, \\\\\n  π_1 &= π_0 a + π_1 (1-b).\n\\end{align*}\\] From the first equation, we get \\(π_0 = π_0 (1-a) + π_1 b\\), which simplifies to \\(π_0 a = π_1 b\\), or equivalently \\(π_1 = (a/b) π_0\\) (assuming \\(b &gt; 0\\)). Using the normalization constraint \\(π_0 + π_1 = 1\\), we have \\[\n  π_0 + \\frac{a}{b} π_0 = 1 \\quad \\Rightarrow \\quad π_0 \\left(1 + \\frac{a}{b}\\right) = 1 \\quad \\Rightarrow \\quad π_0 = \\frac{b}{a+b}.\n\\] Therefore, \\(π_1 = 1 - π_0 = \\frac{a}{a+b}\\). The invariant distribution is \\(π = [b/(a+b), a/(a+b)]\\), which matches the result mentioned in point 3.\nWe can write balance equations directly from the state transition diagram. For each state \\(j\\), the balance equation states that the probability flow into state \\(j\\) equals the probability flow out of state \\(j\\). Consider the three-state Markov chain with transition matrix \\[\n  P = \\MATRIX{ 0.3 & 0.7 & 0 \\\\\n               0 & 0.3 & 0.7 \\\\\n               0.7 & 0 & 0.3 }.\n\\]\n\n\n\nThree-state aperiodic Markov chain\n\n\nThe balance equations, equating the flow into and out of each state, can be written as:\n\nState 0:\n\\[\\begin{align*}\n\\text{Inflow} &= \\pi_2 \\cdot 0.7 \\\\\n\\text{Outflow} &= \\pi_0 \\cdot 0.7\n\\end{align*}\\]\nState 1:\n\\[\\begin{align*}\n\\text{Inflow} &= \\pi_0 \\cdot 0.7 \\\\\n\\text{Outflow} &= \\pi_1 \\cdot 0.7\n\\end{align*}\\]\nState 2:\n\\[\\begin{align*}\n\\text{Inflow} &= \\pi_1 \\cdot 0.7 \\\\\n\\text{Outflow} &= \\pi_2 \\cdot 0.7\n\\end{align*}\\]\n\nThus, the balance equations are given by \\[\\begin{align*}\n  \\pi_0 \\cdot 0.7 = \\pi_2 \\cdot 0.7 \\\\\n  \\pi_1 \\cdot 0.7 = \\pi_0 \\cdot 0.7 \\\\\n  \\pi_2 \\cdot 0.7 = \\pi_1 \\cdot 0.7\n\\end{align*}\\] Together with the normalization constraint \\(π_0 + π_1 + π_2 = 1\\), we get \\(π_0 = π_1 = π_2 = 1/3\\). This illustrates how balance equations can be derived directly from the state diagram by considering probability flows.\nThe above example is an illustration of a more general principle. If \\(P\\) is doubly stochastic (i.e., both row sum and column sums are \\(1\\)), then the invariant distribution is a uniform distribution.\nExercises: Computing Invariant Distributions\n\nExample 10.16 Consider the Markov chain with transition matrix\n\\[\n  P = \\MATRIX{ 0.6 & 0.4 & 0 & 0 \\\\\n               0.3 & 0.7 & 0 & 0 \\\\\n               0.2 & 0.3 & 0.3 & 0.2 \\\\\n               0.1 & 0.4 & 0.1 & 0.4 }.\n\\]\n\n\n\nMarkov chain with transient states and closed class\n\n\nThis chain has states \\(\\{0, 1, 2, 3\\}\\) where states \\(\\{0, 1\\}\\) form a closed communicating class and states \\(\\{2, 3\\}\\) are transient. Find all invariant distributions for this chain.\n\n\nExample 10.17 Consider the Markov chain with transition matrix\n\\[\n  P = \\MATRIX{ 1 & 0 & 0 \\\\\n               0.2 & 0.4 & 0.4 \\\\\n               0 & 0 & 1 }.\n\\]\n\n\n\nMarkov chain with absorbing states\n\n\nNote that this chain is not irreducible (it has two absorbing states). Compute all possible invariant distributions.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#limiting-distribution",
    "href": "markov-chains.html#limiting-distribution",
    "title": "10  Markov chains",
    "section": "10.6 Limiting Distribution",
    "text": "10.6 Limiting Distribution\n\nSuppose a finite Markov chain has the property that starting from any initial distribution, the distribution of \\(X_n\\) converges to some distribution \\(\\pi\\) as \\(n \\to \\infty\\): \\[\n\\lim_{n \\to \\infty} P^{(n)}_{i, j} = \\pi_j \\qquad \\text{for all } i, j \\in \\ALPHABET X.\n\\] Then, \\(\\pi\\) is an invariant distribution.\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nWe start by showing that \\(π\\) must be a valid probability distribution. In particular, \\[\n\\sum_{j \\in \\ALPHABET X} π_j =\n\\sum_{j \\in \\ALPHABET X} \\lim_{n \\to ∞} P^{(n)}_{ij} =\n\\lim_{n \\to ∞} \\sum_{j \\in \\ALPHABET X} P^{(n)}_{ij} = 1\n\\]\nWe now show that \\(π\\) is an invariant distribution. \\[\nπ_j = \\lim_{n \\to ∞} P^{(n)}_{ij}\n= \\lim_{n \\to ∞} \\sum_{k \\in \\ALPHABET X} P^{(n-1)}_{ik} P_{kj}\n= \\sum_{k \\in \\ALPHABET X} \\lim_{n \\to ∞} P^{(n-1)}_{ik} P_{kj}\n= \\sum_{k \\in \\ALPHABET X} π_k P_{kj}\n\\] Thus, \\(π\\) is an invariant distribution.\n\n\n\nAs discussed above, limiting distributions exist for ergodic Markov chains (i.e., irreducible and aperiodic Markov chains): for any initial distribution \\(μ^{(0)}\\), we have \\[\\lim_{n \\to ∞} μ^{(n)}_j = π_j, \\quad \\forall j \\in \\ALPHABET X.\\]\nThis is equivalent to the statement that for all \\(i, j \\in \\ALPHABET X\\), \\[\\lim_{n \\to \\infty} P^{(n)}_{ij} = π_j.\\] This is an consequence of :Perron Frobenius Theorem. In matrix form, we have \\[\n\\lim_{n \\to \\infty} P^n =\n\\MATRIX{\n   \\pi \\\\\n   \\pi \\\\\n   \\vdots \\\\\n   \\pi\n}\n\\] where each row of the limiting matrix is the invariant distribution \\(π\\). That is, for large \\(n\\), every row of \\(P^n\\) approaches \\(π\\).\nIn fact, we have a stronger result: the Strong Law of Large Numbers (SLLN) for (finite) Markov chains. Let \\(\\{X_n\\}_{n \\ge 0}\\) be an irreducible and aperiodic (i.e., ergodic) Markov chain. Then, for any initial distribution, \\[\n\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=1}^N \\IND\\{X_n = j\\} \\xrightarrow{a.s.} \\pi_j, \\quad \\forall j \\in \\ALPHABET X.\n\\] This is known as ergodic property: sample average equals ensemble average.\nAn immediate consequence of the ergodic property is that given an ergodic Markov chain \\(\\{X_n\\}_{n \\ge 0}\\) and any function \\(f \\colon \\ALPHABET X \\to \\reals\\), we have \\[\n\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=1}^N f(X_n) \\xrightarrow{a.s.} \\sum_{j\\in \\ALPHABET X} \\pi_j f(j).\n\\]\nIf the chain is periodic with period \\(d &gt; 1\\) (i.e., not aperiodic), the limiting distribution \\(\\lim_{n \\to \\infty} P^n\\) does not exist in the usual sense, as the transition probabilities continue to oscillate. However, if the chain is irreducible, it still possesses a unique invariant distribution \\(π\\). In the periodic case, results like ergodicity and the law of large numbers still hold if we consider time averages over subsequences that are integer multiples of the period \\(d\\), i.e., convergence happens along these subsequences, not along every \\(n\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#exercises",
    "href": "markov-chains.html#exercises",
    "title": "10  Markov chains",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 10.1 (Time reversal of Markov chains) Let \\(\\{X_n\\}_{n \\ge 0}\\) be a Markov chain. Show that for any \\(N &gt; n\\), \\[\n  \\PR(X_n = x_n \\mid X_{n+1:N} = x_{n+1:N})\n  = \\PR(X_n = x_n \\mid X_{n+1} = x_{n+1}).\n\\] Thus, a time reversed Markov chain is also Markov.\n\n\nExercise 10.2 Suppose \\(\\{X_n\\}_{n \\ge 0}\\) is a Markov chain with transition matrix \\(P\\). For a fixed positive integer \\(k\\), define \\(Y_n = X_{kn}\\). Show that \\(\\{Y_n\\}_{n \\ge 0}\\) is a Markov chain with transition matrix \\(P^k\\).\n\n\nExercise 10.3 Suppose a (6-sided) die is ‘fixed’ so that two consecutive rolls cannot have the same outcome. In particular, if the outcome of a roll is \\(i\\), then the next roll cannot be \\(i\\); all \\(5\\) other outcomes are equally likely.\n\nModel the above as a Markov chain.\nIf the outcome of the first roll is \\(1\\), what is the probability that the outcome of the \\(n\\)th roll is also \\(1\\)?",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#further-reading",
    "href": "markov-chains.html#further-reading",
    "title": "10  Markov chains",
    "section": "Further Reading",
    "text": "Further Reading\n\nGrinstead and Snell, Introduction to Probability, Chapter 11 has an excellent treatment of finite Markov chains. Has interesting exercises.\nKemeny and Snell, Finite Markov Chains, Chapter 2–4.\nKemeny, Snell, and Knapp, Denumerable Markov Chains, a slightly advanced version of the previous book.\nDoyle and Snell, Random Walks and Electrical Networks shows a facinating relationship between random walks and circuits!",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "gaussian-processes.html",
    "href": "gaussian-processes.html",
    "title": "11  Gaussian processes",
    "section": "",
    "text": "11.1 Gaussian processes",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gaussian processes</span>"
    ]
  },
  {
    "objectID": "gaussian-processes.html#gaussian-processes",
    "href": "gaussian-processes.html#gaussian-processes",
    "title": "11  Gaussian processes",
    "section": "",
    "text": "A discrete-time stochastic process \\(\\{X_n\\}_{n \\ge 0}\\) is called a Gaussian process if for any finite collection of time indices \\(\\{n_1, n_2, \\dots, n_k\\}\\) with \\(n_i \\ge 0\\), the random vector \\((X_{n_1}, X_{n_2}, \\dots, X_{n_k})\\) has a multivariate Gaussian (normal) distribution.\nEquivalently, a discrete-time stochastic process \\(\\{X_n\\}_{n \\ge 0}\\) is a Gaussian process if every finite linear combination of the random variables \\(X_n\\) is a Gaussian random variable.\nSince a multivariate Gaussian distribution is completely characterized by its mean vector and covariance matrix, a Gaussian process is completely characterized by:\n\nMean function: \\(μ_X(n) = \\EXP[X_n]\\)\nCovariance function: \\(R_X(m,n) = \\COV(X_m, X_n) = \\EXP[(X_m - μ_X(m))(X_n - μ_X(n))]\\)\n\nRecall that a stochastic process is strict sense stationary if for any finite collection of time indices \\(\\{n_1, n_2, \\dots, n_k\\}\\) with \\(n_i \\ge 0\\), the random vector \\((X_{n_1}, X_{n_2}, \\dots, X_{n_k})\\) has the same distribution as the random vector \\((X_{n_1 + h}, X_{n_2 + h}, \\dots, X_{n_k + h})\\) for any \\(h \\ge 0\\). For a Gaussian process, this is equivalent to the mean function being constant and the covariance function depending only on the time difference, i.e., \\[\nμ_X(n) = μ_X \\quad \\text{for all } n \\ge 0, \\quad R_X(m,n) = R_X(|n-m|) \\quad \\text{for all } m, n \\ge 0.\n\\] An implication is that the variance of \\(X_n\\) does not depend on \\(n\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gaussian processes</span>"
    ]
  },
  {
    "objectID": "gaussian-processes.html#properties-of-covariance-functions",
    "href": "gaussian-processes.html#properties-of-covariance-functions",
    "title": "11  Gaussian processes",
    "section": "11.2 Properties of covariance functions",
    "text": "11.2 Properties of covariance functions\n\nSymmetry: \\(R_X(m,n) = R_X(n,m)\\) for all \\(m, n \\ge 0\\).\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\n\\(R_X(m,n) = \\EXP[(X_m - μ_X(m))(X_n - μ_X(n))] = R_X(n,m)\\) by commutativity of multiplication.\n\n\n\nNon-negative definiteness: For any finite collection \\(\\{n_1, \\dots, n_k\\}\\) with \\(n_i \\ge 0\\) and any real numbers \\(a_1, \\dots, a_k\\), we have \\[\n\\sum_{i=1}^k \\sum_{j=1}^k a_i a_j R_X(n_i, n_j) \\ge 0.\n\\]\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nConsider the random variable \\(Y = \\sum_{i=1}^k a_i (X_{n_i} - μ_X(n_i))\\). Then \\[\\begin{align*}\n\\EXP[Y^2] &= \\EXP\\left[\\left(\\sum_{i=1}^k a_i (X_{n_i} - μ_X(n_i))\\right)^2\\right] \\\\\n&= \\EXP\\left[\\sum_{i=1}^k \\sum_{j=1}^k a_i a_j (X_{n_i} - μ_X(n_i))(X_{n_j} - μ_X(n_j))\\right] \\\\\n&= \\sum_{i=1}^k \\sum_{j=1}^k a_i a_j \\EXP[(X_{n_i} - μ_X(n_i))(X_{n_j} - μ_X(n_j))] \\\\\n&= \\sum_{i=1}^k \\sum_{j=1}^k a_i a_j R_X(n_i, n_j).\n\\end{align*}\\] Since \\(\\EXP[Y^2] = \\VAR(Y) \\ge 0\\) (variance is always non-negative), we have \\[\n\\sum_{i=1}^k \\sum_{j=1}^k a_i a_j R_X(n_i, n_j) \\ge 0.\n\\]\n\n\n\nCauchy-Schwarz inequality: \\(|R_X(m,n)|^2 \\le R_X(m,m) R_X(n,n)\\).\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThis is a direct consequence of the Cauchy-Schwarz inequality for random variables: \\[\n|\\EXP[YZ]|^2 \\leq \\EXP[Y^2]\\EXP[Z^2],\n\\] where \\(Y = X_m - μ_X(m)\\) and \\(Z = X_n - μ_X(n)\\). Thus, \\[\n|R_X(m,n)|^2 \\le R_X(m,m) R_X(n,n).\n\\]\n\n\n\nThe variance function is \\(R_X(n,n) = \\VAR(X_n) \\ge 0\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gaussian processes</span>"
    ]
  },
  {
    "objectID": "gaussian-processes.html#examples-of-gaussian-processes",
    "href": "gaussian-processes.html#examples-of-gaussian-processes",
    "title": "11  Gaussian processes",
    "section": "11.3 Examples of Gaussian processes",
    "text": "11.3 Examples of Gaussian processes\n\n// Standard Normal variate using Box-Muller transform.\nfunction gaussianRandom(mean=0, stdev=1) {\n    const u = 1 - Math.random();\n    const v = Math.random();\n    const z = Math.sqrt( -2.0 * Math.log( u ) ) * Math.cos( 2.0 * Math.PI * v );\n    return z * stdev + mean;\n}\n\n\n\n\n\n\n\nExample 11.1 (Cosine with Gaussian amplitude) Consider a process \\(\\{X_n\\}_{n \\ge 0}\\) defined as: \\[\nX_n = U \\cos(\\Omega n) + V \\sin(\\Omega n)\n\\] where \\(\\Omega \\in \\mathbb{R}\\) is a constant frequency, and \\(U\\) and \\(V\\) are independent Gaussian random variables with \\(U, V \\sim \\mathcal{N}(0, \\sigma^2)\\).\nA sample path of this process is shown below.\n\nviewof rerun_cosine_gauss = Inputs.button(\"Generate sample path\")\n\npoints_cosine_gauss = {\n    rerun_cosine_gauss\n    const N = 120\n    const sigma = 1.0\n    const f = 0.1\n    const U = gaussianRandom(0, sigma)\n    const V = gaussianRandom(0, sigma)\n    var points = new Array(N+1)\n    for(var n=0; n &lt;= N; n++) {\n      var val = U * Math.cos(2 * Math.PI * f * n) + V * Math.sin(2 * Math.PI * f * n)\n      points[n] = { n: n, X_n: val }\n    }\n    return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  height: 150,\n  y: {domain: [-2, 2]},\n  marks: [\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    Plot.line(points_cosine_gauss, {x: \"n\", y: \"X_n\", strokeWidth: 1.5}),\n    Plot.dot(points_cosine_gauss, {x: \"n\", y: \"X_n\", fill: \"currentColor\", r: 2}),\n  ]\n})\n\n\n\n\n\n\nFind the mean and covariance function of the process. Is the process stationary?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nMean function:\n\\[\\begin{align*}\n\\mu_X(n) &= \\EXP[X_n] = \\EXP[U \\cos(\\Omega n) + V \\sin(\\Omega n)] \\\\\n&= \\EXP[U] \\cos(\\Omega n) + \\EXP[V] \\sin(\\Omega n) \\\\\n&= 0\n\\end{align*}\\] since \\(\\EXP[U] = \\EXP[V] = 0\\).\nCovariance function:\n\\[\\begin{align*}\nR_X(m,n) &= \\EXP[X_m X_n] \\\\\n&= \\EXP[(U \\cos(\\Omega m) + V \\sin(\\Omega m))(U \\cos(\\Omega n) + V \\sin(\\Omega n))] \\\\\n&= \\EXP[U^2] \\cos(\\Omega m) \\cos(\\Omega n) + \\EXP[UV] \\cos(\\Omega m) \\sin(\\Omega n) \\\\\n&\\quad + \\EXP[VU] \\sin(\\Omega m) \\cos(\\Omega n) + \\EXP[V^2] \\sin(\\Omega m) \\sin(\\Omega n)\n  \\\\\n&= \\sigma^2 [\\cos(\\Omega m) \\cos(\\Omega n) + \\sin(\\Omega m) \\sin(\\Omega n)] \\\\\n&= \\sigma^2 \\cos(\\Omega (m-n))\n\\end{align*}\\] using the identity \\(\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b)\\).\nSince cosine is even: \\[\nR_X(m,n) = \\sigma^2 \\cos(\\Omega |m-n|)\n\\]\nThus, the process is sationary.\n\n\n\n\nExample 11.2 (White noise process (i.i.d. Gaussian)) A white noise process \\(\\{W_n\\}_{n \\ge 0}\\) is a Gaussian process where \\(\\{W_n\\}_{n \\ge 0}\\) is an i.i.d. sequence with \\(W_n \\sim \\mathcal{N}(0, \\sigma^2)\\) for some \\(\\sigma &gt; 0\\).\nA sample path of white noise is shown below.\n\nviewof rerun_white = Inputs.button(\"Generate sample path\")\n\npoints_white = {\n    rerun_white\n    const N = 120\n    const sigma = 1.0\n    var points = new Array(N)\n    for(var n=0; n &lt; N; n++) {\n      var val = gaussianRandom(0, sigma)\n      points[n] = { n: n, W_n: val }\n    }\n    return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  height: 150,\n  marks: [\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    Plot.line(points_white, {x: \"n\", y: \"W_n\", strokeWidth: 1}),\n    Plot.dot(points_white, {x: \"n\", y: \"W_n\", fill: \"currentColor\", r: 2}),\n  ]\n})\n\n\n\n\n\n\nFind the mean and covariance function of the process. Is the process stationary?\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nMean function:\nSince \\(W_n \\sim \\mathcal{N}(0, \\sigma^2)\\) for all \\(n \\ge 0\\), we have: \\[\n\\mu_W(n) = \\EXP[W_n] = 0 \\quad \\text{for all } n \\ge 0\n\\]\nCovariance function:\nFor \\(m, n \\ge 0\\):\n\nIf \\(m = n\\): \\(R_W(m,n) = \\COV(W_m, W_n) = \\VAR(W_n) = \\sigma^2\\)\nIf \\(m \\neq n\\): Since \\(W_m\\) and \\(W_n\\) are independent, \\(R_W(m,n) = \\COV(W_m, W_n) = 0\\)\n\nTherefore: \\[\nR_W(m,n) = \\sigma^2 \\IND_{\\{m=n\\}} \\quad \\text{for } m, n \\ge 0\n\\] where \\(\\IND_{\\{m=n\\}}\\) is the indicator function that equals 1 when \\(m = n\\) and 0 otherwise.\n\n\n\n\n\nExample 11.3 (Gaussian random walk) A Gaussian random walk \\(\\{X_n\\}_{n \\ge 0}\\) is a Gaussian process defined by: \\[\nX_0 = 0, \\quad X_{n+1} = X_n + W_n, \\quad n \\ge 0\n\\] where \\(\\{W_n\\}_{n \\ge 0}\\) is an i.i.d. sequence with \\(W_n \\sim \\mathcal{N}(0, σ^2)\\) for some \\(σ &gt; 0\\).\nA sample path of a Gaussian random walk is shown below.\n\nviewof rerun_rw = Inputs.button(\"Generate sample path\")\n\npoints_rw = {\n    rerun_rw\n    const N = 120\n    const sigma = 1.0\n    var points = new Array(N+1)\n    var X = 0\n    points[0] = { n: 0, X_n: 0 }\n    for(var n=1; n &lt;= N; n++) {\n      var W = gaussianRandom(0, sigma)\n      X = X + W\n      points[n] = { n: n, X_n: X }\n    }\n    return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  height: 150,\n  marks: [\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    Plot.line(points_rw, {x: \"n\", y: \"X_n\", strokeWidth: 1.5}),\n    Plot.dot(points_rw, {x: \"n\", y: \"X_n\", fill: \"currentColor\", r: 2}),\n  ]\n})\n\n\n\n\n\n\nFind the mean and covariance function of the process. Is the process sationary?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nMean function:\nSince \\(X_0 = 0\\) and \\(X_n = \\sum_{k=0}^{n-1} W_k\\) for \\(n \\ge 1\\), we have: \\[\n\\mu_X(n) = \\EXP[X_n] = \\EXP\\left[\\sum_{k=0}^{n-1} W_k\\right] = \\sum_{k=0}^{n-1} \\EXP[W_k] = 0 \\quad \\text{for all } n \\ge 0\n\\]\nCovariance function:\nFor \\(m, n \\ge 0\\), without loss of generality, assume \\(m \\le n\\). Then: \\[\\begin{align*}\nR_X(m,n) &= \\COV(X_m, X_n) = \\EXP[X_m X_n] \\\\\n&= \\EXP\\left[\\left(\\sum_{i=0}^{m-1} W_i\\right)\\left(\\sum_{j=0}^{n-1} W_j\\right)\\right] \\\\\n&= \\EXP\\left[\\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} W_i W_j\\right] \\\\\n&= \\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} \\EXP[W_i W_j]\n\\end{align*}\\]\nSince \\(W_i\\) and \\(W_j\\) are independent for \\(i \\neq j\\) and \\(\\EXP[W_i] = 0\\), we have \\(\\EXP[W_i W_j] = 0\\) for \\(i \\neq j\\) and \\(\\EXP[W_i^2] = \\sigma^2\\) for \\(i = j\\). Therefore: \\[\nR_X(m,n) = \\sum_{i=0}^{m-1} \\EXP[W_i^2] = m \\sigma^2\n\\]\nFor the general case \\(m, n \\ge 0\\): \\[\nR_X(m,n) = \\sigma^2 \\min(m,n)\n\\] Thus, the process is not sationary.\n\n\n\n\nExample 11.4 (AR(1) process) An AR(1) process (also called discrete-time Ornstein-Uhlenbeck process) \\(\\{X_n\\}_{n \\ge 0}\\) is a Gaussian process defined by the recursion: \\[\nX_{n+1} = a X_n + W_n, \\quad n \\ge 0\n\\] where \\(|a| &lt; 1\\) and \\(\\{W_n\\}_{n \\ge 0}\\) is an i.i.d. sequence with \\(W_n \\sim \\mathcal{N}(0, σ^2)\\).\nA sample path of an AR(1) process is shown below.\n\nviewof rerun_ar1 = Inputs.button(\"Generate sample path\")\n\npoints_ar1 = {\n    rerun_ar1\n    const N = 120\n    const sigma = 1.0\n    const a = 0.8\n    const var0 = sigma*sigma / (1 - a*a)\n    var points = new Array(N+1)\n    var X = gaussianRandom(0, Math.sqrt(var0))\n    points[0] = { n: 0, X_n: X }\n    for(var n=1; n &lt;= N; n++) {\n      var W = gaussianRandom(0, sigma)\n      X = a * X + W\n      points[n] = { n: n, X_n: X }\n    }\n    return points\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  height: 150,\n  marks: [\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    Plot.line(points_ar1, {x: \"n\", y: \"X_n\", strokeWidth: 1.5}),\n    Plot.dot(points_ar1, {x: \"n\", y: \"X_n\", fill: \"currentColor\", r: 2}),\n  ]\n})\n\n\n\n\n\n\nFind the mean and covariance function of the process assuming \\(X_0 = x_0\\).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nMean function:\nThe mean function satisfies the recursion: \\[\n\\mu_X(n) = a \\mu_X(n-1)\n\\] with \\(\\mu_X(0) = x_0\\). Therefore, \\[\n\\mu_X(n) = a^n x_0\n\\]\nCovariance function:\nWe first consider the case \\(n = m\\). Then: \\[\nR_X(n,n) = \\EXP[X_n^2] = \\EXP[(a X_{n-1} + W_n)^2] = a^2 \\EXP[X_{n-1}^2] + 2a \\EXP[X_{n-1} W_n] + \\EXP[W_n^2]\n\\] Since \\(X_{n-1}\\) and \\(W_n\\) are independent, we have \\(\\EXP[X_{n-1} W_n] = \\EXP[X_{n-1}] \\EXP[W_n] = 0\\). Therefore:\n\\[\nR_X(n,n) = a^2 R_X(n-1,n-1) + \\sigma^2\n\\] with \\(R_X(0,0) = 0\\). By induction, we have: \\[\nR_X(n,n) = \\sigma^2 \\sum_{k=0}^{n-1} a^{2k}.\n\\]\nNow consider the case \\(n \\neq m\\). Without loss of generality, assume \\(n &lt; m\\). Using the recursion \\(X_m = a X_{m-1} + W_{m-1}\\): \\[\nR_X(n,m) = \\EXP[X_n X_m] = \\EXP[X_n (a X_{m-1} + W_{m-1})] = a \\EXP[X_n X_{m-1}] + \\EXP[X_n W_{m-1}]\n\\] Since \\(X_n\\) and \\(W_{m-1}\\) are independent (because \\(m-1 \\ge n\\) when \\(n &lt; m\\)), we have \\(\\EXP[X_n W_{m-1}] = 0\\). Therefore: \\[\nR_X(n,m) = a R_X(n, m-1)\n\\] with the base case \\(R_X(n,n)\\) computed above. Therefore, we have \\[\nR_X(n,m) = a^{m-n} R_X(n,n) = a^{m-n} \\sigma^2 \\sum_{k=0}^{n-1} a^{2k}\n\\]\n\n\n\n\n\n\n\n\n\nTipInvariant distribution and relationship to Markov chains\n\n\n\nObserve that the process \\(\\{X_n\\}_{n \\ge 0}\\) satisfies the Markov property. So, it is also called a Gauss Markov process. From the results of Example 11.4, observe that \\[\n   \\lim_{n \\to \\infty} \\mu_X(n) = 0 \\quad \\text{and} \\quad \\lim_{n \\to \\infty} R_X(n,n) = \\frac{σ^2}{1-a^2}\n\\] for \\(m, n \\ge 0\\). The above limits show that the distribution of \\(X_n\\) converges to an invariant distribution with mean 0 and variance \\(\\frac{σ^2}{1-a^2}\\).\nAs was the case for Markov chains, if we start with the invariant distribution, i.e., \\(X_0 \\sim \\mathcal{N}(0, \\frac{σ^2}{1-a^2})\\), the process will remain in the invariant distribution for all time. Furthermore, the process is a strict sense stationary process because \\[\n   \\mu_X(n) = 0, \\quad \\forall n \\ge 0\n\\] and using similar recursions as above, we have \\[\nR_X(n, m) = a^{|n-m|} R_X(n,n) = a^{|n-m|} \\frac{σ^2}{1-a^2}.\n\\]\n\n\nThe above ideas can be generalized to vector valued processes as well.\n\nIn general, let \\(\\{W_n\\}_{n \\ge 0}\\) be a vector valued white noise process, where \\(W_n \\sim \\mathcal{N}(0, Σ_W)\\). Then \\(μ_X(n) = 0\\) and \\(R_X(n,m) = Σ_W \\IND\\{n = m\\}\\). The process is called a standard White noise process if \\(Σ_W = I\\).\nConsider a stochastic process \\(\\{X_n\\}_{n \\ge 0}\\) defined as \\[\n  X_{n+1} = A_n X_n + B_n W_n.\n\\] where \\(A_n\\) and \\(B_n\\) are deterministic matrices and \\(\\{W_n\\}_{n \\ge 0}\\) is standard White noise. Then, \\(\\{X_n\\}_{n \\ge 0}\\) is Gauss Markov process and it can be shown that for any \\(\\ell &lt; n &lt; m\\), we have \\[\n  R_X(\\ell, m) = R_X(\\ell,n) R_X(n,n)^{-1} R_X(n,m).\n\\]\nThe process is stationary when \\(A_n\\) and \\(B_n\\) do not depend on \\(n\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gaussian processes</span>"
    ]
  },
  {
    "objectID": "gaussian-processes.html#review-of-discrete-time-lti-systems",
    "href": "gaussian-processes.html#review-of-discrete-time-lti-systems",
    "title": "11  Gaussian processes",
    "section": "11.4 Review of discrete-time LTI systems",
    "text": "11.4 Review of discrete-time LTI systems\n\nConsider an LTI system with impulse resposnse \\(\\{h_k\\}_{k \\ge 0}\\). The transfer function of the system is given by the Z-transform of the impulse response: \\[\nH(z) = \\sum_{k=0}^∞ h_k z^{-k}\n\\]\nThe system is stable if \\[ \\sum_{k=0}^∞ |h_k| &lt; ∞, \\] Equivalently, the ROC of \\(H(z)\\) contains the unit circle \\(|z| = 1\\), i.e., all poles of \\(H(z)\\) lie inside the unit circle.\nThe Z-transform evaluated on the unit circle is called the Fourier transform, i.e., \\[\n   H(e^{j\\omega}) = \\sum_{k=-\\infty}^∞ h_k e^{-j\\omega k}\n\\] Note that we have a clash of notation. In Signals and Systems, \\(\\omega\\) is used to denote the frequency in radians per second, while in probabiblity \\(\\omega\\) is used to denote events in the sample space. To avoid confusion, we we will work use \\(\\Omega\\) to denote the frequency (it still clashes with the notation for sample space, but the meaning should be clear from the context). Thus, the Fourier transform is given by \\[\n   H(e^{j\\Omega}) = \\sum_{k=-\\infty}^∞ h_k e^{-j\\Omega k}\n\\]\nThe inverse Fourier transform is given by: \\[\n   h_k = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} H(e^{j\\Omega}) e^{j\\Omega k} d\\Omega\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gaussian processes</span>"
    ]
  },
  {
    "objectID": "gaussian-processes.html#stationary-gaussian-processes-through-lti-systems",
    "href": "gaussian-processes.html#stationary-gaussian-processes-through-lti-systems",
    "title": "11  Gaussian processes",
    "section": "11.5 Stationary Gaussian processes through LTI systems",
    "text": "11.5 Stationary Gaussian processes through LTI systems\n\nWhen a stationary Gaussian process is passed through a linear time-invariant (LTI) system, the output is also a stationary Gaussian process. This is a fundamental result in signal processing and stochastic control.\nSuppose a stationary Gaussian process \\(\\{X_n\\}_{n ≥ 0}\\) is passed through an LTI system \\(H(z)\\). For every \\(ω ∈ Ω\\), the output process is given by \\[\nY_n(ω) = \\sum_{k=0}^∞ h_k X_{n-k}(ω)\n  \\] which is a sum of Gaussian random variables, and hence is Gaussian.\nThe output \\(\\{Y_n\\}_{n ≥ 0}\\) is a stationary Gaussian process.\n\n\n\n\n\n\nNoteProof\n\n\n\nThe mean function of the output process is given by \\[\n  μ_Y(n) = \\EXP[Y_n] = \\EXP\\left[\\sum_{k=0}^∞ h_k X_{n-k}\\right] = \\sum_{k=0}^∞ h_k \\EXP[X_{n-k}] = \\sum_{k=0}^∞ h_k μ_X = μ_X \\sum_{k=0}^∞ h_k = μ_X H(1)\n\\] which does not depend on \\(n\\).\nThe covariance function of the output process is given by \\[\n  \\begin{align*}\n    R_Y(n,m)\n      &= \\EXP[Y_n Y_m] \\\\\n      &= \\EXP\\left[\\sum_{k=0}^\\infty h_k X_{n-k} \\sum_{j=0}^\\infty h_j X_{m-j}\\right] \\\\\n      &= \\sum_{k=0}^\\infty \\sum_{j=0}^\\infty h_k h_j \\EXP[X_{n-k} X_{m-j}] \\\\\n      &= \\sum_{k=0}^\\infty \\sum_{j=0}^\\infty h_k h_j R_X(|(n-k)-(m-j)|) \\\\\n      &= \\sum_{k=0}^\\infty \\sum_{j=0}^\\infty h_k h_j R_X(|(n-m)-(k-j)|)\n  \\end{align*}\n\\] which only depends on \\(n-m\\).\nTherefore, the output process is a stationary Gaussian process.\n\n\nFor a stationary Gaussian process \\(\\{X_n\\}_{n \\ge 0}\\) with covariance function \\(R_X(k) = R_X(|n-m|)\\) where \\(k = |n-m|\\), the power spectral density (PSD) \\(S_X(\\omega)\\) is defined as the discrete-time Fourier transform (DTFT) of the covariance function: \\[\nS_X(\\omega) = \\sum_{n=-\\infty}^{\\infty} R_X(n) e^{-j\\omega n}, \\quad \\omega \\in [-\\pi, \\pi]\n\\] where \\(j = \\sqrt{-1}\\) and \\(R_X(-n) = R_X(n)\\) by symmetry of the covariance function. For this infinite sum to converge, we require that \\(\\sum_{n=-\\infty}^{\\infty} |R_X(n)| &lt; \\infty\\) (absolute summability of the covariance function).\nThe covariance function can be recovered from the power spectral density via the inverse DTFT: \\[\nR_X(n) = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} S_X(\\omega) e^{j\\omega n} \\, d\\omega, \\quad n \\in \\mathbb{Z}\n\\] This establishes a Fourier transform pair between the covariance function in the time domain and the power spectral density in the frequency domain.\nProperties of power spectral density:\n\nNon-negativity: \\(S_X(\\omega) \\ge 0\\) for all \\(\\omega \\in [-\\pi, \\pi]\\)\nSymmetry: \\(S_X(\\omega) = S_X(-\\omega)\\) (for real-valued processes)\nTotal power: The variance of the process is given by \\[\n\\VAR(X_n) = R_X(0) = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} S_X(\\omega) \\, d\\omega\n\\]\n\nLinear filtering: If \\(\\{Y_n\\}_{n \\ge 0}\\) is the output of a stable LTI system with frequency response \\(H(\\omega)\\) and input \\(\\{X_n\\}_{n \\ge 0}\\), then \\[\nS_Y(\\omega) = |H(\\omega)|^2 S_X(\\omega)\n\\] where \\(H(\\omega) = \\sum_{n=-\\infty}^{\\infty} h_n e^{-j\\omega n}\\) is the DTFT of the impulse response \\(\\{h_n\\}_{n=-\\infty}^{\\infty}\\).\n\n\n\n\n\n\nNoteProof\n\n\n\n\n\nThe output process is given by the convolution: \\[\nY_n = \\sum_{k=-\\infty}^{\\infty} h_k X_{n-k}\n\\]\nThe covariance function of the output is: \\[\n\\begin{align*}\nR_Y(n) &= \\COV(Y_0, Y_n) = \\EXP[Y_0 Y_n] - \\EXP[Y_0]\\EXP[Y_n] \\\\\n&= \\EXP\\left[\\sum_{k=-\\infty}^{\\infty} h_k X_{-k} \\sum_{j=-\\infty}^{\\infty} h_j X_{n-j}\\right] - \\mu_Y^2 \\\\\n&= \\sum_{k=-\\infty}^{\\infty} \\sum_{j=-\\infty}^{\\infty} h_k h_j \\EXP[X_{-k} X_{n-j}] - \\mu_Y^2\n\\end{align*}\n\\]\nSince \\(\\{X_n\\}\\) is stationary with mean \\(\\mu_X\\) and covariance \\(R_X(m)\\), we have: \\[\n\\EXP[X_{-k} X_{n-j}] = R_X((n-j) - (-k)) + \\mu_X^2 = R_X(n - j + k) + \\mu_X^2\n\\]\nTherefore: \\[\n\\begin{align*}\nR_Y(n) &= \\sum_{k=-\\infty}^{\\infty} \\sum_{j=-\\infty}^{\\infty} h_k h_j (R_X(n - j + k) + \\mu_X^2) - \\mu_Y^2 \\\\\n&= \\sum_{k=-\\infty}^{\\infty} \\sum_{j=-\\infty}^{\\infty} h_k h_j R_X(n - j + k) + \\mu_X^2 \\sum_{k=-\\infty}^{\\infty} h_k \\sum_{j=-\\infty}^{\\infty} h_j - \\mu_Y^2\n\\end{align*}\n\\]\nSince \\(\\mu_Y = \\mu_X H(1)\\) and \\(H(1) = \\sum_{k=-\\infty}^{\\infty} h_k\\), the constant terms cancel, leaving: \\[\nR_Y(n) = \\sum_{k=-\\infty}^{\\infty} \\sum_{j=-\\infty}^{\\infty} h_k h_j R_X(n - j + k)\n\\]\nMaking the substitution \\(\\ell = j - k\\), we get: \\[\nR_Y(n) = \\sum_{k=-\\infty}^{\\infty} \\sum_{\\ell=-\\infty}^{\\infty} h_k h_{k+\\ell} R_X(n - \\ell) = \\sum_{\\ell=-\\infty}^{\\infty} R_X(n - \\ell) \\sum_{k=-\\infty}^{\\infty} h_k h_{k+\\ell}\n\\]\nThe inner sum \\(\\sum_{k=-\\infty}^{\\infty} h_k h_{k+\\ell}\\) is the autocorrelation of the impulse response, which we denote as \\(r_h(\\ell)\\). This is the convolution of \\(h_k\\) with \\(h_{-k}\\): \\[\nr_h(\\ell) = \\sum_{k=-\\infty}^{\\infty} h_k h_{k+\\ell} = (h * \\tilde{h})(\\ell)\n\\] where \\(\\tilde{h}_k = h_{-k}\\) is the time-reversed impulse response.\nTherefore: \\[\nR_Y(n) = \\sum_{\\ell=-\\infty}^{\\infty} r_h(\\ell) R_X(n - \\ell) = (r_h * R_X)(n)\n\\]\nTaking the DTFT of both sides and using the convolution property of the DTFT: \\[\nS_Y(\\omega) = \\mathcal{F}\\{r_h\\}(\\omega) \\cdot S_X(\\omega)\n\\]\nwhere \\(\\mathcal{F}\\{r_h\\}(\\omega)\\) is the DTFT of \\(r_h(\\ell)\\). The DTFT of the autocorrelation \\(r_h(\\ell)\\) is: \\[\n\\mathcal{F}\\{r_h\\}(\\omega) = \\sum_{\\ell=-\\infty}^{\\infty} r_h(\\ell) e^{-j\\omega \\ell} = \\sum_{\\ell=-\\infty}^{\\infty} \\sum_{k=-\\infty}^{\\infty} h_k h_{k+\\ell} e^{-j\\omega \\ell}\n\\]\nMaking the substitution \\(m = k + \\ell\\): \\[\n\\begin{align*}\n\\mathcal{F}\\{r_h\\}(\\omega) &= \\sum_{k=-\\infty}^{\\infty} \\sum_{m=-\\infty}^{\\infty} h_k h_m e^{-j\\omega (m-k)} \\\\\n&= \\sum_{k=-\\infty}^{\\infty} h_k e^{j\\omega k} \\sum_{m=-\\infty}^{\\infty} h_m e^{-j\\omega m} \\\\\n&= H^*(\\omega) H(\\omega) = |H(\\omega)|^2\n\\end{align*}\n\\]\nwhere \\(H^*(\\omega)\\) is the complex conjugate of \\(H(\\omega)\\).\nTherefore: \\[\nS_Y(\\omega) = |H(\\omega)|^2 S_X(\\omega)\n\\]\nThis shows that the power spectral density of the output is the power spectral density of the input multiplied by the squared magnitude of the frequency response.\n\n\n\nInterpretation: The power spectral density describes how the power (variance) of a stationary process is distributed across different frequencies. It is a fundamental tool in signal processing for analyzing the frequency content of random signals.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gaussian processes</span>"
    ]
  },
  {
    "objectID": "gaussian-processes.html#note-on-non-gaussian-processes",
    "href": "gaussian-processes.html#note-on-non-gaussian-processes",
    "title": "11  Gaussian processes",
    "section": "11.6 Note on non-Gaussian processes",
    "text": "11.6 Note on non-Gaussian processes\nSince all the properties discussed above (mean function, covariance function, linear transformations, etc.) depend only on the first two moments (mean and covariance), they apply to stochastic processes beyond Gaussian processes, as long as we restrict our attention to first and second moments.\nHowever, it is important to distinguish between two notions of stationarity:\n\nA stochastic process \\(\\{X_n\\}_{n \\ge 0}\\) is called wide-sense stationary (or weakly stationary) if:\n\nThe mean function is constant: \\(μ_X(n) = μ_X\\) for all \\(n \\ge 0\\)\nThe covariance function depends only on the time difference: \\(R_X(m,n) = R_X(|n-m|)\\) for some function \\(R_X\\)\n\nA stochastic process \\(\\{X_n\\}_{n \\ge 0}\\) is called strict-sense stationary (or strongly stationary) if for any finite collection of time indices \\(\\{n_1, \\dots, n_k\\}\\) and any integer \\(h \\ge 0\\), the joint distribution of \\((X_{n_1}, \\dots, X_{n_k})\\) is the same as the joint distribution of \\((X_{n_1+h}, \\dots, X_{n_k+h})\\).\n\nFor any stochastic process, strict-sense stationarity implies wide-sense stationarity. However, the converse is not true in general. For Gaussian processes, since they are completely determined by their mean and covariance, wide-sense stationarity implies strict-sense stationarity. Thus, for Gaussian processes, the two notions are equivalent.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gaussian processes</span>"
    ]
  },
  {
    "objectID": "gaussian-processes.html#exercises",
    "href": "gaussian-processes.html#exercises",
    "title": "11  Gaussian processes",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 11.1 (Verifying covariance functions) Show that the following functions are valid covariance functions for a discrete-time Gaussian process:\n\n\\(R_X(m,n) = σ^2 \\min(m,n)\\) for \\(m, n \\ge 0\\)\n\\(R_X(m,n) = ρ^{|n-m|}\\) for \\(m, n \\ge 0\\) and \\(|ρ| &lt; 1\\)\n\\(R_X(m,n) = \\frac{σ^2}{1-a^2} a^{|n-m|}\\) for \\(m, n \\ge 0\\) and \\(|a| &lt; 1\\)\n\n\n\nExercise 11.2 (Properties of Gaussian random walk) Let \\(\\{W_n\\}_{n \\ge 0}\\) be a Gaussian random walk with \\(R_W(m,n) = σ^2 \\min(m,n)\\). Show that:\n\nFor \\(0 \\le m &lt; n\\), \\(W_n - W_m \\sim \\mathcal{N}(0, σ^2(n-m))\\)\nFor \\(0 \\le n_1 &lt; n_2 &lt; n_3 &lt; n_4\\), the increments \\(W_{n_2} - W_{n_1}\\) and \\(W_{n_4} - W_{n_3}\\) are independent\n\\(\\EXP[W_m W_n] = σ^2 \\min(m,n)\\)\n\n\n\nExercise 11.3 (Conditional distribution of Gaussian random walk) For a Gaussian random walk \\(\\{W_n\\}_{n \\ge 0}\\) with \\(R_W(m,n) = σ^2 \\min(m,n)\\), compute the conditional distribution of \\(W_n\\) given \\(W_5 = 1\\) for: a. \\(n = 3\\) b. \\(n = 7\\)\nInterpret the results.\n\n\nExercise 11.4 (Gaussian process regression) Consider a Gaussian process \\(\\{X_n\\}_{n \\ge 0}\\) with mean function \\(μ_X(n) = 0\\) and covariance function \\(R_X(m,n) = ρ^{|n-m|}\\) where \\(ρ = 0.8\\).\nGiven observations \\(X_2 = 1\\) and \\(X_8 = -0.5\\):\n\nCompute the conditional mean and variance of \\(X_5\\).\nCompute the conditional mean and variance of \\(X_9\\).\nWhich prediction has higher uncertainty? Why?\n\n\n\nExercise 11.5 (Sum of Gaussian processes) Let \\(\\{X_n\\}_{n \\ge 0}\\) and \\(\\{Y_n\\}_{n \\ge 0}\\) be independent Gaussian processes with mean functions \\(μ_X(n)\\), \\(μ_Y(n)\\) and covariance functions \\(R_X(m,n)\\), \\(R_Y(m,n)\\) respectively.\nShow that \\(\\{X_n + Y_n\\}_{n \\ge 0}\\) is a Gaussian process and find its mean and covariance functions.\n\n\nExercise 11.6 (Stationary Gaussian process) A Gaussian process \\(\\{X_n\\}_{n \\ge 0}\\) is wide-sense stationary with covariance function \\(R_X(k) = ρ^{k}\\) for \\(k \\ge 0\\) and \\(|ρ| &lt; 1\\).\n\nFind the mean function \\(μ_X(n)\\).\nShow that \\(R_X(k)\\) is a valid covariance function.\nCompute \\(\\VAR(X_n)\\).\nFor what values of \\(k\\) is the correlation between \\(X_n\\) and \\(X_{n+k}\\) less than 0.1?",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gaussian processes</span>"
    ]
  },
  {
    "objectID": "gaussian-processes.html#further-reading",
    "href": "gaussian-processes.html#further-reading",
    "title": "11  Gaussian processes",
    "section": "Further Reading",
    "text": "Further Reading\n\nGubner, Chapter 15.\nGrimmett and Stirzaker, Chapter 13.\nRasmussen and Williams, Gaussian Processes for Machine Learning, MIT Press, 2006. Available online.\nAdler and Taylor, Random Fields and Geometry, Springer, 2007.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Gaussian processes</span>"
    ]
  }
]