[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability and Random Signals II",
    "section": "",
    "text": "Course Outline",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#general-information",
    "href": "index.html#general-information",
    "title": "Probability and Random Signals II",
    "section": "General Information (Fall 2025)",
    "text": "General Information (Fall 2025)\n\nInstructor\n\n\nAditya Mahajan\nOffice Hours: Tuesday 10:00am–11:00am\n\n\nTeaching Assistants\n\n\nZiqi Huang\n\n\nLectures\n\n\n8:35am–9:55am Monday, Wednesday (ENGTR 2100)\n\n\nTutorials\n\n\n12:35pm–1:25pm Friday, (ENGTR 2110)\n\n\nPrerequisites\n\n\nECSE 205 (Probability and Random Signals I)\nECSE 206 or ECSE 316 (Signals and Systems)\n\n\nCommunication\n\nUse the discussion board on myCourses for all questions related to the course. Only personal emails related to medical exceptions for missing a deliverable will be answered.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Probability and Random Signals II",
    "section": "Course Content",
    "text": "Course Content\n\n\n\n\n\n\n\nWeek\nMaterial Covered\n\n\n\n\n1\nProbability spaces, algebra of events, axioms of probability\n\n\n2\nRandom variables and random vectors\n\n\n3\nRandom variables and random vectors (continued)\n\n\n4\nConditional probability and conditional expectation\n\n\n5\nMoment generating functions and sums of random variables\n\n\n6\nProbability inequalities\n\n\n7\nReview and Mid-Term\n\n\n8\nConvergence of random variables and the strong law of large numbers\n\n\n9\nMarkov chains\n\n\n10\nMarkov chains (continued)\n\n\n11\nPoisson processes\n\n\n12\nGaussian processes and minimum mean squared error estimation\n\n\n13\nWide sense stationary processes\n\n\n\nThe material for the lecture notes is taken from various sources including the textbook and the reference book, as well as the lecture notes of Prof. Ioannis Psaromiglikos (McGill) and Prof. Ashutosh Nayyar (USC).",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#course-material",
    "href": "index.html#course-material",
    "title": "Probability and Random Signals II",
    "section": "Course Material",
    "text": "Course Material\n\nTextbook\n\n\nGrimmett and Stirzaker, Probability and Random Processes, 4th edition, Oxford University Press, 2020.\n\n\n“Engineering” Graduate Probability textbooks\n\n\nJ.A. Gubner, Probability and Random Processes for Electrical and Computer Engineers, Cambridge University Press, 2006.\nS.H. Chan, Introduction to Probability for Data Science, Michigan Publishing, 2021.\nH. Hsu, Probability, Random Variables, and Random Processes, McGraw Hill, 1997.\n\n\nExercise books\n\n\nF. Mosteller, Fifty challenging problems in probability with solutions, Courier Corporation, 1987. Available online\nGrimmett and Stirzaker, One Thousand Exercises in Probability, Oxford University Press, 2000.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Probability and Random Signals II",
    "section": "Evaluation",
    "text": "Evaluation\n\nAssignments (25%) Weekly homework assignments. Typically, each assignment will consist of four or five questions, out of which one or two randomly selected questions will be graded.\nMid Term (25%) Closed book in-class exam. Oct 8 (during class time)\nFinal Exam (50%) Closed book, in-person exam. Will be scheduled by the exam office and the dates will be announced later.\nThe final exam will cover all the material seen in the class during the term.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#marking-policy",
    "href": "index.html#marking-policy",
    "title": "Probability and Random Signals II",
    "section": "Marking policy",
    "text": "Marking policy\n\nAssignments must be submitted electronically on myCourses as a PDF. You may write the assignments on paper and then scan them as a PDF (there are several such apps available for all phone platforms), or write on a tablet and convert to PDF, or type using a word processor.\nThere will no make-up examination for students who miss a mid-term.\n\nStudent who miss the exam due to a valid reason (see Faculty of Engineering policy) should notify the instructor within a week of the exam and provide necessary documentation.\nIf, and only if, proper documentation for a missed exam is presented, the marks for the missed exam will be shifted to the final exam.\nStudents who miss the mid-term exam for any other reason (e.g., no medical note, going to the exam at the wrong time, or on the wrong day, etc.) will get zero marks on the exam.\n\nAny request for reevaluation of a mid-term or an assignment must be made in writing within a week of its return. Note that requesting a re-grade will mean that you WHOLE assignment or exam will be re-graded.\n\n\nRight to submit in English or French written work that is to be graded.\n\nIn accord with McGill University’s Charter of Students’ Rights, students in this course have the right to submit in English or in French any written work that is to be graded.\n\nAcademic Integrity\n\nMcGill University values academic integrity. Therefore all students must understand the meaning and consequences of cheating, plagiarism and other academic offences under the Code of Student Conduct and Disciplinary Procedures (see McGill’s guide to academic honesty for more information).\nL’université McGill attache une haute importance à l’honnêteté académique. Il incombe par conséquent à tous les étudiants de comprendre ce que l’on entend par tricherie, plagiat et autres infractions académiques, ainsi que les conséquences que peuvent avoir de telles actions, selon le Code de conduite de l’étudiant et des procédures disciplinaires (pour de plus amples renseignements, veuillez consulter le guide pour l’honnêteté académique de McGill.)",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#course-delivery",
    "href": "index.html#course-delivery",
    "title": "Probability and Random Signals II",
    "section": "Course delivery",
    "text": "Course delivery\nThe course is taught in a “chalk and board” style; there will be no power point presentations. All students are expected to attend lectures and take notes. Partial notes on some of the material will be provided, but are not a substitute for the material covered in class.\n© Instructor-generated course materials (e.g., handouts, notes, summaries, exam questions) are protected by law and may not be copied or distributed in any form or in any medium without explicit permission of the instructor. Note that infringements of copyright can be subject to follow up by the University under the Code of Student Conduct and Disciplinary Procedures.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "index.html#additional-notes",
    "href": "index.html#additional-notes",
    "title": "Probability and Random Signals II",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nAs the instructor of this course I endeavor to provide an inclusive learning environment. However, if you experience barriers to learning in this course, do not hesitate to discuss them with me or contact the office of Student Accessibility and Achievement.\nEnd-of-course evaluations are one of the ways that McGill works towards maintaining and improving the quality of courses and the student’s learning experience. You will be notified by e-mail when the evaluations are available. Please note that a minimum number of responses must be received for results to be available to students.",
    "crumbs": [
      "Probability and Random Signals II",
      "Course Outline"
    ]
  },
  {
    "objectID": "probability-spaces.html",
    "href": "probability-spaces.html",
    "title": "1  Introduction to Probability",
    "section": "",
    "text": "1.1 Background\nThis is a graduate course on probability and random signals. I am going to assume that everyone is familiar with the basics of undergraduate probability. For example, you should be able to answer the following questions:\nYou should also be familiar with the following concepts:\nSome of you might have also seen the following concepts\nIn this course, we will revisit these topics with a more formal approach.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#background",
    "href": "probability-spaces.html#background",
    "title": "1  Introduction to Probability",
    "section": "",
    "text": "A fair 6-sided die is rolled twice. What is the probability that the sum of the rolls equals 7?\nA biased coin with \\(\\PR({\\rm heads}) = 3/4\\) is tossed 10 times. What is the probability of obtaining 3 consecutive heads?\n\n\n\nRandom variables, probability distributions, and expectations\nConditional distributions and independent random variables\n\n\n\nThe law of large numbers\nThe central limit theorem",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#why-do-we-care-about-probability-theory",
    "href": "probability-spaces.html#why-do-we-care-about-probability-theory",
    "title": "1  Introduction to Probability",
    "section": "1.2 Why do we care about probability theory",
    "text": "1.2 Why do we care about probability theory\nProbability theory is used to model, analyze, and design various engineering systems. Broadly speaking, there are three forms of random phenomenon that arise in systems: noise, uncertainty, and randomness.\n\nNoise. Noise refers to effects beyond our control. For example, in a digital communication system, when a waveform indicating a binary \\(0\\) is transmitted, background radiation may distort the signal, causing the receiver to incorrectly decode it as \\(1\\). This interference can be modeled as noise in the channel. Similar examples arise in computer networks, photonics, and other areas.\nUncertainty. Uncertainty refers to effects that arise due to lack of information. For example, to bid in an electricity market, a wind farm may need to know how much energy it can generate in the next hour, but this depends on the speed of wind which depends on a lot of factors and is difficult to predict precise. This lack of knowledge can be modelled as uncertainty and the system modeler may have a quantified belief on that uncertainty based on historical data. Similar examples arise in circuits (where quality of a chip may depend on the vagaries in the manufacturing process).\nRandomness. Sometimes introducing randomness can improve system performance. For example, power of two choices algorithm in multi-server load balancing randomly selects two servers and assigns the task to the one with lighter load. This simple strategy effectively reduces the peak load in the servers without needing to query all servers. Similar algorithms are used in bandwidth allocation in ad-hoc networks and randomized routing in networks on chips.\n\nIn this course, we will not focus on how such probabilistic models are constructed. Instead we will study the mathematical properties these models should satisfy and explore the implications of those properties. For the modeling and application-specific details, consider the following courses:\n\nECSE 506: Stochastic Control and Decision Theory\nECSE 508: Multi-agent systems\nECSE 510: Filtering and Prediction for Stochastic Systems\nECSE 511: Introduction to Digital Communication\nECSE 515: Optical Fibre Communications\nECSE 518: Telecommunication Network Analysis\nECSE 521: Digital Communication 1\nECSE 541: Design of Multiprocessor Systems-on-Chip\nECSE 551: Machine Learning for Engineers\nECSE 554: Applied Robotics\nECSE 608: Machine Learning\nECSE 610: Wireless Communication\nECSE 620: Information Theory\nECSE 621: Statistical Detection and Estimation\nECSE 623: Digital Communications 2\nECSE 626: Statistical Computer Vision\n\nAs you can see, probability theory in truly a foundational tool for Electrical Engineering.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#review-of-set-theory",
    "href": "probability-spaces.html#review-of-set-theory",
    "title": "1  Introduction to Probability",
    "section": "1.3 Review of Set Theory",
    "text": "1.3 Review of Set Theory\n\n1.3.1 Basic set operations\nA set is a collection of objects. We say that a set \\(B\\) is a subset of set \\(A\\) (written as \\(B \\subseteq A\\)) if all elements of \\(B\\) are also elements of \\(A\\). We say that \\(B\\) is a proper subset (written \\(B \\subsetneq A\\)) if \\(B \\subseteq A\\) and \\(B \\neq A\\).\n\nExercise 1.1 Let \\(A = \\{1, 2, 3\\}\\). Find all subsets of \\(A\\).\n\nThe set of all subsets of \\(A\\) is also called the power set of \\(A\\) (denoted by \\(2^A\\)). The notation \\(2^A\\) represents that the power set of \\(A\\) contains \\(2^{|A|}\\) elements. For example, your answer to Exercise 1.1 must have \\(2^3 = 8\\) elements.\nGiven two sets \\(A\\) and \\(B\\), we define the set difference \\(A\\setminus B\\) to be all elements of \\(A\\) not in \\(B\\). Note that mathematically \\(A \\setminus B\\) is well defined even if \\(B \\not\\subseteq A\\). In particular \\[\nA \\setminus B = A \\setminus (A \\cap B).\n\\]\n\nExercise 1.2 Compute \\(A \\setminus B\\) for the following:\n\n\\(A = \\{1,2,3,4\\}\\) and \\(B = \\{1, 2\\}\\).\n\\(A = \\{1,2,3,4\\}\\) and \\(B = \\{1, 2, 5\\}\\).\n\n\nGiven a collection \\(\\{A_1, A_2, \\dots, A_n\\}\\) of sets, we define two operations:\n\nUnion \\(A_1 \\cup A_2  \\cup \\cdots \\cup A_n\\) as follows \\[\n  \\bigcup_{i=1}^n A_i = \\{ a: a \\in A_i \\text{ for some } i \\}.\n\\] This means that an element belongs to \\(A_1 \\cup A_2  \\cup \\cdots \\cup A_n\\) if it belongs to at least one of \\(A_1\\), \\(A_2\\), \\(\\ldots\\), \\(A_n\\).\nIntersection \\(A_1 \\cap A_2  \\cap \\cdots \\cap A_n\\) as follows \\[\n  \\bigcap_{i=1}^n A_i = \\{ a: a \\in A_i \\text{ for all } i \\}\n\\] This means that an element belongs to \\(A_1 \\cap A_2  \\cap \\cdots \\cap A_n\\) if it belongs to all of \\(A_1\\), \\(A_2\\), \\(\\ldots\\), \\(A_n\\).\n\nA collection \\(\\{A_1, A_2, \\dots, A_n\\}\\) is disjoint if for every \\(i \\neq j\\), \\(A_i \\cap A_j = \\emptyset\\), where \\(\\emptyset\\) denotes the empty set.\nGiven a universal set \\(Ω\\) and a collection \\(\\{B_1, B_2, \\dots, B_m\\}\\) of subsets of \\(Ω\\), we say that \\(\\{B_1, B_2, \\dots, B_m\\}\\) is a partition of \\(Ω\\) if \\(\\{B_1, B_2, \\dots, B_m\\}\\) are pairwise disjoint and their union equals the universal set \\(Ω\\).\n\n\n\n\n\n\nFigure 1.1: Example of a partition\n\n\n\n\nExample 1.1 Let \\(Ω = \\{1,2,3,4\\}\\). The following are partitions of \\(Ω\\):\n\n\\(\\{ \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\} \\}\\).\n\\(\\{ \\{1, 2\\}, \\{3, 4\\} \\}\\).\n\\(\\{ \\{1\\}, \\{2, 3\\}, \\{4\\} \\}\\).\n\nThe follow are not partitions of \\(Ω\\) [Explain why?]\n\n\\(\\{ \\{1\\}, \\{2\\}, \\{3\\}, \\}\\).\n\\(\\{ \\{1, 2, 3\\}, \\{3, 4\\} \\}\\).\n\\(\\{ \\{1\\}, \\{2, 3\\}, \\{4, 5\\} \\}\\).\n\n\nIn most of our discussion, we will work with a pre-specified universal set \\(Ω\\). In this setting we use \\(A^c\\) (read as the complement of \\(A\\)) as a short hand for \\(Ω\\setminus A\\).\n\n\n\n\n\n\nPartitions are useful because they allow breaking up a set into disjoint pieces. In particular, suppose \\(\\{B_1, \\dots, B_m\\}\\) is a partition and \\(A\\) is any subset of \\(Ω\\).\nThen, \\[\nA = (A \\cap B_1) \\cup (A \\cap B_2) \\cup \\cdots \\cup (A \\cap B_m)\n\\] where each of the components is disjoint.\n\n\n\n\n\n1.3.2 Properties of set operations\n\nCommutative \\[A \\cup B = B \\cup A\n\\quad\\text{and}\\quad\nA \\cap B = B \\cap A\\]\nAssociative \\[A \\cup (B \\cup C)= (A \\cup B) \\cup C\n\\quad\\text{and}\\quad\nA \\cap (B \\cap C)= (A \\cap B) \\cap C\\]\nDistributive \\[A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cap C)\n\\quad\\text{and}\\quad\nA \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\]\nDe Morgan’s Law \\[(A \\cup B)^c = A^c \\cap B^\\cap\n\\quad\\text{and}\\quad\n(A \\cap B)^c = A^c \\cup B^c\\]\n\n\nExercise 1.3 Use distributive property to simplify:\n\n\\([1,4] \\cap ([0,2] \\cup [3,5])\\).\n\\([2,4] \\cup ([3,5] \\cap [1,4])\\).\n\n\n\n\n1.3.3 An algebra (or field) on sets\nGiven a universal set \\(Ω\\), a collection \\(\\ALPHABET F = \\{F_1, F_2, \\dots, F_m\\}\\) of subsets of \\(Ω\\) is called an algebra if it satisfies the following properties:\n\n\\(\\emptyset \\in \\ALPHABET F\\) and \\(Ω \\in \\ALPHABET F\\).\nClosed under complements: if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nClosed under finite unions and finite intersections: if \\(A_1, \\dots, A_n \\in \\ALPHABET F\\), then \\[\nA_1 \\cup A_2 \\cup \\cdots \\cup A_n \\in \\ALPHABET F\n\\quad\\text{and}\\quad\nA_1 \\cap A_2 \\cap \\cdots \\cap A_n \\in \\ALPHABET F\n\\]\n\nWe will sometimes use the notation “\\((Ω,\\ALPHABET F)\\) is an algebra of sets” or “\\(\\ALPHABET F\\) is an algebra on \\(Ω\\)”. Some examples of algebras are as follows:\n\nThe smallest algebra associated with \\(Ω\\) is \\(\\{\\emptyset, Ω\\}\\).\nIf \\(A\\) is any subset of \\(Ω\\), then \\(\\{\\emptyset, A, A^c, Ω\\}\\) is an algebra.\nFor any set \\(Ω\\), the power-set \\(2^Ω\\) is an algebra on \\(Ω\\). As an illustration, check that the power set defined in Exercise 1.1 is an algebra.\n\nThese are all examples of a general principle: The power-set of any partition of a set is an algebra.\n\nIf the partition is \\(\\{Ω\\}\\), then the power-set \\(\\{ \\emptyset, Ω \\}\\) is an algebra.\nIf the partition is \\(\\{A, A^c\\}\\), then the power-set \\(\\{ \\emptyset, A, A^c, Ω\\}\\) is an algebra.\nIf the partition is the collection of all singleton elements of a set, then the power-set \\(2^Ω\\) is an algebra.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#function",
    "href": "probability-spaces.html#function",
    "title": "1  Introduction to Probability",
    "section": "1.4 Function",
    "text": "1.4 Function\nA function \\(f\\) is a rule that assign each input from a set \\(\\ALPHABET X\\) (called the domain) to exactly one output in another set \\(\\ALPHABET Y\\) (called the co-domain). Symbolically, this is written as \\[\n  f \\colon \\ALPHABET X \\to \\ALPHABET Y\n\\] and we say \\(f\\) maps \\(\\ALPHABET X\\) to \\(\\ALPHABET Y\\).\nThe set of values \\(\\{ f(x) : x \\in \\ALPHABET X\\}\\) is called the range. By definition, the range is a subset of the co-domain \\(Y\\), but the range may or may not be equal to \\(\\ALPHABET Y\\). For example, consider the function \\(f \\colon \\reals \\to \\reals\\) defined by \\(f(x) = x^2\\). The range of the function is \\(\\reals_{\\ge 0}\\) which is a strict subset of the co-domain \\(\\reals\\).\n\nA function is called onto or surjective if its range is equal to its co-domain.\nA function is called one-to-one or injective if different inputs maps to different outputs.\nA function which is both onto and one-to-one is called bijective. A bijective function is invertible because for every \\(y \\in \\ALPHABET Y\\), there is a unique \\(x\\) such that \\(f(x) = y\\).\nFor a set \\(\\ALPHABET B \\subset \\ALPHABET Y\\), the preimage or inverse image of \\(\\ALPHABET B\\) under \\(f\\) is defined as \\[\n  f^{-1}(\\ALPHABET B) = \\{ x \\in \\ALPHABET X :: f(x) \\in \\ALPHABET B \\},\n\\] which is a subset of \\(\\ALPHABET X\\).\n\n\nExample 1.2 Consider the following functions:\n\n\\(f_1 \\colon \\reals \\to \\reals\\)\n\\(f_2 \\colon \\reals \\to \\reals_{\\ge 0}\\)\n\\(f_3 \\colon \\reals_{\\ge 0} \\to \\reals\\)\n\\(f_4 \\colon \\reals_{\\ge 0} \\to \\reals_{\\ge 0}\\)\n\nall given by \\(f_i(x) = x^2\\), \\(i \\in \\{1, \\dots, 4\\}\\).\nExplain if each of the function is onto, into, or bijective.\n\n\nExample 1.3 Let \\(f \\colon \\reals \\to \\reals\\) be given by \\(f(x) = 2x + 1\\). Find \\(f^{-1}([3,7])\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to solve for \\(2x + 1 \\in [3,7]\\), which is equivalent to \\[\n  3 \\le 2x + 1 \\le 7\n\\] which is equivalent to \\[\n  1 \\le x \\le 3.\n\\]\nThus, \\(f^{-1}([3,7]) = [1,3]\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#mathematical-model-of-probability",
    "href": "probability-spaces.html#mathematical-model-of-probability",
    "title": "1  Introduction to Probability",
    "section": "1.5 Mathematical model of probability",
    "text": "1.5 Mathematical model of probability\nTo mathematically model probability statements, we need to model the sequence of events that may lead to the occurrence of \\(A\\): this is called a random experiment; the result of an experiment is called an outcome.\nIn general, the outcome of an experiment is not certain. We can only talk about the collection of possible outcomes. The collection of possible outcomes of an experiment is called the sample space and denoted by \\(Ω\\).\n\nExercise 1.4 What is the sample space for the toss of a coin?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(Ω = \\{ H, T \\}\\).\n\n\n\n\nExercise 1.5 What is the sample space for the roll of a (6-sided) die?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(Ω = \\{ 1,2,3,4,5,6 \\}\\).\n\n\n\nAn event is any subset of the sample space. If the outcome of the random experiment belongs to the event \\(A\\), we say that “the event \\(A\\) has occurred”. Some examples of events are:\n\nHead occurs in Exercise 1.4 (\\(A = \\{H\\}\\))\nBoth head and tail occur in Exercise 1.4 (\\(A = \\emptyset\\); this is an event that cannot happen, sometimes called the impossible event)\nAn even number is thrown in Exercise 1.5 (\\(A = \\{2,4,6\\}\\)).\n\nNote that events are subset of the sample space but not all subsets of a sample space may be events. The reasons are too complicated to explain, but the high-level explanation is that everything is okay for discrete sample spaces, but weird things can happen in continuous sample spaces.\nProbability (denoted by \\(\\PR\\)) is a function which assigns a number between \\(0\\) and \\(1\\) to every event. This number indicates what is the chance that the event occurs. Such a function should satisfy some axioms, which we will explain below.\nFirst, to define a function, we need to define it’s domain and co-domain Let’s denote the domain (i.e., the set of all events to which we can assign a probability) by \\(\\ALPHABET F\\). We expect probability to satisfy certain properties, which imposes constraints on the domain:\n\nProbability of an impossible event (e.g., getting both heads and tails when we toss a coin) should be zero. Thus, \\(\\emptyset \\in \\ALPHABET F\\).\nProbability of something happening (e.g., getting either a head or a tail when we toss a coin) should be one. Thus, \\(Ω \\in \\ALPHABET F\\).\nIf we assign probability to an event \\(A\\) then we should be able to assign probability to “\\(A\\) does not occur” i.e., \\(A^c\\). Thus, if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nIf we can talk about probability of \\(A\\) and \\(B\\), then we should be able to talk about probability that either \\(A\\) or \\(B\\) occurs and both \\(A\\) and \\(B\\) occur. Thus, if \\(A, B \\in \\ALPHABET F\\), then \\(A \\cup B \\in \\ALPHABET F\\) and \\(A \\cap B \\in \\ALPHABET F\\).\n\nThus, the domain of \\(\\PR\\) must be an algebra! However, when we go beyond finite sample spaces, being an algebra is not sufficient. But we first provide some examples of probability for finite sample spaces.\n\nExample 1.4 (Uniform probability) Consider a finite set \\(Ω\\) with \\(\\ALPHABET F = 2^Ω\\). The uniform probability \\(\\PR\\) on \\(Ω\\) is given by \\[\n\\PR(A) = \\frac{\\ABS{A}}{\\ABS{Ω}},\n\\quad \\forall A \\in \\ALPHABET F\n\\]\n\nAn illustration of uniform probability distirbution on the outcomes of a fair coin or a fair dice. Next we give an example of a probability measure that is not uniform.\n\nExample 1.5 Consider a six-sided die where \\(Ω = \\{1, 2, \\dots, 6 \\}\\), \\(\\ALPHABET F = 2^Ω\\) and \\(\\PR\\) is given by \\[\n\\PR(A) = \\sum_{ω \\in A} q(ω),\n\\quad \\forall A \\in \\ALPHABET F\n\\] where \\[\nq(1) = q(2) = q(3) = q(4) = q(5) = \\frac 2{15}\n\\quad\\text{and}\\quad\nq(6) = \\frac{1}{3}.\n\\]\nVerify that\n\n\\(\\PR(Ω) = 1\\)\n\\(\\PR(\\{3,4,5\\}) = \\frac{6}{15}\\).\n\\(\\PR(\\{1,3,4,5\\}) = \\frac{8}{15}\\).\n\n\nWe now come back to the fact that restricting the domain of \\(\\PR\\) to be an algebra is not sufficient as is illustrated by the following example.\n\nExample 1.6 A coin is tossed repeatedly until a head turns up. The sample space is \\(Ω = \\{ω_1, ω_2, \\dots\\}\\) where \\(ω_n\\) denotes the event that the first \\(n-1\\) tosses are tails followed by a head.\n\nSuppose we are interested in finding the probability of the event that the coin is tossed an even number of times, i.e., \\(A = \\{ω_2, ω_4, \\dots\\}\\). Note that \\(ω_2, ω_4, \\dots \\in \\ALPHABET F\\). However, \\(A\\) is a set. If we want to assign probability to \\(A\\) in terms of probability of \\(ω_n\\), we require \\(\\ALPHABET F\\) to be closed under countable unions. This motivates the following definition.\n\n\n\n\n\n\n\\(σ\\)-algebra\n\n\n\nGiven a universal set \\(Ω\\), a collection \\(\\ALPHABET F = \\{F_1, F_2, \\dots\\}\\) of subsets of \\(Ω\\) is called a \\(\\boldsymbol{σ}\\)-algebra if it satisfies the following properties:\n\n\\(\\emptyset \\in \\ALPHABET F\\) and \\(Ω \\in \\ALPHABET F\\).\nClosed under complements: if \\(A \\in \\ALPHABET F\\) then \\(A^c \\in \\ALPHABET F\\).\nClosed under countable unions: if \\(A_1, A_2, \\dots \\in \\ALPHABET F\\), then \\[\n\\bigcup_{n=1}^∞ A_n \\in \\ALPHABET F\n\\]\n\n\n\n\n\n\n\n\n\nThe distinction between algebras and \\(σ\\)-algebras is technical. The reason that we need to consider \\(σ\\)-algebras is to do with the definition of probability on continuous sample spaces. Take \\(Ω = [0,1]\\) and consider a random experiment where “any outcome is equally likely”. Intuitively we capture this feature by assuming that for any interval \\([a,b]\\) with \\(0 \\le a \\le b \\le 1\\), we have \\[\\begin{equation}\\label{eq:uniform}\n\\PR([a,b]) = b - a.\n\\end{equation}\\]\nWe have seen that if we want \\(\\PR\\) to be a meaningful measure, the domain \\(\\ALPHABET F\\) must at least be an algebra. We have also seen that the power-set \\(2^Ω\\) is always an algebra. So, it is tempting to take \\(\\ALPHABET F = 2^{[0,1]}\\). However, it turns out that \\(2^{[0,1]}\\) includes some weird sets (technically, non-measurable sets) due to which we cannot define a function \\(\\PR\\) on \\(2^{[0,1]}\\) that satisfies \\(\\eqref{eq:uniform}\\).\nTo workaround this technical limitation, we revisit the minimum requirements that we need from the domain of \\(\\PR\\). Since we are interested in \\(\\PR([a,b])\\), \\(\\ALPHABET F\\) must contain intervals (and therefore all finite unions and intersections of intervals). Since we are working with continuous sample spaces, we also want \\(\\PR\\) to be continuous, i.e., for any sequence of sets \\(\\{A_n\\}_{n \\ge 1}\\), we want \\(\\PR(\\lim_{n \\to ∞} A_n) = \\lim_{n \\to ∞} \\PR(A_n)\\). It turns out that the additional requirement of continuity implies that \\(\\ALPHABET F\\) must be closed under countable unions as well. Thus, the domain \\(\\ALPHABET F\\) must at least be a \\(σ\\)-algebra.\nSo, we restrict to the simplest choice of the domain \\(\\ALPHABET F\\) needed for \\(\\eqref{eq:uniform}\\) and continuity to hold. For technical reasons, we need another property known as completeness. See Sec. 1.6 of the textbook.\n\n\n\n\n\n\n\n\n\n\\(σ\\)-algebra generated by a collection and Borel \\(σ\\)-algebra\n\n\n\nGiven collection \\(\\ALPHABET S\\) of subsets of \\(Ω\\), we have the following:\n\nThe power-set \\(2^Ω\\) contains \\(\\ALPHABET S\\). Therefore, there is at least one \\(σ\\)-algebra containing \\(\\ALPHABET S\\).\nIf \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) are \\(σ\\)-algebras containing \\(\\ALPHABET S\\), then \\(\\ALPHABET F_1 \\cap \\ALPHABET F_2\\) also contains \\(\\ALPHABET S\\).\n\nThus, if we take the intersection of all \\(σ\\)-algebras containing \\(\\ALPHABET S\\), we get the smallest \\(σ\\)-algebra containing \\(\\ALPHABET S\\), which is sometimes denoted by \\(σ(\\ALPHABET S)\\).\nOne commonly used \\(σ\\)-algebra is the Borel \\(σ\\)-algebra, which is defined as follows.1 Let \\(Ω\\) be a subset of \\(\\reals\\) and \\(\\ALPHABET S\\) be the collection of all open intervals in \\(Ω\\). Then \\(σ(\\ALPHABET S)\\) is called the “Borel \\(σ\\)-algebra on Ω” and often denoted by \\(\\mathscr{B}(Ω)\\).\n\n\n1 Borel \\(σ\\)-algebra is usually defined for any topological space. We restrict our definition to subsets of reals.\nDefinition 1.1 (Probability space) A probability space is a tuple \\((Ω, \\ALPHABET F, \\PR)\\) comprising of a set \\(Ω\\), a \\(σ\\)-algebra \\(\\ALPHABET F\\) on \\(Ω\\), and a function \\(\\PR \\colon \\ALPHABET F \\to [0,1]\\) that satisfies the following axioms of proability\n\nNon-negativity. \\(\\PR(A) \\ge 0\\).\nNormalization. \\(\\PR(Ω) = 1\\).\nCountable additivity. If \\(A_1, A_2, \\dots Ω\\) is a collection of disjoint events in \\(\\ALPHABET F\\), then, \\[\n\\PR\\biggl( \\bigcup_{n=1}^∞ A_n \\biggr) =\n\\sum_{n=1}^∞ \\PR(A_n).\n\\]\n\n\nSome immediate implications of the axioms of probability are the following.\n\nLemma 1.1 (Properties of probability measures)  \n\nProbability of complement. \\(\\PR(A^c) = 1 - \\PR(A)\\).\nMonotonicity. If \\(A \\subset B\\), then \\(\\PR(B) = \\PR(A) + \\PR(B \\setminus A) \\ge \\PR(A)\\).\nInclusion-exclusion. Given two events \\(A\\) and \\(B\\), \\[\n\\PR(A \\cup B) = \\PR(A) + \\PR(B) - \\PR(A \\cap B).\n\\]\nContinuity. Let \\(A_1, A_2, \\dots\\) be (weakly) increasing sequence of events, i.e., \\(A_1 \\subseteq A_2 \\subseteq A_3 \\subseteq \\cdots\\). Define \\[\n  A = \\lim_{n \\to ∞} A_n = \\bigcup_{n=1}^∞ A_n.\n\\] Then, \\[\n  \\PR(A) = \\lim_{n \\to ∞} \\PR(A_n).\n\\]\nSimilarly, let \\(B_1, B_2, \\dots\\) be (weakly) decreasing sequence of events, i.e., \\(B_1 \\supseteq B_2 \\supseteq B_3 \\supseteq \\cdots\\). Define \\[\n   B = \\lim_{n \\to ∞} B_n = \\bigcup_{n=1}^∞ B_n.\n\\] Then, \\[\n   \\PR(B) = \\lim_{n \\to ∞} \\PR(B_n).\n\\]\nUnion bound. For any sequence of events \\(\\{A_n\\}_{n \\ge 1}\\), we have \\[\n\\PR\\biggl( \\bigcup_{n=1}^{∞} A_n \\biggr) \\le\n\\sum_{n=1}^{∞} \\PR(A_n).\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof of parts (a)–(c) is elementary and left as an exercise. Part (d) is more technical and is essentially equivalent to countable additivity. See the textbook for a proof. Union bound is an immediate consequence of inclusion-exclusion and continuity.\n\n\n\n\nExample 1.7 Let \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr B[0,1]\\), and \\(\\PR\\) be any probability measure on \\((Ω, \\ALPHABET F)\\). Take any \\(a \\in (0,1)\\).\n\nConsider \\(A_n = \\bigl[0, a - \\frac 1n \\bigr)\\). Then \\(A = \\lim_{n \\to ∞} A_n =  [0, a)\\).\nConsider \\(A_n = \\bigl[0, a - \\frac 1n \\bigr]\\). Then \\(A = \\lim_{n \\to ∞} A_n =  [0, a)\\).\nConsider \\(B_n = \\bigl[0, 1 + \\frac 1n \\bigr)\\). Then \\(B = \\lim_{n \\to ∞} B_n = [0,a]\\).\nConsider \\(B_n = \\bigl[0, 1 + \\frac 1n \\bigr]\\). Then \\(B = \\lim_{n \\to ∞} B_n = [0,a]\\).\n\nIn these examples, continuity implies that \\(\\PR(A) = \\lim_{n \\to ∞} \\PR(A_n)\\) and \\(\\PR(B) = \\lim_{n \\to ∞} \\PR(B_n)\\).\n\n\n\n\n\n\n\nSome terminology\n\n\n\n\nAn event \\(A\\) is called null if \\(\\PR(A) = 0\\). Null event should not be confused with impossible event \\(\\emptyset\\).\nWe say that \\(A\\) occurs almost surely (abbreviated to a.s.) if \\(\\PR(A) = 1\\).\n\n\n\n\nExample 1.8 Consider \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr B([0,1])\\), and \\(\\PR\\) to be the uniform probability distribution on \\(Ω\\). Consider the event \\(A\\) that the outcome is a rational number. \\(A\\) is a countable set (because the set of rational numbers is countable). For any \\(x \\in A\\), \\(\\{x\\} \\in \\ALPHABET F\\), and \\(\\PR(\\{x\\}) = 0\\) (we can infer this from Example 1.7 by thinking of \\(\\{x\\}\\) as the limit of intervals \\(\\bigl[x, x+ \\frac 1n\\bigr]\\)). Thus, by countable additivity, \\(\\PR(A) = 0\\). Hence, \\(A\\) is null.\nThe above analysis implies that \\(\\PR(A^c) = 1\\), thus the event that the outcome is irrational occurs almost surely.\n\n\nExercise 1.6 Consider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) and events \\(A, B \\in \\ALPHABET F\\). Using axioms of probability, show that:\n\nIf \\(B \\subseteq A\\), then \\(\\PR(A \\setminus B) = \\PR(A) - \\PR(B)\\). Hence, argue that \\(\\PR(A) \\ge \\PR(B)\\).\n\\(\\PR(A \\cup B) = \\PR(A) + \\PR(B) - \\PR(A \\cap B)\\).\n\\(\\PR(A) = \\PR(A \\cap B) + \\PR(A \\cap B^c)\\).\n\n\n\nExercise 1.7 Consider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) and events \\(A, B, C \\in \\ALPHABET F\\). Prove that \\[\n    \\PR(A \\cup B \\cup C) = 1 - \\PR(A^c \\mid B^c \\cap C^c) \\PR(B^c \\mid C^c) \\PR(C^c).\n  \\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#conditional-probability",
    "href": "probability-spaces.html#conditional-probability",
    "title": "1  Introduction to Probability",
    "section": "1.6 Conditional Probability",
    "text": "1.6 Conditional Probability\nConditional probabilities quantify the uncertainty of an event when it is known that another event has occurred\n\nDefinition 1.2 Let \\((Ω,\\ALPHABET F, \\PR)\\) be a probability space and \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(B) &gt; 0\\). Then, the conditional probability that \\(A\\) occurs given that \\(B\\) occurs is defined as \\[\n\\PR(A | B) = \\dfrac{ \\PR(A \\cap B) }{ \\PR(B) }.\n\\]\n\nThe notation \\(\\PR(A | B)\\) is read as “probability of \\(A\\) given \\(B\\)” or “probability of \\(A\\) conditioned on \\(B\\)”.\n\nExercise 1.8 Suppose we roll a fair six-sided die (a fair die means that all outcomes are equally likely). Consider the events \\(A\\) that the outcomes is prime and \\(B\\) that the outcome is a multiple of \\(3\\). Compute \\(\\PR(A | B)\\) and \\(\\PR(B | A)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have \\(Ω = \\{1, 2, 3, 4, 5, 6\\}\\), \\(A = \\{2, 3, 5\\}\\), and \\(B = \\{3, 6\\}\\). Note that \\(\\PR(A) = \\frac 12\\) and \\(\\PR(B) = \\frac 13\\).\nThus, \\[ \\PR(A | B) = \\frac{ \\PR(A \\cap B) }{ \\PR(B) }\n= \\frac{ \\PR(\\{3\\}) }{ \\PR(\\{3,6\\}) }\n= \\frac{ \\ABS{\\{3\\}} }{ \\ABS{\\{3,6\\}} } = \\frac {1}{2}.\n\\] Similarly, \\[ \\PR(B | A) = \\frac{ \\PR(B \\cap A) }{ \\PR(A) }\n= \\frac{ \\PR(\\{3\\}) }{ \\PR(\\{2,3,5\\}) }\n= \\frac{ \\ABS{\\{3\\}} }{ \\ABS{\\{2,3,5\\}} } = \\frac {1}{3}.\n\\]\n\n\n\n\nExercise 1.9 Suppose we roll two fair six-sided dice. Consider the event \\(A\\) that the maximum of the two rolls is less than or equal to \\(8\\) and the event \\(B\\) that the minimum of the two rolls is greater than or equal to \\(6\\). Compute \\(\\PR(A|B)\\) and \\(\\PR(B|A)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNote that \\(Ω = \\{ 1, 2,3, 4, 5, 6\\}^2\\) and \\(\\PR\\) is uniform probability on all outcomes. The sets \\(A\\), \\(B\\), and \\(A \\cap B\\) are shown in Figure 1.2. Note that \\(\\PR(A) = \\PR(B) = \\frac{26}{36} = \\frac{13}{18}\\).\n\n\n\n\n\n\nFigure 1.2: The different events in Exercise 1.9\n\n\n\nThus, we have \\[\n\\PR(A|B) = \\frac{ \\PR(A \\cap B) } { \\PR(B) }\n= \\frac{ \\ABS{ A \\cap B} }{ \\ABS{B} }\n= \\frac{16}{26} = \\frac{8}{13}\n\\] and \\[\n\\PR(B|A) = \\frac{ \\PR(B \\cap A) } { \\PR(A) }\n= \\frac{ \\ABS{ B \\cap A} }{ \\ABS{A} }\n= \\frac{16}{26} = \\frac{8}{13}\n\\]\n\n\n\n\n\n\n\n\n\nConditional probabilities are probabilities\n\n\n\nConditional probabilities are legitimate probability measures on \\((Ω, \\ALPHABET F)\\). In particular, fix event \\(B\\) with \\(\\PR(B) &gt; 0\\). Then\n\n\\(\\PR(A \\mid B) \\ge 0\\).\n\\(\\PR(Ω \\mid B) = \\dfrac{\\PR(Ω \\cap B)}{\\PR(B)} = 1\\).\nFor disjoint events \\(A_1, A_2 \\in \\ALPHABET F\\), \\[\\PR(A_1 \\cup A_2 \\mid B) =\n\\frac{ \\PR( (A_1 \\cup A_2) \\cap B) }{ \\PR(B) } =\n\\frac{ \\PR( (A_1 \\cap B) \\cup (A_2 \\cap B) ) }{ \\PR(B) } =\n\\frac{ \\PR(A_1 \\cap B) + \\PR (A_2 \\cap B) }{ \\PR(B) } =\n\\PR(A_1 \\mid B) + \\PR(A_2 \\mid B)\\] where we have used the fact that \\((A_1 \\cap B)\\) and \\((A_2 \\cap B)\\) are disjoint.\n\n\n\n\nExercise 1.10 Given an event \\(B\\) with \\(\\PR(B) &gt; 0\\), show that\n\n\\(\\PR(A^c | B) = 1 - P(A|B)\\).\n\\(\\PR(A_1 \\cup A_2 | B) = \\PR(A_1 | B) + \\PR(A_2 | B) - \\PR(A_1 \\cap A_2 | B)\\).\nIf \\(A_1 \\subset A_2\\) then \\(\\PR(A_1 | B) \\le \\PR(A_2 | B)\\).\n\n\nThe definition of conditional probability gives rise to the chain rule.\n\nLemma 1.2 (Chain rule of probability) Let \\(A\\) and \\(B\\) be events in a probability space \\((Ω, \\ALPHABET F, \\PR)\\).\n\nIf \\(\\PR(B) &gt; 0\\), then \\(\\PR(A \\cap B) = \\PR(A | B) \\PR(B)\\).\nIf \\(\\PR(A) &gt; 0\\), then \\(\\PR(A \\cap B) = \\PR(B | A) \\PR(A)\\).\n\n\nCombining the chain rule with the basic properties of partitions, we get the law of total probability.\n\nLemma 1.3 (Law of total probability) Let \\(\\{B_1, B_2, \\dots, B_m\\}\\) be a partition of \\(Ω\\) such that \\(\\PR(B_i) &gt; 0\\) for all \\(i\\). Then, \\[\n\\PR(A) = \\sum_{i=1}^m \\PR(A \\cap B_i)\n= \\sum_{i=1}^m \\PR(A | B_i) \\PR(B_i).\n\\]\n\n\n\n\n\n\nFigure 1.3: Illustration of Law of total probability\n\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider \\(m=2\\), in which case the result can be simplified as \\[\\PR(A) = \\PR(A|B)\\PR(B) + \\PR(A|B^c) \\PR(B^c).\\]\nTo prove this observe that \\[\\begin{equation}\\label{eq:two-step}\nA = A \\cap (B \\cup B^c) = (A \\cap B) \\cup (A \\cap B^c).\n\\end{equation}\\] The events \\(A \\cap B\\) and \\(A \\cap B^c\\) are disjoint. Therefore, by additivity, we have \\[\n\\PR(A) = \\PR(A \\cap B) + \\PR(A \\cap B^c).\n\\] Then, by the definition of conditional probability, we have \\(\\PR(A \\cap B) = \\PR(A|B) \\PR(B)\\) and \\(\\PR(A \\cap B^c) = \\PR(A|B^c) \\PR(B^c)\\). Substituting in the above, we get \\(\\eqref{eq:two-step}\\).\nThe argument for the general case is similar.\n\n\n\n\nExercise 1.11 There are two routes for a packet to be transmitted from a source to the destination.\n\nThe packet takes route \\(R_1\\) with probability \\(\\frac 34\\) and takes route \\(R_2\\) with probability \\(\\frac 14\\).\nOn route \\(R_1\\), the packet is dropped with probability \\(\\frac 13\\).\nOn route \\(R_2\\), the packet is dropped with probability \\(\\frac 14\\).\n\nFind the probability that the packet reaches the destination.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe start by defining some events: Let \\(R_1\\) denote the event that the packet took route \\(R_1\\) and \\(R_2\\) denote the event that the packet took route \\(R_2\\). Let \\(D\\) denote the event that the packet was dropped.\nThen, the information given in the question can be written as:\n\n\\(\\PR(R_1) = \\frac 34\\) and \\(\\PR(R_2) = \\frac 14\\).\n\\(\\PR(D | R_1) = \\frac 13\\). Thus, \\(\\PR(D^c | R_1) = 1 - \\PR(D | R_1) = \\frac 23\\).\n\\(\\PR(D | R_2) = \\frac 14\\). Thus, \\(\\PR(D^c | R_2) = 1 - \\PR(D | R_2) = \\frac 34\\).\n\nThen, by the law of total probability, we have \\[\\begin{align*}\n\\PR(D^c) &= \\PR(D^c | R_1) \\PR(R_1) + \\PR(D^c | R_2) \\PR(R_2) \\\\\n&= \\frac 34 \\frac 23 + \\frac 14 \\frac 34\n= \\frac {11}{16}.\n\\end{align*}\\]\n\n\n\n\nLemma 1.4 (Bayes rule) For any events \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(A), \\PR(B) &gt; 0\\), we have \\[\n\\PR(B|A) = \\dfrac{\\PR(A|B)\\PR(B)}{\\PR(A)}.\n\\]\nIn general, if \\(\\{B_1, B_2, \\dots, B_m\\}\\) is a partition of \\(Ω\\) such that \\(\\PR(B_i) &gt; 0\\) for all \\(i\\). Then, \\[\n\\PR(B_i|A) =\n\\dfrac{ \\PR(A|B_i) \\PR(B_i) }\n{\\displaystyle \\sum_{j=1}^m \\PR(A|B_j) \\PR(B_j)}\n\\] where we have used the law of total probability (Lemma 1.3) in the denominator.\n\n\nExercise 1.12 Consider the model of Exercise 1.11. Suppose we know that the packet was dropped. What is the probability that it was transmitted via route \\(R_1\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall the events \\(R_1\\), \\(R_2\\), and \\(D\\) defined in the solution of Exercise 1.11. We were given that \\[\\PR(R_1) = \\frac 34, \\quad \\PR(R_2) = \\frac 14, \\quad\n\\PR(D|R_1) = \\frac 13, \\quad \\PR(D|R_2) = \\frac 14.\\] We had compute that \\[\n\\PR(D) = 1 - \\PR(D^c) = \\frac 5{16}.\n\\]\nThus, by Bayes rule, we have \\[\n\\PR(R_1 | D) = \\frac{ \\PR(D | R_1) \\PR(R_1) }{ \\PR(D) }\n= \\frac{ \\frac 13 \\frac 34 } { \\frac 5{16} } = \\frac 45.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#independence",
    "href": "probability-spaces.html#independence",
    "title": "1  Introduction to Probability",
    "section": "1.7 Independence",
    "text": "1.7 Independence\nIn general, the knowledge that an event \\(B\\) has occurred changes the probability of event \\(A\\), since \\(\\PR(A)\\) is replaced by \\(\\PR(A|B)\\). If the knowledge that \\(B\\) has occurred does not does not change our belief about \\(A\\), i.e., when \\(\\PR(A|B) = \\PR(A)\\), we say “\\(A\\) and \\(B\\) are independent”. This leads to the following definition.\n\nDefinition 1.3 The events \\(A, B \\in \\ALPHABET F\\) are called independent if \\[\n\\PR(A|B) = \\PR(A)\n\\quad\\text{or}\\quad\n\\PR(B|A) = \\PR(B).\n\\] An alternative but equivalent definition is \\[\n\\PR(A \\cap B) = \\PR(A) \\PR(B).\n\\]\nWe will use the notation \\(A \\independent B\\) to denote that the events \\(A\\) and \\(B\\) are independent.\n\n\n\n\n\n\n\nIt is common for students to make the mistake and think that independence means \\(A \\cap B = \\emptyset\\). This is not true!\n\n\n\n\nExample 1.9 The events \\(A\\) and \\(B\\) defined in Exercise 1.8 are independent.\n\n\nExample 1.10 The events \\(A\\) and \\(B\\) defined in Exercise 1.9 are not independent.\n\n\nExercise 1.13 \\(A \\independent B\\) implies the following:\n\n\\(A \\independent B^c\\).\n\\(A^c \\independent B\\).\n\\(A^c \\independent B^c\\).\n\n\n\n\n\n\n\n\nIndependence of \\(σ\\)-algebras\n\n\n\nIn the discussion below, we assume that the probability space \\((Ω, \\ALPHABET F, \\PR)\\) is fixed.\n\nTwo sub-\\(σ\\)-algebras \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) of \\(\\ALPHABET F\\) are said to be independent if every event \\(A_1 \\in \\ALPHABET F_1\\) is independent of every event \\(A_2 \\in \\ALPHABET F_2\\).\nFor any event \\(A\\), let \\(σ(A)\\) denote the smallest \\(σ\\)-algebra containing \\(A\\), i.e., \\(σ(A) = \\{\\emptyset, A, A^c, Ω\\}\\).\nExercise 1.13 implies that independence of \\(A\\) and \\(B\\) implies the independence of \\(σ(A)\\) and \\(σ(B)\\). The reverse implication is trivially true. Thus, independence of events is equivalent to the independence of the smallest \\(σ\\)-algebra containing those events.\n\n\n\n\nDefinition 1.4 A family of events \\(\\{A_1, A_2, \\dots, A_n\\}\\) is called independent (sometimes mutually independent if for all non-empty subset of indices \\(\\{k_1, \\dots, k_m\\} \\subset \\{1,\\dots,n\\}\\), we have \\[\n\\PR\\bigl( A_{k_1} \\cap A_{k_2} \\cap \\cdots \\cap A_{k_m} \\bigr)\n= \\PR(A_{k_1}) \\PR(A_{k_2}) \\cdots \\PR(A_{k_m}).\n\\]\n\n\nExercise 1.14 Three bits are transmitted over a noisy channel. For each bit, the probability of correct reception is \\(λ\\). The error events for the three transmissions are mutually independent. Find the probability that two bits are received correctly.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor \\(i \\in \\{1, 2, 3\\}\\), let\n\n\\(E_i\\) denote the event that bit \\(i\\) is received incorrectly\n\\(C_i\\) denote the event that bit \\(i\\) is received correctly\n\nMoreover let \\(S\\) denote the event that two bits are received correctly. Then, \\[\nS = (C_1 \\cap C_2 \\cap E_3) \\cup (C_1 \\cap E_2 \\cap C_3)\n\\cap (E_1 \\cap C_2 \\cap C_3).\n\\] Note that the three events in the right hand side are disjoint. Thus, \\[\\begin{align*}\n\\PR(S) &=\n\\PR(C_1 \\cap C_2 \\cap E_3) +  \\PR(C_1 \\cap E_2 \\cap C_3) +\n\\PR(E_1 \\cap C_2 \\cap C_3) \\\\\n&=\n\\PR(C_1)\\PR(C_2)\\PR(E_3) +  \\PR(C_1)\\PR(E_2)\\PR(C_3) +\n\\PR(E_1)\\PR(C_2)\\PR(C_3) \\\\\n&= 3 (1-λ) λ^2.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\nPairwise independence vs independence\n\n\n\nA family of events \\(\\{A_1, A_2, \\dots, A_n\\}\\) is pairwise independent if for every \\(i,j \\in \\{1, \\dots, n\\}\\), \\(i \\neq j\\), we have \\[ \\PR(A_i \\cap A_j) = \\PR(A_i) \\PR(A_j). \\]\nPairwise independence is weaker than Independence. For instance, three events \\(A\\), \\(B\\), and \\(C\\) are pairwise independent if \\[\n\\PR(A \\cap B) = \\PR(A) \\PR(B),\n\\quad\n\\PR(B \\cap C) = \\PR(B) \\PR(C),\n\\quad\\text{and}\\quad\n\\PR(C \\cap A) = \\PR(C) \\PR(A).\n\\] For independence, in addition to the above, we also need \\[ \\PR(A \\cap B \\cap C) = \\PR(A)\\PR(B) \\PR(C). \\]\nThe following example illustrates shows that independence is stronger than pairwise independence. Consider an urn with \\(M\\) red balls and \\(M\\) blue balls. Two balls are drawn at random, one at a time, with replacement. Consider the following events:\n\n\\(A\\) is the event that the first ball is red.\n\\(B\\) is the event that the second ball is blue.\n\\(C\\) is the event that both balls are of the same color.\n\nObserve that\n\n\\(A \\cap B\\) is the event that the first is red and second is blue.\n\\(B \\cap C\\) is the event that both balls are blue.\n\\(C \\cap A\\) is the event that both balls are red.\n\\(A \\cap B \\cap C = \\emptyset\\)\n\nTherefore,\n\n\\(\\PR(A) = \\PR(B) = \\PR(C) = \\frac 14\\).\n\\(\\PR(A \\cap B) = \\PR(B \\cap C) = \\PR(C \\cap A) = \\frac 14\\).\n\\(\\PR(A \\cap B \\cap C) = \\emptyset\\).\n\nThus, \\(A\\), \\(B\\), \\(C\\) are pairwise independent but not independent.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#product-spaces",
    "href": "probability-spaces.html#product-spaces",
    "title": "1  Introduction to Probability",
    "section": "1.8 Product spaces",
    "text": "1.8 Product spaces\nSo far, we have restricted attention to the outcome of one experiment. It is also possible to construct probability models which combine the outcome of two independent experiments, e.g., suppose we toss a coin and also roll a die. Let \\((Ω_1, \\ALPHABET F_1, \\PR_1)\\) and \\((Ω_2, \\ALPHABET F_2, \\PR_2)\\) be the probability spaces associated with the two experiments? What is the probability space \\((Ω, \\ALPHABET F, \\PR)\\) of the joint experiments?\nThe sample space should obviously be \\(Ω = Ω_1 \\times Ω_2\\). When \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) are finite, then we can simply define \\(\\ALPHABET F = \\ALPHABET F_1 \\times \\ALPHABET F_2\\) and for any \\(A = (A_1, A_2) \\in \\ALPHABET F\\), \\(\\PR(A)\\) to be \\(\\PR(A_1) \\PR(A_2)\\). You would have implicitly consutrcted such product spaces when dealing with joint experiments (like the coin toss and die roll example above) in your undergrad courses.\nHowever, things are a bit more complicated when \\(\\ALPHABET F_1\\) and \\(\\ALPHABET F_2\\) are not finite. The difficulty is that \\(\\ALPHABET F_1 \\times \\ALPHABET F_2\\) is not a \\(σ\\)-algebra. So, take \\(\\ALPHABET F\\) to be \\(σ(\\ALPHABET F_1 \\times \\ALPHABET F_2)\\) (which is the smallest \\(σ\\)-algebra comtaining \\(\\ALPHABET F_1 \\times \\ALPHABET F_2\\)) and define \\(\\PR\\) to be the extension of \\(\\PR_1 \\times \\PR_2\\) from \\(\\ALPHABET F_1 \\times \\ALPHABET F_2\\) to \\(σ(\\ALPHABET F_1 \\times \\ALPHABET F_2)\\) (one can show that such an extension exists). Such product space is often written as \\[\n(Ω, \\ALPHABET F, \\PR) = (Ω_1 \\times Ω_2, \\ALPHABET F_1 \\otimes \\ALPHABET F_2, \\PR_1 \\otimes \\PR_2). \\]\nWe will not worry too much about the technical details of such product spaces, but will use the above notation at times in the course.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability-spaces.html#notes",
    "href": "probability-spaces.html#notes",
    "title": "1  Introduction to Probability",
    "section": "Notes",
    "text": "Notes\nThe material for this section is fairly standard and adapted from various sources. Both Grimmit and Stirzaker and Gubner have an excellent coverage of this material. The idea of categorizing random phenomenon as noise, uncertainty, or randomness is borrowed from Maxim Raginsky’s course notes. See the book Against the Gods, which provides a fascinating historical account of the development of probability theory and, why a conceptual leap was needed to recognize that uncertainty could be modeled and measured.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "random-variables.html",
    "href": "random-variables.html",
    "title": "2  Random variables and random vectors",
    "section": "",
    "text": "2.1 Classification of random variables\nIn many situations, we are not directly interested in the outcome of a random experiment, but a consequence of the outcome. Such consequences may be thought of as a function of the outcome. When they are real-valued, such functions of the outcome are called random variables.\nThe random variable \\(X \\colon Ω \\to \\reals\\) induces a probability measure on \\(\\reals\\). Formally, to define such a probability measure, we need an associated \\(σ\\)-algebra on \\(\\reals\\). As discussed in last lecture, the commonly used \\(σ\\)-algebra on reals is the Borel \\(σ\\)-algebra, \\(\\mathscr{B}(\\reals)\\). For everything to be consistent, we require the function \\(X\\) to satisfy a property known as measurability.\nFor instance, for Example 2.1, the CDF is given by \\[\nF_X(x) = \\begin{cases}\n0, & \\hbox{if } x &lt; 0,  \\\\\n\\frac 14, & \\hbox{if } 0 \\le x &lt; 1, \\\\\n\\tfrac 34, & \\hbox{if } 1 \\le x &lt; 2,\\\\\n1, &\\hbox{if } 2 \\le x.\n\\end{cases}\\]\nThere are three types of random variables",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables and random vectors</span>"
    ]
  },
  {
    "objectID": "random-variables.html#classification-of-random-variables",
    "href": "random-variables.html#classification-of-random-variables",
    "title": "2  Random variables and random vectors",
    "section": "",
    "text": "A random variable \\(X\\) is said to be discrete if it takes values in a finite or countable subset \\(\\text{range}(X) \\coloneqq \\{x_1, x_2, \\dots\\}\\) of \\(\\reals\\). A discrete random variable has a probability mass function (PMF) \\(p \\colon \\reals \\to [0,1]\\) which satisfies the following properties:\n\n\\(p(x) = \\PR(X = x) = F(x) - F(x^{-})\\).\n\\(F(x) = \\sum_{x_n : x_n \\le x} p(x_n).\\)\n\nThus, for a discrete random variable, the CDF is a piecewise constant function\n\nA random variable \\(X\\) is called continuous if there exists an integrable function \\(f \\colon \\reals \\to [0, ∞)\\) called the probability denisity function such that the CDF can be written as \\[\nF(x) = \\int_{-∞}^x f(x) dx.\n\\]\nThus, for a continuous random variable, the CDF is a continuous function\n\nA random variable is called mixed if it is neither discrete nor continuous. For a mixed random variable, the CDF has has jumps at a finite or countable infinite number of points and it is continuous over one or many intervals.\nAs an example, consider the following random experiment. A fair coin is tossed: if the outcome is heads, then \\(X \\sim \\text{Bernoulli}(0.5)\\); if the outcome is tails; then \\(X \\sim \\text{Uniform}[0,1]\\). Thus (from the law of total probability), the CDF of \\(X\\) is \\[\n   F_X(x) = \\begin{cases}\n   0, & \\hbox{if } x &lt; 0 \\\\\n   \\frac 14 & \\hbox{if } x = 0 \\\\\n   \\frac 14 + \\frac x2 & \\hbox{if } 0 &lt; x &lt; 1 \\\\\n   1 & \\hbox{if } x \\ge 1.\n   \\end{cases}.\n   \\]\n\n\n\n\n\n\n\n\\(σ\\)-algebra generated by random variables\n\n\n\nA discrete random variable creates a partition of the sample space. In particular, suppose \\(X\\) is a random variable and \\(\\{x_1, x_2, \\dots\\}\\) is the range of \\(X\\). Define \\[A_n = \\{ω \\in Ω : X(ω) = x_n \\} = X^{-1}(x_n)\\] Then, \\(\\{A_1, A_2, \\dots \\}\\) is a partition of \\(Ω\\).\n\n\n\n\n\n\nProof that it is a partition\n\n\n\n\n\nTo show that \\(\\{A_1, A_2, \\dots \\}\\) forms a partition, we need to establish two properties:\n\n\\(A_i \\cap A_j = \\emptyset\\).\n\\(\\bigcup_{i=1}^∞ A_i = Ω\\).\n\nThe details are left as an exercise.\n\n\n\nThe power-set of \\(\\{A_1, A_2, \\dots\\}\\) is called the \\(σ\\)-algebra generated by \\(X\\) and denoted by \\(σ(X)\\). This \\(σ\\)-algebra captures the crux of measurability. As an illustration, let’s reconsider Example 2.1. In this case, the range of \\(X\\) is \\(\\{0, 1, 2\\}\\). The partition corresponding to \\(σ(X)\\) is shown in Figure 2.1.\n\n\n\n\n\n\nFigure 2.1: Illustration of \\(σ(X)\\) for Example 2.1\n\n\n\n\n\n\nLemma 2.2 (Properties of PMFs and PDFs)  \n\nProperties of PMFs\n\nFor a discrete random variable, \\(\\sum_{x \\in \\text{range}(X)}p(x) = 1\\).\nFor any event \\(A \\in \\ALPHABET F\\), \\(\\PR(X \\in A) = \\sum_{x \\in \\text{range}(X) \\cap A} p(x)\\).\n\nProperties of PDFs\n\nFor a continuous random variable, \\(\\int_{-∞}^{∞} f(x)\\, dx = 1\\).\nFor any event \\(A \\in \\ALPHABET F\\), \\(\\PR(X \\in A) = \\int_{x \\in A} f(x)\\,dx\\).\nThe PDF is the derivative of CDF: \\[\nf_X(x) = \\frac{d}{dx} F_X(x).\n\\]\n\n\n\n\n2.1.1 Some examples of discrete random variables\nWe now consider some other examples of discrete random variables\n\nExample 2.5 (Binomial random variable) A Binomial random variable is the sum of intendant and identically Bernoulli random variables (we will prove this fact later). For example, if a biased coin (with \\(\\PR(H) = p\\)) is tossed \\(n\\) times, then the number of heads is a binomial random variable with parameters \\(n\\) and \\(p\\), which is denoted by \\(\\text{Binomial}(n,p)\\). For such a random variable, \\[\np_X(k) = \\binom n k p^k (1-p)^{n-k}, \\quad 0 \\le k \\le n.\n\\]\n\n\nExample 2.6 (Geometric random variable) A geometric random variable is the number of trails in i.i.d. Bernoulli random variables. For example, if a biased coin (with \\(\\PR(H) = p\\)) is tossed repeated, the number of tosses needed for the first head is a geometric random variable with parameter \\(p \\in (0,1)\\)_, which is denoted by \\(\\text{Geo}(p)\\). For such a random variable, \\[\np_X(k) = (1-p)^{k-1} p, \\quad k \\in \\integers_{&gt; 0}.\n\\]\n\n\nExample 2.7 (Poisson random variable) Poisson random variables model many different phenomenon ranging from photoelectric effect in photonics to inter-packet arrival times in computer networks. A random variable is said to Poisson random variable with parameter \\(λ &gt; 0\\), which is denoted by \\(\\text{Poisson}(λ)\\), if \\[\np_X(k) = \\frac{λ^k}{k!} e^{-λ}, \\quad k \\in \\integers_{\\ge 0}.\n\\]\n\n\nExample 2.8 (Uniform random variable) A random variable is said to have a (discrete) uniform distribution over a discrete set \\(\\ALPHABET S\\) if \\[p_X(k) = \\frac 1{\\ABS{\\ALPHABET S}}, \\quad k \\in \\ALPHABET S.\\]\n\n\n\n2.1.2 Some examples of continuous random variables\n\nExample 2.9 (Uniform random variable) A random variable is said to have a (continuous) uniform distribution over an interval \\([a, b]\\), where \\(a &lt; b\\) if \\[f(x) = \\frac 1{b - a}, \\quad x \\in [a,b].\\]\n\n\nExample 2.10 (Exponential random variable) A random variable is said to have an exponential distribution with parameter \\(λ &gt; 0\\), which is denoted by \\(\\text{exp}(λ)\\) if \\[f(x) = λ e^{-λ x}, \\quad x \\ge 0.\\]\n\n\nExample 2.11 (Gaussian random variable) A random variable is said to have a Gaussian distribution with mean \\(μ\\) and standard deviation \\(σ &gt; 0\\), which is denoted by \\(\\mathcal N(μ, σ^2)\\) if \\[f(x) = \\frac 1{\\sqrt{2 π}\\, σ}\n\\exp\\left( -\\frac {(x-μ)^2}{2 σ^2} \\right),\n\\quad x \\in \\reals.\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables and random vectors</span>"
    ]
  },
  {
    "objectID": "random-variables.html#expectation-of-random-variables",
    "href": "random-variables.html#expectation-of-random-variables",
    "title": "2  Random variables and random vectors",
    "section": "2.2 Expectation of random variables",
    "text": "2.2 Expectation of random variables\nSuppose we generate \\(N\\) i.i.d. (independent and identically distributed) samples \\(\\{s_1, s_2, \\dots, s_N\\}\\) of a random variable \\(X\\) and compute the average: \\[ m = \\frac 1N \\sum_{n=1}^N s_n. \\] When \\(X\\) is discrete and takes values \\(\\{x_1, x_2, \\dots, x_n\\}\\), we expect that the number of times we obtain a value \\(x_i\\) is approximately \\(Np(x_i)\\) when \\(N\\) is large. Thus, \\[ m \\approx \\frac 1N \\sum_{i=1}^n x_i \\, N p(x_i)  = \\sum_{i=1}^n x_i p(x_i). \\]\nThis quantity is called the expectation or the expected value or the mean value of the random variable \\(X\\) and denoted by \\(\\EXP[X]\\).\n\nDefinition 2.2 The expectation of a random variable \\(X\\) is defined as follows:\n\nwhen \\(X\\) is discrete and takes values \\(\\{x_1, x_2, \\dots, x_n \\}\\), then \\[\\EXP[X] = \\sum_{i=1}^n x_i p(x_i).\\]\nwhen \\(X\\) is continuous, then \\[\\EXP[X] = \\int_{-∞}^{∞} x f(x)\\, dx. \\]\nThus, we can think of the expected value as the center of mass of the PDF.\n\n\n\n\n\n\n\n\nDoes the summation or integration exist?\n\n\n\nWhen \\(X\\) takes countably or uncountably infinite values, we need to be a bit more precise by what we mean by the summation (or the integration) formula above. In particular, we do not want the answer to depend on the order in which we do the summation or the integration (i.e., we do not want \\(∞ - ∞\\) situation). This means that the sum or the integral should be :absolutely convergent. Such random variables are called integrable random variables.\nFormally, expectation is defined only for integrable random variables.\nTo illustrate why this is important, consider a discrete random variable defined over \\(\\integers\\setminus\\{0\\}\\) where \\[\np(n) = p(-n) = \\frac {1}{2C n^2}, \\quad n \\in \\naturalnumbers\n\\] where \\(C\\) is a normalizing constant given by \\[\nC = \\sum_{n=1}^∞ \\frac 1{n^2} = \\frac{π^2}{6}.\n\\] Then, observe that \\[\\begin{align*}\n\\EXP[X] &= \\sum_{n=1}^∞ \\frac{n}{2 C n^2}\n+ \\sum_{n=-∞}^{-1} \\frac{n}{2 C n^2} \\\\\n&= \\frac 1{2C} \\sum_{n=1}^∞ \\frac{1}{n}\n+ \\frac 1{2C} \\sum_{n=-∞}^{-1} \\frac{1}{n} \\\\\n&= \\frac{∞}{2C} - \\frac{∞}{2C}\n\\end{align*}\\] which is undefined.\nThe concern here is that the summation is undefined. Mathematically, we are okay when the summation is infinity. For example, consider another random variable \\(Y\\) defined over \\(\\naturalnumbers\\) for which \\[\np(n) = \\frac {1}{C n^2}, \\quad n \\in \\naturalnumbers\n\\] where \\(C\\) is as defined above. This is called the Zipf distribution. By following an argument same as above, we see that \\[\\EXP[Y] = ∞.\\]\n\n\n\nExample 2.12 Find the mean of \\(X\\) defined in Example 2.1.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\EXP[X] = \\frac 14 \\cdot 0 + \\frac 12 \\cdot 1 + \\frac 14 \\cdot 2 = 1.\n\\]\n\n\n\n\nExercise 2.2 Find the expected value of the random variables with the following distributions:\n\n\\(\\text{Bernoulli}(p)\\).\n\\(\\text{Binomial}(n,p)\\).\n\\(\\text{Geo}(p)\\).\n\\(\\text{Poisson}(λ)\\).\n\\(\\text{Uniform}[a,b]\\).\n\\(\\text{Exp}(λ)\\).\n\n\n\nLemma 2.3 For any (measurable) function \\(g \\colon \\reals \\to \\reals\\), we have\n\nwhen \\(X\\) is discrete and takes values \\(\\{x_1, x_2, \\dots, x_n \\}\\), then \\[\\EXP[g(X)] = \\sum_{i=1}^n g(x_i) p(x_i).\\]\nwhen \\(X\\) is continuous, then \\[\\EXP[g(X)] = \\int_{-∞}^{∞} g(x) f(x)\\, dx. \\]\n\nBoth expressions are defined only when the sum/integral is absolutely convergent.\n\n\n\n\n\n\n\nHow to avoid a proof\n\n\n\n\n\nThis result is sometimes called *the law of the unconscious statistician (LOTUS). One typically shows this result by defining a new random variable \\(Y = g(X)\\), computing its PMF/PDF \\(f_Y\\) and then using the definition in Definition 2.2.\nA simpler proof is to define expectation by Lemma 2.3 for any (measurable) function \\(g\\). Then the definition of Definition 2.2 falls off as a special case for \\(g(x) = x\\). No proofs needed!\n\n\n\n\nExercise 2.3 Suppose \\(X \\sim \\text{Unif}[-1,1]\\). Compute \\(\\EXP[X^2]\\).\n\n\nLemma 2.4 (Properties of expectation)  \n\nLinearity. For any (measurable) functions \\(g\\) and \\(h\\) \\[\\EXP[g(X) + h(X)] = \\EXP[ g(X)] + \\EXP[ h(X) ]. \\] As a special case, for a constant \\(c\\), \\[\\EXP[X + c] = \\EXP[X] + c.\\]\nScaling. For any constant \\(c\\), \\[\\EXP[cX] = c\\EXP[X].\\]\nBounds. If \\(a \\le X(ω) \\le b\\) for all \\(ω \\in Ω\\), then \\[ a \\le \\EXP[X] \\le b. \\]\nIndicator of events. For any (Borel) subset \\(B\\) of \\(\\reals\\), we have \\[\\EXP[ \\IND_{\\{ X \\in B \\}}] = \\PR(X \\in B). \\]\n\n\n\nA continuous random variable is said to be symmetric if \\(f_X(-x) = f_X(x)\\) for all \\(x \\in \\reals\\). A symmetric random variable has mean \\(0\\).\nA continuous random variable is said to be symmetric around \\(m\\) if \\(f(m - x) = f(m + x)\\), for all \\(x \\in \\reals\\). The mean of such a random variable is \\(m\\).\n\n\n2.2.1 Higher moments\n\nThe \\(m\\)-th moment, \\(m \\ge 1\\) of a random variable \\(X\\) is defined as \\(\\EXP[X^m]\\).\nThe \\(m\\)-th central moment is defined as \\(\\EXP[(X - μ)^m]\\), where \\(μ = \\EXP[X]\\).\nFor second central moment (i.e., \\(m=2\\)) is called variance. The variance satisfies the following: \\[\\VAR(X) = \\EXP[X^2] - (\\EXP[X])^2.\\]\nThe positive square root of variance is called the standard deviation. Variance is often denoted by \\(σ^2\\) and the standard deviation by \\(σ\\).\n\n\nExample 2.13 Find the variance of \\(X\\) defined in Example 2.1.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first compute \\[\n\\EXP[X^2] = \\frac 14 \\cdot 0^2 + \\frac 12 \\cdot 1^2 + \\frac 14 \\cdot 2^2 = \\frac 32.\n\\] Therefore, \\[\n\\VAR(X) = \\EXP[X^2] - \\EXP[X]^2 = \\frac 32 - 1 = \\frac 12.\n\\]\n\n\n\n\nLemma 2.5 (Properties of variance)  \n\nScaling. For any constant \\(c\\), \\[\\VAR(cX) = c^2 \\VAR(X).\\]\nShift invariance. For any constant \\(c\\), \\[\\VAR(X + c) = \\VAR(X).\\]\n\n\nThe mean and variance of common random variables is show in Table 2.1\n\n\n\nTable 2.1: Mean and variance of common random variables\n\n\n\n\n\n\n\n\n\n\n\nRandom variable\nParameter(s)\nMean\nVariance\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p\\)\n\\(p(1-p)\\)\n\n\nBinomial\n\\((n,p)\\)\n\\(np\\)\n\\(np(1-p)\\)\n\n\nGeometric\n\\(p\\)\n\\(\\dfrac 1p\\)\n\\(\\dfrac{1-p}{p}\\)\n\n\nPoisson\n\\(λ\\)\n\\(λ\\)\n\\(λ\\)\n\n\nUniform\n\\((a,b)\\)\n\\(\\frac 12 (a+b)\\)\n\\(\\frac 1{12}(b-a)^2\\)\n\n\nExponential\n\\(λ\\)\n\\(\\dfrac 1 λ\\)\n\\(\\dfrac 1{λ^2}\\)\n\n\nGaussian\n\\((μ,σ)\\)\n\\(μ\\)\n\\(σ^2\\)",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables and random vectors</span>"
    ]
  },
  {
    "objectID": "random-variables.html#random-vectors-and-joint-distributions.",
    "href": "random-variables.html#random-vectors-and-joint-distributions.",
    "title": "2  Random variables and random vectors",
    "section": "2.3 Random vectors and joint distributions.",
    "text": "2.3 Random vectors and joint distributions.\nSuppose \\(X\\) and \\(Y\\) are two random variables defined on the same probability space. The CDFs \\(F_X\\) and \\(F_Y\\) provide information about their individual probabilities. To understand how they behave together, we need to think of the random vector \\((X,Y)\\) taking values in \\(\\reals^2\\). The natural way to do so is to think of the joint CDF \\[\nF_{X,Y}(x,y) = \\PR(\\{ ω \\in Ω : X(ω) \\le x, Y(ω) \\le y \\})\n\\] where we may write the right hand side as \\(\\PR(X \\le x, Y \\le y)\\) for short.\n\nLemma 2.6 (Properties of CDFs)  \n\nRegularity properties\n\n\\(\\lim_{x \\to -∞} F_{X,Y}(x,y) = 0\\), \\(\\lim_{y \\to -∞} F_{X,Y}(x,y)\\) and \\(\\lim_{x,y \\to +∞} F(x,y) = 1\\).\nJoint CDFs are non-decreasing, i.e., if \\((x_1,y_1) &lt; (x_2, y_2)\\), then \\(F_{X,Y}(x_1,y_1) \\le F_{X,Y}(x_2,y_2)\\).\nJoint CDFs are continuous from above, i.e., \\[\\lim_{u,v \\downarrow 0}F_{X,Y}(x+u,y+v) = F_{X,Y}(x,y).\\]\n\\(\\PR(X = x, Y = y) = F(x,y) - F(x^{-},y^{-})\\).\n\nMarginalization of joint CDFs\n\n\\(\\lim_{y \\to ∞} F_{X,Y}(x,y) = F_X(x)\\)\n\\(\\lim_{x \\to ∞} F_{X,Y}(x,y) = F_Y(y)\\)\n\n\n\n\nExercise 2.4 Consider random variables \\(X\\) and \\(Y\\) with joint CDF \\(F\\). Show that \\[\n\\PR(a &lt; X \\le b, c &lt; Y \\le d) = F(b,d) - F(a,d) - F(b,c) + F(a,c).\n\\]\n\n\n2.3.1 Classification of random vectors\nAs was the case for random variables, we can also classify random vectors as discrete, continuous, and mixed.\n\nA random vector \\((X,Y)\\) is called jointly discrete if it takes values in a countable subset of \\(\\reals^2\\) (we denote this subset by \\(\\text{range}(X,Y)\\)). The jointly discrete random variables have a joint PMF \\(f \\colon \\reals \\to [0,1]\\) given by \\[ \\PR(X = x, Y = y) = p(x,y). \\]\nA random vector \\((X, Y)\\) is called jointly continuous if its CDF can be expressed as \\[ F(x,y) = \\int_{-∞}^x \\int_{-∞}^{y} f(u,v)\\, du dv, \\quad\nx,y \\in \\reals \\] for some integrable function \\(f \\colon \\reals^2 \\to [0, ∞)\\) which is called the joint PDF.\nA random vector \\((X,Y)\\) is called jointly mixed if it is neither jointly discrete nor jointly continuous.\n\n\nLemma 2.7 (Properties of PMFs and PDFs)  \n\nProperties of PMFs\n\nNormalization. For a jointly discrete random vector \\((X,Y)\\), \\[\\sum_{x,y \\in \\text{range}(X,Y)}p_{X,Y}(x,y) = 1.\\]\nFor any event \\(A \\in \\ALPHABET F\\), \\[\\PR((X,Y) \\in A) = \\sum_{(x,y) \\in \\text{range}(X,Y) \\cap A} p_{X,Y}(x,y).\\]\nMarginalization.\n\n\\(\\displaystyle \\sum_{x \\in \\text{range(X)}} p_{X,Y}(x,y) = p_Y(y)\\).\n\\(\\displaystyle \\sum_{y \\in \\text{range(Y)}} p_{X,Y}(x,y) = p_X(x)\\).\n\n\nProperties of PDFs\n\nNormalization. For a jointly continuous random vector \\((X,Y)\\), \\[\\int_{-∞}^{∞} \\int_{-∞}^{∞} f_{X,Y}(x,y)\\, dxdy = 1.\\]\nFor any event \\(A \\in \\ALPHABET F\\), \\[\\PR((X,Y) \\in A) = \\iint_{(x,y) \\in A} f_{X,Y}(x,y)\\,dxdy.\\]\nMarginalization.\n\n\\(\\displaystyle \\int_{-∞}^{∞} f_{X,Y}(x,y) dx = f_Y(y)\\).\n\\(\\displaystyle \\int_{-∞}^{∞} f_{X,Y}(x,y) dy = f_X(x)\\).\n\n\n\n\nThe above discussion generalizes in the obvious manner to more than two random variables as well. Thus, we can talk about random vectors \\(X = (X_1, \\dots, X_n) \\in \\reals^n\\). In practice, we often do not make a distinction between random variables and random vectors and refer both of them simply as random variables.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables and random vectors</span>"
    ]
  },
  {
    "objectID": "random-variables.html#independence-of-random-vectors",
    "href": "random-variables.html#independence-of-random-vectors",
    "title": "2  Random variables and random vectors",
    "section": "2.4 Independence of random vectors",
    "text": "2.4 Independence of random vectors\n\nDefinition 2.3 Two random variables \\(X\\) and \\(Y\\) defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\) are said to be independent if the sigma algebras \\(σ(X)\\) and \\(σ(Y)\\) are independent.\n\nThe above definition means that if we take any (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\), then the events \\(\\{X \\in B_1\\}\\) and \\(\\{X \\in B_2\\}\\) are independent, i.e., \\[\n\\PR(X \\in B_1, Y \\in B_2) = \\PR(X \\in B_1) \\PR(Y \\in B_2).\n\\]\nUsing this, we can show that following:\n\n\\(X\\) and \\(Y\\) are independent if and only if \\[\n   F_{X,Y}(x,y) = F_X(x) F_Y(y), \\quad \\forall x, y \\in \\reals.\n\\]\nTwo jointly continuous random variables \\(X\\) and \\(Y\\) are independent if and only if \\[\n   f_{X,Y}(x,y) = f_X(x) f_Y(y), \\quad \\forall x, y \\in \\reals.\n\\]\nTwo jointly discrete random variables \\(X\\) and \\(Y\\) are independent if and only if \\[\n   p_{X,Y}(x,y) = p_X(x) p_Y(y), \\quad \\forall x, y \\in \\reals.\n\\]\n\nAn immediate implication of the above definition is the following.\n\nProposition 2.1 Let \\(X\\) and \\(Y\\) be independent random variables defined on a common probability space. Consider \\(U = g(X)\\) and \\(V = h(Y)\\) for some (measurable) functions \\(g\\) and \\(h\\). Then, \\(U\\) and \\(V\\) are independent.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider any (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\) and consider the events \\(\\{ U \\in B_1 \\}\\) and \\(\\{ V \\in B_2 \\}\\). Note that\n\n\\(\\{ U \\in B_1 \\} = \\{ X \\in g^{-1}(B_1) \\}\\).\n\\(\\{ V \\in B_2 \\} = \\{ Y \\in h^{-1}(B_2) \\}\\).\n\nSince the random variables \\(X\\) and \\(Y\\) are independent, the events \\(\\{ X \\in g^{-1}(B_1) \\}\\) and \\(\\{ Y \\in h^{-1}(B_2) \\}\\). Which implies that the events \\(\\{ U \\in B_1 \\}\\) and \\(\\{ V \\in B_2 \\}\\) are independent. Consequently, the random variables \\(U\\) and \\(V\\) are independent.\n\n\n\n\nProposition 2.2 Let \\(X\\) and \\(Y\\) be independent random variables defined on a common probability space. Then \\(X\\) and \\(Y\\) are independent if and only if \\[\\begin{equation}\\label{eq:expectation-product}\n  \\EXP[ g(X) h(Y) ] = \\EXP[ g(X) ] \\EXP[ h(Y) ]\n\\end{equation}\\] for all (measurable) functions \\(g\\) and \\(h\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThere are two claims here.\n\nIf \\(X\\) and \\(Y\\) are independent then \\(\\eqref{eq:expectation-product}\\) holds.\nIf \\(\\eqref{eq:expectation-product}\\) holds, then \\(X\\) and \\(Y\\) are independent.\n\nWe will prove the first claim assuming that \\(X\\) and \\(Y\\) are continuous. Similar argument works for the discrete case as well. \\[\\begin{align*}\n  \\EXP[ g(X) h(Y) ]\n  &= \\int_{-∞}^∞ \\int_{-∞}^∞ g(x) h(y) f_{X,Y}(x,y)\\, dx dy \\\\\n  &\\stackrel{(a)}= \\int_{-∞}^∞ \\int_{-∞}^∞ g(x) h(y) f_{X}(x) f_{Y}(y)\\, dy dx \\\\\n  &\\stackrel{(b)}= \\int_{-∞}^∞ \\left[ \\int_{-∞}^∞ g(x)f_{X}(x)\\, dx \\right]h(y) f_{Y}(y)  \\, dy \\\\\n  &\\stackrel{(c)}= \\left[ \\int_{-∞}^∞ g(x)f_{X}(x)\\, dx \\right]\n  \\left[\\int_{-∞}^∞  h(y) f_{Y}(y)  \\, dy \\right] \\\\\n  &= \\EXP[ g(X) ] \\EXP [ h(Y) ]\n\\end{align*}\\] where \\((a)\\) follows from the fact that \\(X \\independent Y\\), \\((b)\\) and \\((c)\\) are simple algebra, and the last step uses the definition of expectation.\nTo prove the second claim, pick any (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\) and consider the functions \\(g(x) = \\IND_{B_1}(x)\\) and \\(h(y) = \\IND_{B_2}(y)\\). Observe that \\[\\begin{align*}\n  \\PR(X \\in B_1, Y \\in B_2)\n  &= \\EXP[\\IND_{ \\{ X \\in B_1, Y \\in B_2 \\}}] \\\\\n  &\\stackrel{(d)}= \\EXP[\\IND_{ \\{ X \\in B_1 \\}} \\IND_{\\{ Y \\in B_2 \\}}] \\\\\n  &\\stackrel{(e)}=\\EXP[\\IND_{ \\{ X \\in B_1 \\}}\\ \\EXP[ \\IND_{\\{ Y \\in B_2 \\}}] \\\\\n  &\\stackrel{(f)}= \\PR(X \\in B_1) \\PR(Y \\in B_2)\n\\end{align*}\\] where \\((d)\\) follows from basic algebra, \\((e)\\) follows from \\(\\eqref{eq:expectation-product}\\), and \\((f)\\) follows from expectation of an indicator.\nThe above equation shows that for any arbitrary (Borel) subsets \\(B_1\\) and \\(B_2\\) of \\(\\reals\\), \\(\\PR(X \\in B_1, Y \\in B_2) = \\PR(X \\in B_1) \\PR(Y \\in B_2)\\). Hence, \\(\\{X \\in B_1\\} \\independent \\{Y \\in B_2 \\}\\). Since \\(B_1\\) and \\(B_2\\) were arbitrary, we have \\(X \\independent Y\\).\n\n\n\n\nExercise 2.5 Let \\(X\\) and \\(Y\\) be independent random variables defined on a common probability space. Show that\n\n\\(\\EXP[XY] = \\EXP[X] \\EXP[Y]\\).\n\\(\\VAR(XY) = \\VAR(X) \\VAR(Y)\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables and random vectors</span>"
    ]
  },
  {
    "objectID": "random-variables.html#correlation-and-covariance",
    "href": "random-variables.html#correlation-and-covariance",
    "title": "2  Random variables and random vectors",
    "section": "2.5 Correlation and covariance",
    "text": "2.5 Correlation and covariance\nLet \\(X\\) and \\(Y\\) be random variables defined on the same probability space.\n\nCorrelation between \\(X\\) and \\(Y\\) is defined as \\(\\EXP[XY]\\).\nCovariance between \\(X\\) and \\(Y\\) is defined as \\(\\COV(X,Y) = \\EXP[(X - μ_X) (Y - μ_Y)]\\). The covariance satisfies the following: \\[\n\\COV(X,Y) = \\EXP[XY] - \\EXP[X] \\EXP[Y].\n\\]\nCorrelation coefficient between \\(X\\) and \\(Y\\) is defined as \\[ρ_{XY} = \\frac{\\COV(X,Y)}{\\sqrt{\\VAR(X) \\VAR(Y)}}.\\]\nThe correlation coefficeint satisfies \\(\\ABS{ρ_{XY}} \\le 1\\) with equality if and only if \\(\\PR(aX + bY = c) = 1\\) for some \\(a,b,c \\in \\reals\\). [The proof follows from Cauchy-Schwartz inequality, which we will study later]\n\\(X\\) and \\(Y\\) are said to be uncorrelated if \\(ρ_{XY} = 0\\), which is equivalent to \\(\\COV(X,Y) = 0\\) or \\(\\EXP[XY] = \\EXP[X] \\EXP[Y]\\).\nNote that \\[\\begin{align*}\n  \\VAR(X + Y) &= \\EXP[ ((X - \\EXP[X]) + (Y - \\EXP[Y]) )^2 ]\n  \\\\\n  &= \\VAR(X) + \\VAR(Y) + 2\\COV(X,Y).\n\\end{align*}\\] Thus, when \\(X\\) and \\(Y\\) are uncorrelated, we have \\[ \\VAR(X + Y) = \\VAR(X) + \\VAR(Y). \\]\nIndepdent random variables are uncorrelated but the reverse in not true.\n\n\nExample 2.14 Consider the probability space \\((Ω, \\ALPHABET F, \\PR)\\) where \\(Ω = [0, 2 π)\\), \\(\\ALPHABET F\\) is the Borel \\(σ\\)-algebra on \\([0, 2 π)\\) and \\(\\PR\\) is the uniform distribution on \\(Ω\\). Define \\(X(ω) = \\cos ω\\) and \\(Y(ω) = \\sin ω\\). Show that \\(X\\) and \\(Y\\) are uncorrelated but not independent.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe event \\(\\{X = 1\\}\\) corresponds to \\(ω = 0\\) and therefore \\(\\{Y = 0\\}\\). Thus, \\(X\\) and \\(Y\\) are not independent.\nObserve that\n\n\\(\\displaystyle \\EXP[X] = \\int_{0}^{2 π} \\cos ω \\frac{1}{2 π}\\, d ω = 0\\).\n\\(\\displaystyle \\EXP[Y] = \\int_{0}^{2 π} \\sin ω \\frac{1}{2 π}\\, d ω = 0\\).\n\\(\\displaystyle \\EXP[XY] = \\int_{0}^{2 π} \\cos ω \\sin ω \\frac{1}{2 π}\\, d ω =\n\\frac{1}{4{π}} \\int_0^{2 π} \\cos 2 ω\\, d ω = 0\\).\n\nThus, \\[\\EXP[XY] = \\EXP[X]\\EXP[Y].\\]\n\n\n\n\n2.5.1 Correlation and covariance for random vectors\nSome of these concepts also generalize to random vectors. First, we define expected value for random vectors and random matrices.\n\nIf \\(X = [X_1, \\dots, X_n] \\in \\reals^n\\), then \\[ \\EXP[X] = \\MATRIX{ \\EXP[X_1] & \\cdots & \\EXP[X_n] }. \\]\nIf \\(X = \\MATRIX{ X_{1,1} & \\cdots & X_{1,n} \\\\ X_{2,1} & \\cdots & X_{2,n} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nX_{m,1} & \\cdots & X_{m,n} } \\in \\reals^{m \\times n}\\) is a ranom matrix, then \\[ \\EXP[X] = \\MATRIX{ \\EXP[X_{1,1}] & \\cdots & \\EXP[X_{1,n}] \\\\ \\EXP[X_{2,1}] & \\cdots & \\EXP[X_{2,n}] \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\EXP[X_{m,1}] & \\cdots & \\EXP[X_{m,n}] }.\n\\]\n\nWe the above notation, we can define the following:\n\nThe correlation matrix of a random vector \\(X \\in \\reals^n\\) is defined as \\[ R = \\EXP[X X^\\TRANS],\\] where \\(X^\\TRANS\\) denotes the transpose of \\(X\\).\nThe correlation matrix is symmetric, i.e., \\(R = R^\\TRANS\\)\nThe covariance matrix of a random vector \\(X \\in \\reals^n\\) is defined as \\[\\COV(X) = \\EXP[ (X - μ_X) (X - μ_X)^\\TRANS].\\]\nThe covariance matrix is symmetric. Moreover, \\([\\COV(X)]_{i,j} = \\COV(X_i,X_j)\\).\nThe cross correlation matrix of random vectors \\(X \\in \\reals^n\\) and \\(Y \\in \\reals^m\\) is a \\(n × p\\) matrix given by \\[\n  R_{XY} = \\EXP[X Y^\\TRANS].\n\\]\nThe cross covariance matrix of random vectors \\(X \\in \\reals^n\\) and \\(Y \\in \\reals^m\\) is a \\(n × p\\) matrix given by \\[\n  \\COV(X,Y) = \\EXP[ (X - μ_X) (Y - μ_Y)^\\TRANS ].\n\\]\nTwo random vectors \\(X\\) and \\(Y\\) are called uncorrelated if \\[\\EXP[ (X - μ_X) (Y - μ_Y)^\\TRANS ] = 0 \\]\nTwo random vectors \\(X\\) and \\(Y\\) are called orthogonal if \\[\\EXP[X Y^\\TRANS] = 0 \\]\nBoth the correlation and covariance matrices are positive semidefinite. Thus, their eigenvalues are real and non-negative.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables and random vectors</span>"
    ]
  },
  {
    "objectID": "random-variables.html#functions-of-random-variables",
    "href": "random-variables.html#functions-of-random-variables",
    "title": "2  Random variables and random vectors",
    "section": "2.6 Functions of random variables",
    "text": "2.6 Functions of random variables\nIn an interconnected systems, the output of one system is used as input to another system. To analyze such systems, it is important to understand how to analyze functions of random variables.\nIn particular, let \\(X\\) be a random variable defined on \\((Ω, \\ALPHABET F, \\PR)\\). Suppose \\(g \\colon \\reals \\to \\reals\\) is a (measurable) function. Define \\(Y = g(X)\\).\n\nSince \\(g\\) is measurable, for any (Borel) subset of \\(\\reals\\), we have that \\(C = g^{-1}(B) \\in \\mathscr B(\\reals)\\). Therefore, \\(X^{-1}(C) \\in \\ALPHABET F\\). Thus, we can think of \\(Y\\) as a random variable.\nSince \\(Y\\) is a random variable, it is possible to compute its CDF and PMF/PDF as appropriate. We illustrate this concept via some examples.\n\n\nExample 2.15 Suppose \\(X \\sim \\text{Uniform}[0,2]\\). Consider a function \\(g\\) given by \\[\ng(x) = \\begin{cases}\n  x & x \\in (0,1] \\\\\n  1- x & x \\in (1,2] \\\\\n  0 & \\hbox{otherwise}\n\\end{cases}\n\\] Define \\(Y = g(X)\\). Find \\(F_Y(y)\\) and \\(f_Y(y)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom the definition of \\(g\\), we know that the rannge of \\(g\\) is \\([0,1]\\). Thus, we know that the support of \\(Y\\) is \\([0,1]\\).\n\nFor any \\(y &lt; 0\\), the event \\(\\{Y \\le y\\} = \\emptyset\\). Therefore, \\(F_Y(y) = 0\\).\nFor any \\(y &gt; 1\\), the event \\(\\{Y \\le y\\} = Ω\\). Therefore, \\(F_Y(y) = 1\\).\nNow consider a \\(y \\in (0,1)\\). We have \\[\n\\{Y \\le y \\} = \\{ X \\le y \\} \\cup \\{X \\ge 2 - y \\}.\n\\] Thus, \\[\nF_Y(y) = F_X(y) + F_X(2-y) = \\frac {y}{2} + 1 - \\frac{2-y}{2} = y.\n\\] Thus, \\[\n   f_Y(y) = \\dfrac{d}{dy} F_Y(y) = 1, \\quad y \\in [0,1].\n\\] Thus, \\(Y\\) is \\(\\text{Uniform}[0,1]\\).\n\n\n\n\n\nExample 2.16 Suppose \\(X \\sim \\text{Uniform}[0,4]\\). Consider a function \\(g\\) given by \\[\ng(x) = \\begin{cases}\n  x & x \\in (0,1] \\\\\n  1 & x \\in (1, 3) \\\\\n  4- x & x \\in (3,4] \\\\\n  0 & \\hbox{otherwise}\n\\end{cases}\n\\] Define \\(Y = g(X)\\). Find \\(F_Y(y)\\) and \\(f_Y(y)\\).\n\nThe same idea can be used for functions of multiple random variables as we illustrate via the following examples.\n\nExample 2.17 Suppose \\(X \\sim \\mathcal{N}(μ,σ^2)\\). Show that \\(Z = (X - μ)/σ\\) is a standard normal random variable, i.e., \\(Z \\sim \\mathcal{N}(0,1)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can write the CDF \\(F_Z(z)\\) as \\[\\begin{align*}\nF_Z(z) &= \\PR(Z \\le z) = \\PR\\left( \\frac{X - μ}{σ} \\le z \\right)\n= \\PR(X \\le σ z + μ)\n\\\\\n&= \\int_{-∞}^{σ z + μ} \\frac{1}{\\sqrt{2 π}\\, σ} \\exp\\left(\n- \\frac{(x-μ)^2}{2 σ^2}\\, dx \\right)\n\\\\\n&= \\int_{-∞}^{z} \\frac{1}{\\sqrt{2 π}} \\exp\\left(\n- \\frac{y^2}{2}\\, dy \\right)\n\\end{align*}\\] where the last step uses the change of variables \\(y = (x-μ)/σ\\).\nThus, \\[f_Z(z) = \\frac{d F_Z(z)}{dz} = \\frac{1}{\\sqrt{2 π}} e^{-z^2/2}.\\] Thus, \\(Z \\sim \\mathcal{N}(0,1)\\).\n\n\n\n\nExample 2.18 Suppose \\(X \\sim \\text{Uniform}[0,1]\\), and \\(F \\colon \\reals \\to [0,1]\\) is a function that satisfies the following properties: there exist a pair \\((a,b)\\) with \\(a &lt; b\\) (we allow \\(a\\) to be \\(-∞\\) and \\(b\\) to be \\(∞\\)) such that\n\n\\(F(y) = 0\\) for \\(y \\le a\\)\n\\(F(y) = 1\\) for \\(y \\ge b\\)\n\\(F(y)\\) is strctly increasing in \\((a,b)\\).\n\nThus, \\(F\\) satisifies the properties of the CDF of a continuous random variable and \\(F\\) is invertible in the interval \\((a,b)\\).\nDefine \\(Y = F^{-1}(X)\\). Show that \\(F_Y(y) = F(y)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can write the CDF \\(F_Y(y)\\) as \\[\nF_Y(y) = \\PR(Y \\le y) = \\PR(F^{-1}(X) \\le y)\n\\]\nSince \\(F\\) is strictly increasing, \\(F^{-1}(X) \\le y\\) is equivalent to \\(X \\le F(y)\\). Thus, \\[\nF_Y(y) = \\PR(X \\le F(y)) = F_X(F(y)) = F(y)\n\\] where the last step uses the fact tht \\(X\\) is uniform over \\([0,1]\\).\n\n\n\n\nExample 2.19 Suppose \\(X_1\\) and \\(X_2\\) are continuous random variables and \\(Y = X_1 + X_2\\). Find the PDF \\(f_Y(y)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can write the CDF \\(F_Y(y)\\) as follows: \\[\nF_Y(y)\n= \\int_{-∞}^∞ \\int_{-∞}^{y - x_1} f_{X_1,X_2}(x_1, x_2)\\, d x_2 d x_1 \\\\\n\\] Therefore, \\[\\begin{align*}\nf_Y(y) &= \\frac{d F_Y(y)}{dy} \\\\\n&= \\int_{-∞}^∞ \\frac{d}{dy} \\int_{-∞}^{y-x_1} f_{X_1, X_2}(x_1, x_2) \\, dx_2\\, dx_1 \\\\\n&= \\int_{-∞}^∞ f_{X_1, X_2}(x_1, y - x_1)\\, dx_1.\n\\end{align*}\\]\n\n\n\n\nExample 2.20 Repeat Example 2.19 when \\(X_1\\) and \\(X_2\\) are independent.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn this case, \\(f_{X_1, X_2}(x_1, x_2) = f_{X_1}(x_1) f_{X_2}(x_2)\\). Therefore, we get \\[f_Y(y) = \\int_{-∞}^{∞} f_{X_1}(x_1) f_{X_2}(y - x_2) d x_1 = (f_{X_1} * f_{X_2})(y)\\] where \\(*\\) denotes convolution.\n\n\n\n\nExample 2.21 Repeat Example 2.20 when \\(X_1 \\sim \\text{Poisson}(λ_1)\\) and \\(X_2 \\sim \\text{Poisson}(λ_2)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall that for a Poisson random variable \\(X\\) with parameter \\(λ\\) \\[\np_X(k) = e^{-λ} \\frac{λ^k}{k!}, \\quad k \\ge 0\n\\]\nThus, \\[\\begin{align*}\np_Y(n) &= (p_{X_1} * P_{X_2})(n)\n= \\sum_{k=-∞}^{∞} p_{X_1}(k) p_{X_2}(n-k)\n\\\\\n&=\\sum_{k=0}^{n} p_{X_1}(k) p_{X_2}(n-k)  \n\\\\\n&= \\sum_{k=0}^n e^{-λ_1 - λ_2} \\frac{ λ_1^k λ_2^{n-k} }{ k! (n-k)! }\n\\\\\n&= e^{-(λ_1 + λ_2)} \\frac{1}{n!}\n\\sum_{k=0}^n \\frac{n!}{k!(n-k)!} λ_1^k λ_2^{n-k}\n\\\\\n&= e^{-(λ_1 + λ_2)} \\frac{(λ_1 + λ_2)^n}{n!}\n\\end{align*}\\]\nThus, \\(Y \\sim \\text{Poisson}(λ_1 + λ_2)\\).\n\n\n\n\nExample 2.22 Let \\(X\\) and \\(Y\\) be random variables defined on a common probability space. Define \\[\nU = \\max(X,Y)\n\\quad\nV = \\min(X,Y).\n\\] Find \\(F_U\\) and \\(F_V\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe first look at \\(F_U\\). By definition \\[ F_U(u) = \\PR(X \\le u, Y \\le u) = F_{X,Y}(u,u).\\]\nNow consider \\(F_V\\). The event \\(\\{V \\le v\\}\\) can be expressed as \\[ \\{ V \\le v \\} = \\{ X \\le v \\} \\cup \\{Y \\le v \\} \\cap \\{X \\le v \\} \\cap \\{Y \\le v\\}.\\] Thus, \\[F_V(v) = F_X(v) + F_Y(v) - F_{X,Y}(v,v). \\]\n\n\n\n\n\n2.6.1 Change of variables formulas\nFor continuous random variables, it is possible to obtain a general change of variable formula to obtain the PDF of functions of random variable in terms of their joint PDF. My personal view is that it is simpler to reason about such change of variables from first principles, but nonetheless it is good to know the results.\n\nSuppose \\(X\\) is a continuous random variable with PDF \\(f_X\\) and \\(Y = g(X)\\), where \\(g\\) is a continuous and one-to-one function (from \\(\\text{Range}(X)\\) to \\(\\text{Range}{Y}\\)). Thus, \\(g\\) must be either strictly increasing or strictly decreasing, and in both cases the inverse \\(g^{-}\\) is well defined.\n\nIf \\(g^{-1}\\) is strictly increasing, we have \\[ F_Y(y) = \\PR(Y \\le y) = \\PR(X \\le g^{-1}(y) = F_X(g^{-1}(y))\\] Therefore, \\[ f_Y(y) = \\frac{d F_X(g^{-1}(y))}{dy} = f_X(g^{-1}(y)) \\frac{d g^{-1}(y)}{dy}. \\]\nIf \\(g^{-1}\\) is strictly decreasing, we have \\[ F_Y(y) = \\PR(Y \\le y) = \\PR(X \\ge g^{-1}(y)) = 1 - F_X(g^{-1}(y)).\\] Therefore, \\[ f_Y(y) = - \\frac{d F_X(g^{-1}(y))}{dy} = - f_X(g^{-1}(y)) \\frac{d g^{-1}(y)}{dy}. \\]\nThe above two formulas can be combined as \\[ \\bbox[5pt,border: 1px solid]{f_Y(y) = f_X(g^{-1}(y)) \\left| \\frac{d g^{-1}(y)}{dy} \\right|} \\]\n\nFrom calculus, we know that if \\(h(y) = g^{-1}(y)\\), then \\(h'(y) = 1/g'(h(y))\\). Thus, the above expression can be simplified as \\[ \\bbox[5pt, border: 1px solid]{f_Y(y) = \\frac{f_X(x)}{\\ABS{g'(x)}}, \\quad \\text{where } x = g^{-1}(y).} \\]\nResolve Example 2.17 using the above formula.\nIf the transform \\(g(x)\\) is not one-to-one (as in Example 2.15), we can obtain \\(f_Y(y)\\) as follows. Suppose \\(y = g(x)\\) has finite roots, denoted by \\(\\{x^{(k)}\\}_{k=1}^m\\). Then, \\[ f_Y(y) = \\sum_{k=1}^m \\frac{f_X(x^{(k)})}{\\ABS{g'(x^{(k)})}}. \\]\nResolve Example 2.15 using the above formula.\nNow suppose \\(\\{X_1, \\dots, X_n\\}\\) are jointly continuous random variables with joint PDF \\(f\\). Consider \\(n\\) random variables: \\[\\begin{align*}\n   Y_1 &= g_1(X_1, \\dots, X_n) \\\\\n   Y_2 &= g_2(X_1, \\dots, X_n) \\\\\n   \\vdots &= \\vdots \\\\\n   Y_n &= g_n(X_1, \\dots, X_n).\n\\end{align*}\\] We can view this as an equation between two \\(n\\)-dimensional vectors \\(Y = \\VEC(Y_1, \\dots, Y_n)\\) and \\(X = \\VEC(X_1, \\dots, X_n)\\) written as \\[ Y = g(X) \\]\nAs was the case for the scalar system, for a given \\(y \\in \\reals^n\\), the vector equation \\(y = g(x)\\) may have zero, one, or multiple solutions.\n\nIf \\(y = g(x)\\), \\(y \\in \\reals^n\\) has no solution, then \\[ f_Y(y) = 0. \\]\nIf \\(y = g(x)\\), \\(y \\in \\reals^n\\) has one solution \\(x \\in \\reals^n\\), then \\[ f_Y(y) = \\frac{f_X(x)}{\\ABS{J(x)}}, \\quad \\text{where } y = g(x)\\] and \\(J(x)\\) denotes the Jacobian on \\(g(x)\\) evaluated at \\(x = (x_1, \\dots, x_n)\\), i.e., \\[\n\\def\\1#1#2{\\dfrac{∂ g_{#1}}{∂ x_{#2}}}\nJ(x_1, \\dots, x_n) =\n\\DET{ \\1 11 & \\cdots & \\1 1n \\\\\n      \\vdots & \\vdots & \\vdots \\\\\n      \\1 n1 & \\cdots & \\1 nn }\n\\]\nIf \\(y = g(x)\\), \\(y \\in \\reals^n\\) has multiple solutions given by \\(\\{x^{(1)}, \\dots, x^{(m)}\\}\\), then \\[ f_Y(y) =\n\\sum_{k=1}^m \\frac{f_X(x^{(k)})}{\\ABS{J(x^{(k)})}}.\\]\n\n\n\nExample 2.23 Resolve Example 2.22 using the change of variables formula.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(g_1(x,y) = \\max\\{x, y\\}\\) and \\(g_2(x,y) = \\min\\{x,y\\}\\). Define \\[ U = g_1(X,Y) \\quad\\text{and}\\quad V = g_2(X,Y).\\]\nDefine \\(g(x,y) = \\VEC(g_1(x,y), g_2(x,y))\\). Note that \\(g\\) is not differentiable at \\(x=y\\).\n\nWhen \\(x &gt; y\\), we have \\(g_1(x,y) = x\\) and \\(g_2(x,y) = y\\). Thus, \\[\nJ(x,y) = \\DET{\\1 11 & \\1 12 \\\\ \\1 21 & \\1 22}\n= \\DET{1 & 0 \\\\ 0 & 1} = 1. \\]\nWhen \\(x &lt; y\\), we have \\(g_1(x,y) = y\\) and \\(g_2(x,y) = x\\). Thus, \\[\nJ(x,y) = \\DET{\\1 11 & \\1 12 \\\\ \\1 21 & \\1 22}\n= \\DET{0 & 1 \\\\ 1 & 0} = -1. \\]\n\nWe now compute \\(f_{U,V}(u,v)\\).\n\nIf \\(u &lt; v\\), then the equation \\((u,v) = g(x,y)\\) has no solution. So we set \\[ f_{U,V}(u,v) = 0. \\]\nIf \\(u &gt; v\\), then the equation \\((u,v) = g(x,y)\\) has two solutions: \\(\\{ (u,v), (v,u) \\}\\). Thus, \\[\nf_{U,V}(u,v) = \\frac{f_{X,Y}(u,v)}{\\ABS{1}} + \\frac{f_{X,Y}(v,u)}{\\ABS{-1}}\n= f_{X,Y}(u,v) + f_{X,Y}(v,u). \\]\nIf \\(u = v\\), then the equation \\((u,u) = g(x,y)\\) has one solution \\((u,u)\\). Thus, \\[ f_{U,V}(u,u) = f_{X,Y}(u,u). \\] Note that \\(u = v\\) is a line in two-dimensional space. (Formally, it is a set of measure zero.) Hence, the choice of \\(f_{U,V}\\) at \\(u = v\\) will not affect any probability computations. So we can also set \\[ f_{U,V}(u,u) = 0. \\]\n\nFrom the joint PDF \\(f_{U,V}\\), we can compute the marginals as follows:\n\nFor \\(U\\), we have \\[\nf_U(u) = \\int_{-∞}^{∞} f_{U,V}(u,v) dv\n= \\int_{-∞}^{u} \\bigl[ f_{X,Y}(u,v) + f_{X,Y}(v,u) \\bigr] dv.\n\\] Therefore, \\[\nF_U(u) = \\int_{-∞}^{u} f_U(\\tilde u) d\\tilde u\n= \\int_{-∞}^u \\int_{-∞}^{\\tilde u} \\bigl[ f_{X,Y}(\\tilde u,v) + f_{X,Y}(v,\\tilde u) \\bigr] dv d\\tilde u.\n\\] Note that \\[ \\int_{-∞}^u \\int_{-∞}^{\\tilde u} f_{X,Y}(\\tilde u, v) dv d\\tilde u\n= \\int_{-∞}^u \\int_{-∞}^{x} f_{X,Y}(x, y) dy dx\n\\] and \\[\\begin{align*}\n\\int_{-∞}^u \\int_{-∞}^{\\tilde u} f_{X,Y}(v, \\tilde u) dv d\\tilde u\n&= \\int_{-∞}^u \\int_{-∞}^y f_{X,Y}(x,y) dx dy\n\\\\\n&= \\int_{-∞}^u \\int_{x}^u f_{X,Y}(x,y) dy dx\n\\end{align*}\\] where the last step follows from changing the order of integration.\nSubstituting these back in the expression for \\(F_U(u)\\), we get \\[\nF_U(u)\n= \\int_{-∞}^u \\int_{-∞}^{x} f_{X,Y}(x, y) dy dx\n+ \\int_{-∞}^u \\int_{x}^u f_{X,Y}(x,y) dy dx\n= \\int_{-∞}^u \\int_{-∞}^u f_{X,Y}(x,y) dy dx = F_{X,Y}(u,u). \\]\nFor \\(V\\), we can follow similar algebra as above.\n\n\n\n\n\nExample 2.24 Let \\(X\\) and \\(Y\\) be random variables defined on a common probability space. Define \\[\n  U = X^2 \\quad\\text{and}\\quad V = X + Y.\n\\] Find \\(F_{U,V}\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s consider the system of equations \\[\n  u = x^2 \\quad\\text{and}\\quad v = x + y\n\\] for a given value of \\((u,v)\\). First observe that \\[\n  J(x,y) = \\DET{ 2x & 0 \\\\ 1 & 1 } = 2x.\n\\]\n\nIf \\(u &lt; 0\\), then the system of equations has no solutions. Therefore, \\[\n   F_{U,V}(u,v) = 0, \\quad u &lt; 0.\n\\]\nIf \\(u = 0\\), then the system of equations has one solution: \\[\n   x^{(1)} = 0 \\quad\\text{and}\\quad y^{(1)} = v.\n\\] However, \\(J(0,v) = 0\\). So, \\[\n   f_{U,V}(0,v) = \\frac{f_{X,Y}(0,v)}{J(0,v)}\n\\] is undefined. However, since \\(u = 0\\) is a line in two-dimensions (i.e., a set of measure zero), the choice of \\(f_{U,V}\\) at \\(u = 0\\) will not affect any probability computations. So, we set \\[\n   f_{U,V}(0,v) = 0.\n\\]\nIf \\(u &gt; 0\\), then the system of equations has two solutions: \\[\n   (x^{(1)}, y^{(1)}) = (+\\sqrt{u}, v - \\sqrt{u})\n   \\quad\\text{and}\\quad\n   (x^{(2)}, y^{(2)}) = (-\\sqrt{u}, v + \\sqrt{u})\n\\] Therefore, \\[\n   f_{U,V}(u,v) = \\frac{f_{X,Y}(\\sqrt{u}, v - \\sqrt{u})}{2 \\sqrt{u}}\n   + \\frac{f_{X,Y}(-\\sqrt{u}, v + \\sqrt{u})}{2 \\sqrt{u}}.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random variables and random vectors</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html",
    "href": "conditional-expectation.html",
    "title": "3  Conditional probability and conditional expectation",
    "section": "",
    "text": "3.1 Conditioning on events\nConditional probability is perhaps the most important aspect of probability theory as it explains how to incorporate new information in a probability model. However, formally defining conditional probability is a bit intricate. In the notes, I will first provide an intuitive high-level explanation of conditional probabability. We will then do a deeper dive, trying to develop a bit more intuition about what is actually going on.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html#conditioning-on-events",
    "href": "conditional-expectation.html#conditioning-on-events",
    "title": "3  Conditional probability and conditional expectation",
    "section": "",
    "text": "Recall that conditional probability for events is defined as follows: given a probability space \\((Ω, \\ALPHABET F, \\PR)\\) and events \\(A, B \\in \\ALPHABET F\\) such that \\(\\PR(B) &gt; 0\\), we have \\[\n\\PR(A \\mid B) = \\frac{\\PR(A \\cap B)}{\\PR(B)}.\n\\]\nBuilding on this definition, we can define the conditional CDF of a random variable \\(X\\) conditioned on an event \\(C\\) (such that \\(\\PR(C) &gt; 0\\)) as follows: \\[\nF_{X|C}(x \\mid C) = \\PR(X \\le x \\mid C) = \\frac{\\PR( \\{ X \\le x \\} \\cap C)}{\\PR(C)}.\n\\]\nAs we pointed out, conditional probabilities are probabilities, the conditional CDF defined above satisfies the properties of regular CDFs. In particular\n\n\\(0 \\le F_{X|C}(x\\mid C) \\le 1\\)\n\\(\\displaystyle \\lim_{x \\to -∞} F_{X|C}(x \\mid C) = 0\\)\n\\(\\displaystyle \\lim_{x \\to +∞} F_{X|C}(x \\mid C) = 1\\)\n\\(F_{X|C}(x \\mid C)\\) is non-decreasing function.\n\\(F_{X|C}(x \\mid C)\\) is right-continuous function.\n\nSince \\(F_{X|C}\\) is CDF, we can classify random variables conditioned on an event as discrete or continuous in the usual way. In particular\n\nIf \\(F_{X|C}\\) is piecewise constant, then \\(X\\) conditioned on \\(C\\) is a discrete random variable which takes values in a finite or countable subset \\(\\text{range}(X\\mid C) = \\{x_1, x_2, \\dots\\}\\) of \\(\\reals\\). Furthermore, \\(X\\) conditioned on \\(C\\) has a conditional PMF \\(p_{X|C} \\colon \\reals \\to [0,1]\\) defined as \\[\n  p_{X|C}(x\\mid C) = F_{X|C}(x\\mid C) - F_{X|C}(x^{-} \\mid C).\n\\]\nIf \\(F_{X|C}\\) is continuous, then \\(X\\) conditioned on \\(C\\) is a continuous random variable which has a conditional PDF \\(f_{X|C}\\) given by \\[\nf_{X|C}(x\\mid C) = \\frac{d}{dx} F_X(x \\mid C).\n\\]\nIf \\(F_{X|C}\\) is neither piecewise constant nor continuous, then \\(X\\) conditioned on \\(C\\) is a mixed random variable.\n\nTherefore, a random variable conditioned on an event behaves exactly like a regular random variable. We can define conditional expectation \\(\\EXP[X \\mid C]\\), conditional variance \\(\\VAR(X \\mid C)\\) in the obvious manner.\nAn immediate implication of the law of total probability is the following.\nIf \\(C_1, C_2, \\dots, C_n\\) is a partition of \\(Ω\\), then \\[\n  F_X(x) = \\sum_{i=1}^n F_{X|C_i}(x \\mid C_i) \\PR(C_i).\n\\] Furthermore, if \\(X\\) and \\(X\\) conditioned on \\(C\\) are both discrete, we have \\[\np_X(x) = \\sum_{i=1}^n p_{X|C_i}(x \\mid C_i) \\PR(C_i)\n\\] and if \\(X\\) and \\(X\\) conditioned on \\(C\\) are both continuous, we have \\[\n  f_X(x) = \\sum_{i=1}^n f_{X|C_i}(x \\mid C_i) \\PR(C_i).\n\\]\n\n\nExercise 3.1 Consider the following experiment. A fair coin is tossed. If the outcome is heads, \\(X\\) is a uniform \\([0,1]\\) random variable. If the outcome is tails, \\(X\\) is \\(\\text{Bernoulli}(p)\\) random variable. Find \\(F_X(x)\\).\n\n\nExample 3.1 (Memoryless property of geometric random variable) Let \\(X \\sim \\text{geometric}(p)\\) and \\(m,n\\) be positive integers. Compute \\[ \\PR(X &gt; n + m \\mid X &gt; m). \\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall that the PMF of a geometric random variable is \\[\nP_X(k) = p (1-p)^{k-1}, \\quad k \\in \\naturalnumbers.\n\\] Therefore, \\[\n\\PR(X &gt; m) = \\sum_{k = m + 1}^∞ P_X(k)\n= \\sum_{k=m+1}^{∞} p (1-p)^{k-1} = (1-p)^m.\n\\]\nNow consider \\[\\begin{align*}\n\\PR(X &gt; m + n \\mid X &gt; m) &=\n\\frac{ \\PR(\\{ X &gt; m + n \\} \\cap \\{X &gt; m \\}) } {\\PR(X &gt; m) }\n\\\\\n&=\n\\frac{ \\PR(X &gt; m + n ) } {\\PR(X &gt; m) } \\\\\n&= \\frac{(1-p)^{m+n}}{(1-p)^m} = (1-p)^n = \\PR(X &gt; n).\n\\end{align*}\\]\nThis is called the memoryless property of a geometric random variable.\n\n\n\n\nExample 3.2 (Memoryless property of exponential random variable) Let \\(X \\sim \\text{Exponential}(λ)\\) and \\(t,s\\) be positive reals. Compute \\[ \\PR(X &gt; t + s \\mid X &gt; t). \\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall that the PDF of an exponential random variable is \\[\nf_X(x) = λ e^{-λ x}, \\quad x \\ge 0.\n\\] Therefore, \\[\n\\PR(X &gt; t) = \\int_{t}^{∞} f_X(x)\\, dx = e^{-λ t}.\n\\]\nNow consider \\[\\begin{align*}\n\\PR(X &gt; t + s \\mid X &gt; t) &=\n\\frac{ \\PR(\\{ X &gt; t + s \\} \\cap \\{X &gt; t \\}) } {\\PR(X &gt; t) }\n\\\\\n&=\n\\frac{ \\PR(X &gt; t + s ) } {\\PR(X &gt; t) } \\\\\n&= \\frac{e^{-λ(t+s)}}{e^{-λt}} = e^{-λs} = \\PR(X &gt; s).\n\\end{align*}\\]\nThis is called the memoryless property of a exponential random variable.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html#conditioning-on-random-variables",
    "href": "conditional-expectation.html#conditioning-on-random-variables",
    "title": "3  Conditional probability and conditional expectation",
    "section": "3.2 Conditioning on random variables",
    "text": "3.2 Conditioning on random variables\nWe first start with the case where we are conditioning on discrete random variables.\n\nIf \\(X\\) and \\(Y\\) are random variables defined on a common probability space and \\(Y\\) is discrete, then \\[F_{X|Y}(x \\mid y) = \\PR(X \\le x \\mid Y = y)\\] for any \\(y\\) such that \\(\\PR(Y = y) &gt; 0\\).\nIf \\(X\\) is also discrete, the conditional PMF \\(p_{X| Y}\\) is defined as \\[p_{X|Y}(x|y) = \\PR(X = x \\mid Y = y) = \\frac{p_{X,Y}(x,y)}{P_Y(y)}\\] for any \\(y\\) such that \\(\\PR(Y = y) &gt; 0\\).\nMoreover, we have that \\[ p_{X|Y}(x\\mid y) = F_{X|Y}(x\\mid y) - F_{X|Y}(x^{-}\\mid y). \\]\nThe above expression can be written differently to give the chain rule for random variables: \\[\n   P_{X,Y}(x,y) = P_{Y}(y) P_{X|Y}(x|y).\n\\]\nFor any event \\(B\\) in \\(\\mathscr B(\\reals)\\), the law of total probability may be written as \\[\\PR(X \\in B) = \\sum_{y \\in \\text{range}(Y)} \\PR(X \\in B \\mid Y = y) P_Y(y). \\]\nIf \\(X\\) is independent of \\(Y\\), we have \\[\n   p_{X|Y}(x\\mid y) = p_X(x), \\quad \\forall x,y \\in \\reals.\n\\]\n\nWe now consider the case when we are conditioning on a continuous random variable.\n\nIf \\(Y\\) is continuous, \\(\\PR(Y = y) = 0\\) for all \\(y\\). We may think of \\[ F_{X|Y}(x\\mid y) = \\lim_{δ \\downarrow 0} \\PR(X \\le x \\mid y \\le Y \\le y + δ).\\]\nWhen \\(X\\) and \\(Y\\) are jointly continuous, we define the conditional PDF \\[ f_{X|Y}(x \\mid y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}. \\]\nNote that the conditional PDF cannot be interpreted in the same manner as the conditional PMF because it gives the impression that we are conditioning on a zero-probability event. However, we can view it as a limit as follows:\n\\[\\begin{align*}\n   F_{X|Y}(x\\mid y) &= \\lim_{δ \\downarrow 0} \\PR(X \\le x \\mid y \\le Y \\le y + δ) \\\\\n   &= \\lim_{δ \\downarrow 0} \\dfrac{\\PR(X \\le x, y \\le Y \\le y + δ)}{\\PR(y \\le Y \\le y + δ)} \\\\\n   &= \\lim_{δ \\downarrow 0} \\dfrac{\\int_{-∞}^x \\int_{y}^{y + δ} f_{X,Y}(u,v)\\, dv\\, du }{ \\int_{y}^{y+δ} f_Y(v)\\, dv }\n\\end{align*}\\] If \\(δ\\) is small, we can approximate \\[\n\\int_{y}^{y + δ} f_Y(v)\\, dv \\approx f_Y(y) \\cdot δ\n\\] and \\[\n\\int_{y}^{y + δ} f_{X,Y}(u,v) dv \\approx f_{X,Y}(u,y) \\cdot δ.\n\\] Substituting in the above equation, we get \\[\\begin{align*}\n   F_{X|Y}(x \\mid y) &\\approx \\lim_{δ \\downarrow 0}\n   \\dfrac{ \\int_{-∞}^x f_{X,Y}(u,y) δ du }{ f_Y(y) δ }\n   \\\\\n   &= \\int_{-∞}^x \\left[ \\frac{f_{X,Y}(u,y)}{f_Y(y)} \\right] du\n\\end{align*}\\]. Thus, when \\(X\\) and \\(Y\\) are jointly continuous, we have \\[\nf_{X|Y}(x\\mid y) = \\frac{d}{dx} F_{X|Y}(x \\mid y) =\n  \\dfrac{f_{X,Y}(x,y)}{f_Y(y)}.\n\\]\nThe formal definition of conditional densities requires some ideas from advanced probability theory, which we will not cover in this course. Nonetheless, I will try to explain the intuition behind the formal definitions in the next section.\nThe above expression may be written differently to give the chain rule for random variables: \\[ f_{X,Y}(x,y) = f_Y(y) f_{X|Y}(x \\mid y). \\]\nFor any event \\(B \\in \\mathscr B(\\reals)\\), the law of total probability may be written as \\[\n\\PR(X \\in B) = \\int_{-∞}^∞ \\PR(X \\in B \\mid Y = y) f_Y(y)\\, dy\n\\] An immediate implication of this is \\[\nF_X(x) = \\PR(X \\le x) = \\int_{-∞}^{∞} \\PR(X \\le x \\mid Y = y) f_Y(y)\\, dy\n= \\int_{-∞}^{∞} F_{X|Y}(x|y) f_Y(y)\\, dy.\n\\]\nIf \\(X\\) is independent of \\(Y\\), we have \\[\nf_{X|Y}(x\\mid y) = f_X(x), \\quad \\forall x, y \\in \\reals.\n\\]\nWe can show that conditional PMF and conditional PDF satisfy all the properties of PMFs and PDFs. Therefore, we can define conditional expectation \\(\\EXP[ g(X) \\mid Y = y ]\\) in terms of \\(p_{X|Y}\\) or \\(f_{X|Y}\\). We can similarly define conditional variance\n\n\nExample 3.3 Suppose \\(X\\) and \\(Y\\) are jointly continuous random variables with the joint PDF \\[\nf_{X,Y}(x,y) = \\frac{e^{-x/y} e^{-y}}{y}, \\quad 0 &lt; x &lt; ∞, 0 &lt; y &lt; ∞.\n\\] Find \\(f_{X|Y}\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first compute the marginal \\(f_Y(y)\\).\n\\[\\begin{align*}\nf_Y(y) &= \\int_{-∞}^{∞} f_{X,Y}(x,y) \\, dx \\\\\n&= \\int_{0}^{∞} \\frac{e^{-x/y} e^{-y}}{y} dx \\\\\n&= \\frac{e^{-y}}{y} \\int_{0}^∞ e^{-x/y} dx \\\\\n&= e^{-y}.\n\\end{align*}\\] Thus, \\[ f_{X|Y}(x \\mid y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\n= \\frac{e^{-x/y}}{y}, \\quad 0 &lt; x &lt; ∞, 0 &lt; y &lt; ∞.\n\\]\n\n\n\n\nExample 3.4 Suppose \\(X \\sim \\text{Uniform}[0,1]\\) and given \\(X = x\\), \\(Y\\) is uniformly distributed on \\((0,x)\\). Find the PDF of \\(Y\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe will use the law of total probability. \\[\nF_Y(y) = \\int_{-∞}^{∞} F_{Y|X}(y \\mid x) f_X(x) \\, dx\n= \\int_{0}^1 F_{Y|X}(y \\mid x) \\, dx\n\\] where we have used the fact that \\(f_X(x) = 1\\) for \\(x \\in [0,1]\\). Now, we know that given \\(X = x\\), \\(Y \\sim \\text{uniform}[0,x]\\). Therefore, \\[\nf_{Y|X}(y\\mid x) = \\frac 1x, \\quad 0 &lt; y &lt; x.\n\\] Therefore, \\[\nF_{Y|X}(y \\mid x) = \\begin{cases}\n0 & y &lt; 0 \\\\\n\\dfrac{y}{x} & 0 &lt; y &lt; x \\\\\n1 & y \\ge x\n\\end{cases}\n\\]\nWe will compute \\(F_Y(y)\\) for the three cases separately.\n\nFor \\(y &lt; 0\\), \\[ F_Y(y) = \\int_{0}^1 F_{Y|X}(y|x) dx = 0.\\]\nFor \\(0 &lt; y &lt; 1\\), \\[ F_Y(y) = \\int_{0}^y 1\\, dx + \\int_{y}^1 \\frac{y}{x} \\, dx\n= y - y \\ln y. \\]\nFor \\(y &gt; 1\\), \\[ F_Y(y) = \\int_{0}^1 1 \\, dx = 1. \\]\n\nThus, \\[\nF_Y(y) = \\begin{cases}\n0 & y &lt; 0 \\\\\ny - y \\ln y & 0 \\le y &lt; 1 \\\\\n1 & y &gt; 1.\n\\end{cases}\n\\]\nHence, \\[\nf_Y(y) = \\frac{d F_{Y}(y)}{dy} = - \\ln y, \\quad 0 &lt; y &lt; 1.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "conditional-expectation.html#conditional-expectation",
    "href": "conditional-expectation.html#conditional-expectation",
    "title": "3  Conditional probability and conditional expectation",
    "section": "3.3 Conditional expectation",
    "text": "3.3 Conditional expectation\nDefine \\(ψ(y) = \\EXP[ X \\mid Y = y]\\). In particular, if \\(X\\) and \\(Y\\) are discrete, then \\[\nψ(y) = \\sum_{x \\in \\text{range}(X)} x p_{X|Y}(x|y)\n\\] and, if \\(X\\) and \\(Y\\) are continuous, then \\[\nψ(y) = \\int_{-∞}^{∞} x f_{X|Y}(x|y)\\, dx\n\\] Let \\(Z = ψ(Y)\\). Then \\(Z\\) is a random variable! This is a subtle point, and we will spend some time to develop an intuition of what this means.\n\n3.3.1 Conditioning on a \\(σ\\)-algebra\nThe key idea is conditioning on a \\(σ\\)-algebra. To avoid technical subtleties, we restrict to the discrete case.\n\nConsider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) where \\(\\ALPHABET F\\) is a finite \\(σ\\)-algebra. Let \\(\\ALPHABET G\\) be a sub-\\(σ\\)-algebra of \\(\\ALPHABET F\\). In particular, we assume that there is a partition \\(\\{D_1, D_2, \\dots, D_m\\}\\) of \\(Ω\\) such that \\(\\ALPHABET G = 2^{\\{D_1, \\dots, D_m\\}}\\). The elements \\(D_1, \\dots, D_m\\) are called the atoms of the \\(σ\\)-algebra \\(\\ALPHABET G\\).\n\nTODO: Add example. 4x4 grid. partition for \\(\\ALPHABET G\\).\n\nWe define \\(\\PR(A \\mid \\ALPHABET G)\\) (which we will write as \\(\\EXP[\\IND_{A} \\mid \\ALPHABET G]\\) as \\[\n\\EXP[\\IND_{A} \\mid \\ALPHABET G](ω)\n= \\sum_{i=1}^m \\EXP[ \\IND_{A} \\mid D_i ] \\IND_{D_i}(ω).\n\\] Thus, on each \\(ω \\in D_i\\), the value of \\(\\EXP[I_{A} \\mid \\ALPHABET G]\\) is equal to \\(\\EXP[I_{A} \\mid D_i]\\).\nThis idea can be extended to any random variable instead of \\(\\IND_{A}\\), that is, for any random variable \\(X\\) \\[\n\\EXP[X \\mid \\ALPHABET G](ω)\n= \\sum_{i=1}^m \\EXP[ X \\mid D_i ] \\IND_{D_i}(ω).\n\\] Thus, on each \\(ω \\in D_i\\), the value of \\(\\EXP[X \\mid \\ALPHABET G]\\) is equal to \\(\\EXP[X \\mid D_i]\\).\nWhen \\(\\ALPHABET G = \\{\\emptyset, Ω\\}\\) is the trivial \\(σ\\)-algebra, \\[\\EXP[X \\mid \\{\\emptyset, Ω\\}] = \\EXP[X].\\]\nWhen \\(X = \\IND_{A}\\), \\(\\EXP[ \\IND_{A} \\mid \\ALPHABET G] = \\PR(A \\mid \\ALPHABET G)\\).\nIf \\(X_1\\) and \\(X_2\\) are joint random variables and \\(a_1\\) and \\(a_2\\) are constants, then \\[ \\EXP[ a_1 X_1 + a_2 X_2 \\mid \\ALPHABET G]\n= a_1 \\EXP[X_1 \\mid \\ALPHABET G] + a_2 \\EXP[X_2 \\mid \\ALPHABET G]. \\]\nIf \\(Y\\) is another random variable which is \\(\\ALPHABET G\\) measurable (i.e., \\(Y\\) takes constant values on the atoms of \\(\\ALPHABET G\\)), then \\[\n\\EXP[X Y \\mid \\ALPHABET G] = Y \\EXP[X \\mid \\ALPHABET G].\n\\]\n[The result can be proved pictorially.]\n\n\n\n3.3.2 Smoothing property of conditional expectation\nLet \\(\\ALPHABET H ⊂ \\ALPHABET G ⊂ \\ALPHABET F\\), where \\(⊂\\) means sub-\\(σ\\)-algebra. Let \\(\\{E_1, \\dots, E_k\\}\\) denote the partition corresponding to \\(\\ALPHABET H\\) and \\(\\{D_1, \\dots, D_m\\}\\) denote the partition corresponding to \\(\\ALPHABET G\\). The fact that \\(\\ALPHABET H\\) is a sub-\\(σ\\)-algebra of \\(\\ALPHABET G\\) means that each atom \\(E_i\\) of \\(\\ALPHABET H\\) is a union of atoms of \\(\\ALPHABET G\\) (i.e., \\(\\{D_1, \\dots, D_m\\}\\) is a refinement of the partition \\(\\{E_1, \\dots, E_k\\}\\)). Therefore, \\[\\begin{equation}\\tag{smoothing-property}\n\\bbox[5pt,border: 1px solid]\n{\\EXP[ \\EXP[ X \\mid \\ALPHABET G ] \\mid \\ALPHABET H ] =\n\\EXP[ X \\mid \\ALPHABET H].}\n\\end{equation}\\] This is known as the smoothing property of conditional expectation.\nA special case of the above property is that \\[\n\\EXP[ \\EXP[ X \\mid \\ALPHABET G] ] =\n\\EXP[ X ].\n\\] where we have taken \\(\\ALPHABET H = \\{\\emptyset, Ω\\}\\) as the trivial \\(σ\\)-algebra. Observe that the above definition is equivalent to the law of total probability!\n\n\n3.3.3 Conditioning on random variable\n\nNow suppose \\(Y\\) is a discrete random variable, then \\(\\PR(A \\mid Y)\\) and \\(\\EXP[X \\mid Y]\\) may be viewed as a short-hand notation for \\(\\PR(A \\mid σ(Y))\\) and \\(\\EXP[X \\mid σ(Y)]\\). Similar interpretations hold for conditioning on multiple random variables (or, equivalently, conditioning on random vectors).\nThe smoothing property of conditional expectation can then be stated as \\[\n\\EXP[ \\EXP[ X | Y_1, Y_2 ] Y_1 ] = \\EXP[ X | Y_1 ].\n\\]\nAn implication of the smoothing property is the following: for any (measurable) function \\(g \\colon \\reals \\to \\reals\\), \\[ \\EXP[ g(Y) \\EXP[ X \\mid Y] ] = \\EXP[ X g(Y) ]. \\]\n\n\n\nThis previous property is used for generalizing the definition of conditional expectation to continuous random variables. First, we consider conditioning wrt \\(σ\\)-algebra \\(\\ALPHABET G \\subset \\ALPHABET F\\), which is not necessarily finite (or countable).\nThen, for any non-negative1 random variable \\(X\\), \\(\\EXP[X \\mid \\ALPHABET G]\\) is defined as a \\(\\ALPHABET G\\)-measurable random variable that satisfies \\[ \\EXP[ \\IND_{A} \\EXP[ X \\mid \\ALPHABET G] ] = \\EXP[ X \\IND_{A} ] \\] for every \\(A \\in \\ALPHABET G\\).\n\n1 We start with non-negative random variables just to avoid the concerns with existence of expectation due to \\(∞ - ∞\\) nature. A similar definition works in general as long as we can rule out \\(∞ - ∞\\) case.\nIt can be shown that \\(\\EXP[X \\mid \\ALPHABET G]\\) exists and is unique up to sets of measure zero. Formally, one takes about a “version” of conditional expectation.\nThen \\(\\EXP[X \\mid Y]\\) for \\(Y\\) continuous may be viewed as \\(\\EXP[X \\mid σ(Y)]\\).\nThe formal definition of conditional expectation implies that if we take any Borel subsets \\(B_X\\) of \\(\\reals\\), then \\(\\PR(X \\in B_X \\mid Y)\\) is a (measurable) function \\(m(y)\\) that satisfies \\[\\begin{equation}\\label{eq:defn-cond}\n\\PR(X \\in B_X, Y \\in B_Y) = \\int_{B_Y} m(y) f_Y(y) dy\n\\end{equation}\\] for all Borel subsets \\(B_Y\\) of \\(\\reals\\).\nWe will show that \\[ m(y) = \\int_{B_X} \\frac{ f_{X,Y}(x,y) }{ f_{Y}(y) } \\, dx \\] satisfies \\(\\eqref{eq:defn-cond}\\). In particular, the RHS of \\(\\eqref{eq:defn-cond}\\) is \\[\n\\int_{B_Y} \\left[ \\int_{B_X} \\frac{ f_{X,Y}(x,y) }{ f_{Y}(y) } \\, dx \\right] f_Y(y) \\, dy\n= \\int_{B_Y} \\int_{B_X} f_{X,Y}(x,y) \\, dx \\, dy\n= \\PR(X \\in B_X, Y \\in B_X)\n\\] which equals the LHS of \\(\\eqref{eq:defn-cond}\\). This is why the conditional density is defined the way it is defined!\nFinally, it can be shown that \\(\\PR(A \\mid Y) \\coloneqq \\EXP[\\IND_{A} \\mid σ(Y)]\\), \\(A \\in \\ALPHABET F\\), satisfies the axioms of probability.Therefore, conditional probability satisfies all the properties of probability (and consequently, conditional expectations satisfy all the properties of expectations).\nNote that the definition of conditional expectation generalizes Bayes rule. In particular, for any (measurable) function \\(g \\colon \\reals \\to \\reals\\) we have \\[\\begin{align*}\n\\EXP[ g(X) \\mid Y = y ] &= \\int_{-∞}^∞ g(x) f_{X|Y}(x\\mid y) \\, dx\n\\\\\n&= \\int_{-∞}^∞ g(x) \\frac{f_{X,Y}(x,y)}{f_Y(y)} \\, dx \\\\\n&= \\frac{ \\displaystyle \\int_{-∞}^{∞} g(x) f_{X,Y}(x,y)\\, dx}\n   { f_Y(y)} \\\\\n&= \\frac{ \\displaystyle \\int_{-∞}^{∞} g(x) f_{X,Y}(x,y)\\, dx}\n   {\\displaystyle \\int_{-∞}^{∞} f_{X,Y}(x,y)\\, dx } \\\\\n&= \\frac{ \\displaystyle \\int_{-∞}^{∞} g(x) f_{Y|X}(y|x) f_X(x) \\, dx}\n   {\\displaystyle \\int_{-∞}^{∞} f_{Y|X}(y|x) f_X(x) \\, dx }. \\\\\n  \\end{align*}\\]\n\n\nExercise 3.2 Let \\(X\\) and \\(Y\\) be independent and identically distributed \\(\\text{Bernoulli}(p)\\) random variables.\n\nConsider the events \\(A_k = \\{ ω : X(ω) + Y(ω) = k\\}\\), \\(k \\in \\{0,1,2\\}\\). Find \\(\\PR(A_k \\mid X)\\).\nCompute \\(\\EXP[X + Y \\mid X]\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Conditional probability and conditional expectation</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html",
    "href": "moment-generating-functions.html",
    "title": "4  Moment Generating Functions",
    "section": "",
    "text": "4.1 Moment Generating Functions\nThe moment generating function (MGF) of a random variable \\(X\\) is defined as \\[\nM_X(s) = \\EXP[e^{sX}]\n\\] provided the expectation exists.\nThe MGF of common random variables is shown in Table 4.1.\nIf there exist a neighborhood around origin where \\(M_X(s)\\) is well-defined. Then, we can use the MGF to “generate the moments” of \\(X\\) as follows:",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html#moment-generating-functions",
    "href": "moment-generating-functions.html#moment-generating-functions",
    "title": "4  Moment Generating Functions",
    "section": "",
    "text": "When \\(X\\) is discrete, we have \\[ M_X(s) = \\sum_{x \\in \\text{range}(X)} e^{sx} p_X(x). \\]\nWhen \\(X\\) is continuous, we have \\[ M_X(s) = \\int_{-∞}^∞ e^{sx} f_X(x) \\, dx. \\]\n\n\n\n\n\n\n\nRelationship to Laplace transforms\n\n\n\nAlthough most texts (including the textbook) restrict \\(s\\) to be real, my personal view is that one should really interpret \\(s\\) as a complex number. If we do so, then we have the following:\n\n\\(M_X(-s)\\) is the Laplace transform of the PDF.\n\\(M_X(-j ω)\\) is the Fourier transform of the PDF, which is called the characteristic function of \\(X\\).\n\nTherefore, we can recover the PDF by taking the inverse Laplace transform of MGF. Thus, specifying the MGF of a random variable is equivalent to specifying the PDF.\nHistorically, MGF is defined for \\(s \\in \\reals\\) and there are distributions (e.g., Cauchy) for which MGF does not exist for any \\(s \\neq 0\\). In avoid such situations, one uses the characteristic function because the characteristic function always exists. However, if we view the domain of MGF to be \\(\\mathbb{C}\\), then there is no need for a distinction between MGF and characteristic function.\n\n\n\nExample 4.1 Suppose \\(X\\) is a random variable which takes values \\(\\{0, 1, 2\\}\\) with probabilities \\(\\{\\frac 12, \\frac 13, \\frac 16\\}\\). Then, \\[\\begin{align*}\nM_X(s) &= \\EXP[e^{sX}] \\\\\n&= \\frac 12 e^{s 0} + \\frac 13 e^{s 1} + \\frac 16 e^{s 2} \\\\\n&= \\frac 12 + \\frac 13 e^{s} + \\frac 16 e^{2s}.\n\\end{align*}\\]\n\n\nExample 4.2 Find the MGF of a Poisson random variable with parameter \\(λ\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\\begin{align*}\nM_X(s) &= \\EXP[e^{sX}] = \\sum_{k=0}^{∞}e^{ks} \\frac{λ^k e^{-λ}}{k!} \\\\\n&= e^{-λ} \\sum_{k=0}^{∞} \\frac{(λe^s)^k}{k!} \\\\\n&= e^{-λ} e^{λ e^{s}}.\n\\end{align*}\\]\n\n\n\n\nExample 4.3 Find the MGF of an exponential random variable with parameter \\(λ\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\\begin{align*}\nM_X(s) &= \\EXP[e^{sX}] \\\\\n&= \\int_{0}^∞ e^{sx} λ e^{-λx} \\, dx \\\\\n&= λ \\int_{0}^∞ e^{(s-λ)x} \\, dx \\\\\n&= \\frac{λ}{λ-s}.\n\\end{align*}\\]\nNote that we could have looked up this result from the Laplace transform tables which show that \\[\ne^{at} \\xleftrightarrow{\\hskip 0.5em \\mathcal{L}\\hskip 0.5em } \\frac{1}{s-a}\n\\]\n\n\n\n\n\n\n\nTable 4.1: MGF of common random vables\n\n\n\n\n\n\n\n\n\n\nRandom variable\nParameter(s)\nMGF\n\n\n\n\nBernoulli\n\\(p\\)\n\\(1 - p + p e^s\\)\n\n\nBinomial\n\\((n,p)\\)\n\\((1-p + p e^s)^n\\)\n\n\nGeometric\n\\(p\\)\n\\(\\dfrac{p e^s}{1 - (1-p)e^s}\\)\n\n\nPoisson\n\\(λ\\)\n\\(\\exp(λ e^s - 1)\\)\n\n\nUniform\n\\((a,b)\\)\n\\(\\dfrac{e^{sb} - e^{sa}}{s(b-a)}\\)\n\n\nExponential\n\\(λ\\)\n\\(\\dfrac{λ}{λ-s}\\)\n\n\nGaussian\n\\((μ,σ)\\)\n\\(\\exp\\bigl(μ s + \\frac 12 σ^2 s^2 \\bigr)\\)\n\n\n\n\n\n\n\n\n\\(M_X(0) = 1\\)\n\\(\\dfrac{d}{ds} M_X(s) \\biggr|_{s=0} = \\EXP[X]\\).\n\\(\\dfrac{d^2}{ds^2} M_X(s) \\biggr|_{s=0} = \\EXP[X^2]\\).\nand in general \\(\\dfrac{d^k}{ds^k} M_X(s) \\biggr|_{s=0} = \\EXP[X^k]\\).\nHence, by Taylor series expansion of \\(M_X(s)\\) within the radius of convergence, we get \\[\nM_X(s) = \\sum_{k=0}^∞ \\frac{\\EXP[X^k]}{k!} s^k.\n\\] Thus, when if the MGF is well defined in a neighborhood around origin, knowing all the moments of a distribution is sufficient to construct the MGF. We already saw that the MGF is sufficient to construct the PDF/PMF of the distribution. Thus, the distribution of a random variable is completely characterized by its moments.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe first property follows from definition: \\[\nM_X(0) = \\EXP[e^{0 X}] = \\EXP[1] = 1.\n\\]\nFor the general derivative, we have \\[\\begin{align*}\n\\frac{d^k}{ds^k} M_X(s)\n&= \\int_{-∞}^∞ \\frac{d^k}{ds^k} e^{sx} f_X(x) \\, dx \\\\\n&= \\int_{-∞}^∞ x^k e^{sx} f_X(x) \\, dx.\n\\end{align*}\\]\nTherefore \\[\n\\frac{d^k}{ds^k} M_X(s) \\biggr|_{s=0} = \\int_{-∞}^∞ x^k f_X(x) \\, dx.\n\\]\n\n\n\n\nExample 4.4 Use the MGF of Bernoulli to find its first all the moments of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom Table 4.1, we see that \\[ M_X(s) = 1 - p + p e^{s}. \\] Therefore,\n\n\\(\\dfrac{d}{ds} M_X(s) = p e^s\\).\n\\(\\dfrac{d^2}{ds^2} M_X(s) = p e^s\\).\nand in general \\(\\dfrac{d^k}{ds^k} M_X(s) = p e^s\\).\n\nThus,\n\n\\(\\EXP[X] = \\dfrac{d}{ds} M_X(s) \\biggr|_{s=0} = p\\).\n\\(\\EXP[X^2] = \\dfrac{d^2}{ds^2} M_X(s) \\biggr|_{s=0} = p\\).\nand in general \\(\\EXP[X^k] = \\dfrac{d^k}{ds^k} M_X(s) \\biggr|_{s=0} = p\\).\n\n\n\n\n\n4.1.1 Moment generating functions and sums of independent random variables\n\nTheorem 4.1 Suppose \\(X_1, X_2, \\dots, X_n\\) are independent random variables defined on the same probability space. Let \\[\n  Z = X_1 + X_2 + \\dots + X_n\n\\] Then, \\[\n  M_Z(s) = M_{X_1}(s) M_{X_2}(s) \\cdots M_{X_n}(s).\n\\]\nFurthermore, if the variables are identically distributed, then \\[\n  M_Z(s) = (M_{X_1}(s))^n.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof follows immediately from properties of independent random variables. We prove the result for \\(n=2\\). \\[\nM_Z(s) = \\EXP[e^{sX_1} e^{sX_2}]\n= \\EXP[e^{sX_1} e^{sX_2}] = M_{X_1}(s) M_{X_2}(s).\n\\]\n\n\n\nTheorem 4.1 is a very useful result. An immediate implication of the result is that following:\n\nSum of i.i.d. Bernoulli random variables is a Binomial random variable\nLet \\(X_i \\sim \\text{Ber}(p)\\). Then \\(M_{X_i}(s) = (1 - p + pe^s)\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = (1 - p + pe^s)^n\\).\nSum of independent Binomial random variables with the same \\(p\\) is a Binomial random variable.\nLet \\(X_i \\sim \\text{Binom}(m_i,p)\\). Then \\(M_{X_i}(s) = (1 - p + p e^s)^{m_i}\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = (1 - p + p e^s)^M\\), where \\(M = \\sum_{i=1}^n m_i\\).\nSum of independent Poisson random variables is Poisson.\nLet \\(X_i \\sim \\text{Pois}(λ_i)\\). Then \\(M_{X_i}(s) = e^{λ_i(e^s - 1)}\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = e^{λ(e^s - 1)}\\), where \\(λ = \\sum_{i=1}^n λ_i\\).\nSum of Gaussian random variables is Gaussian.\nLet \\(X_i \\sim \\mathcal N(μ_i, σ_i)\\). Then, \\(M_{X_i}(s) = \\exp( μ_i s + \\frac 12 σ_i^2 s^2)\\).\nLet \\(Z = \\sum_{i=1}^n X_i\\). Then \\(M_Z(s) = \\exp(μ s + \\frac 12 σ^2 s^2)\\), where \\[\n   μ = \\sum_{i=1}^n μ_i\n   \\quad\\text{and}\\quad\n   σ^2 = \\sum_{i=1}^n σ_i^2.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html#the-central-limit-theorem",
    "href": "moment-generating-functions.html#the-central-limit-theorem",
    "title": "4  Moment Generating Functions",
    "section": "4.2 The Central Limit Theorem",
    "text": "4.2 The Central Limit Theorem\nOne of the ways in which MGFs are useful is that they allow us to understand the limiting behavior of sum of i.i.d. random variables.\n\n\n\n\n\n\nConvergence in distribution\n\n\n\nA sequence of random variables \\(\\{X_n\\}_{n \\ge 1}\\) is said to converge in distribution to a random variable \\(X\\) (denoted by \\(X_n \\xrightarrow{D} X\\)) if \\[\n\\lim_{n \\to \\infty} F_{X_n}(x) = F_X(x),\n\\] for all \\(x\\) where \\(F_X\\) is continuous.\n\n\n\nExample 4.5 Consider a sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables where \\(X_n \\sim \\mathcal{N}(0,1/n)\\). Show that \\(X_n \\xrightarrow{D} 0\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(F\\) denote the CDF of the constant random variable \\(X=0\\), i.e., \\[\nF(x) = \\begin{cases}\n0, & x &lt; 0 \\\\\n1, & x \\ge 0\n\\end{cases}\n\\]\nLet \\(Z\\) denote the standard Gaussian random variable. Then \\[\nF_{X_n}(x) = \\PR(X_n \\le x) = \\PR(Z \\le \\sqrt{n} x)\n\\to \\begin{cases}\n1, & x &gt; 0 \\\\\n0, & x &lt; 0\n\\end{cases}\n\\] Thus, \\(F_{X_n}(x)\\) converges to \\(F(x)\\) for all \\(x \\neq 0\\). Recall that the definition of convergence in distribution, does not require convergence of \\(F_{X_n}(x)\\) at points of discontinuity of \\(F\\). So, \\(X_n \\xrightarrow{D} 0\\).\n\n\n\nAn important implication of convergence in distribution is that for any continuous bounded function \\(g\\) \\[\n\\EXP[g(X_n)] \\to \\EXP[g(X)].\n\\] For this reason, convergence in distribution is sometimes called weak convergence.\nThe relationship between PDFs and MGFs implies the following continuity theorem:\n\n\n\n\n\n\nContinuity Theorem\n\n\n\nConsider a sequence of random variables \\(\\{X_n\\}_{n \\ge 1}\\). For ease of notation, we use \\((F_n, M_n)\\) to denote the CDF and MGF of \\(X_n\\).\n\nIf \\(F_n \\to F\\) for some CDF \\(F\\) with MGF \\(M\\), then \\(M_n(s) \\to M(s)\\) for all \\(s\\).\nConversely, if \\(M(s) = \\lim_{n\\to ∞} M_n(s)\\) exists and is continuous at \\(s = 0\\), then \\(M\\) is the MGF of some CDF \\(F\\) and \\(F_n \\to F\\).\n\n\n\nThe proof is a bit involved and technical but makes sense from a signal processing point of view: we know that if a sequence of signals \\(x_n \\to x\\), then \\(\\mathcal L(x_n) \\to \\mathcal L(x)\\).\n\n4.2.1 An illustrative example\nLet \\(\\{X_i\\}_{i \\ge 1}\\) be a sequence of i.i.d. \\(\\text{Ber}(p)\\) random variables. Define \\[\n  S_n = X_1 + \\cdots + X_n.\n\\] As we saw above, \\[\n  M_{S_n}(s) = (1 - p + p e^s)^n\n\\] which is the MGF of \\(\\text{Ber}(n,p)\\) random variable.\nWrite \\(q\\) for \\(1-p\\) and \\(σ_n^2\\) for \\(npq\\) (which is the variance of Bernoulli random variable). Define \\[\n  Z_n = \\frac{S_n - np}{σ_n}\n\\] We know that \\[\\begin{align*}\n  M_{Z_n}(s) &= \\EXP[ e^{s (S_n - np)/σ_n^2} ]\n  = e^{-s n p/σ_n} \\EXP[ e^{s S_n/σ_n}]\n  = e^{-s n p/σ_n} M_{S_n}(s/σ_n).\n  \\\\\n  &= e^{-s n p/σ_n} (q + p e^{s/σ_n})^n\n  = (q e^{-s p/σ_n} + p e^{s q / σ_n})^n\n\\end{align*}\\] Let’s consider the power series expansion of \\(q e^{-sp/σ_n} + pe^{sq/σ_n}\\): \\[\\begin{align*}\n&q\\left(1 - \\frac{ps}{σ_n} + \\frac{p^2 s^2}{2! σ_n^2} - \\frac{p^3 s^3}{3! σ_n^3} + \\cdots \\right)\n+\np\\left(1 + \\frac{qs}{σ_n} + \\frac{q^2 s^2}{2! σ_n^2} + \\frac{q^3 s^3}{3! σ_n^3} + \\cdots \\right)\n\\\\\n& \\quad= 1 + \\frac{pq s^2}{2 σ_n^2} + \\frac{pq(p-q) s^3}{6 σ_n^2} + \\cdots\n\\end{align*}\\] Thus, for large values of \\(n\\), we have \\(\\log(1 + z)^n = n(z - z^2/2 + \\cdots)\\). Therefore, \\[\n\\log M_{Z_n}(s) = \\frac{s^2}{2} + \\frac{(q-p) s^3}{6 \\sqrt{n p q}}\n+ \\text{terms of order $\\dfrac 1n$ or smaller}\n\\] Thus, the logarithm of the MGF of \\(Z_n\\) matches that of the standard normal until the \\(s^2/2\\) term. As \\(n\\) tends to infinity, the remainder terms tend to zero.\nThis convergence of \\(M_{Z_n}(s)\\) to \\(e^{s^2/2}\\) can be used to rigorously prove that the distribution of “standard Binomial” converges to standard normal as \\(n\\) tends to infinity.\n\n\n4.2.2 Two limit theorems\nBuilding on the above example, we prove a form of the law of large numbers.1\n1 Traditionally, these results are proved via the characteristic function and not the MGF. However, as discussed above, my personal view is that we can do everything with MGFs if we define them over complex numbers (as is done in Signals and Systems) rather than real numbers (as is done in probability theory).\nTheorem 4.2 (A law of large numbers) Let \\(\\{X_n\\}_{n \\ge 1}\\) be a sequence of i.i.d. random variables with finite mean \\(μ\\). Then, their partial sums \\(S_n = X_1 + \\cdots + X_n\\) satisfy \\[\n\\frac{1}{n} S_n \\xrightarrow{D} μ\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe theorem asserts that as \\(n \\to ∞\\), \\[\n\\PR(n^{-1}S_n \\le x) =\n\\begin{cases}\n  0 & \\hbox{if } x &lt; μ, \\\\\n  1 & \\hbox{if } x \\ge μ.\n\\end{cases}\n\\]\nLet \\(M_X\\) denote the MGF of \\(X_n\\). From Theorem 4.1, we know that \\[\nM_{n^{-1} S_n}(s) = M_X(s/n)^n.\n\\]\nFrom the Taylor series expansion of \\(M_X(s)\\), we know that \\[\nM_X(s/n) = 1 + μ s + o(s).\n\\] Therefore, \\[M_{n^{-1} S_n}(s) = \\left(1 + \\frac{μs}{n} + o\\left(\\frac{s}{n}\\right) \\right)^n \\to \\exp(μ s)\n\\quad \\text{as} \\quad n \\to ∞,\n\\] which is the MGF of a constant random variable.\n\n\n\nWe will show later that convergence in distribution to a constant implies converge in probability. Therefore, the above implies the weak law of large numbers.\nThe above result shows that when \\(n\\) is large, the sum \\(S_n\\) is approximately the same as \\(n μ\\). The answer to this is provided by the Central Limit Theorem, which asserts that when \\(X_n\\) has finite variance \\(σ\\):\n\n\\(S_n - n μ\\) is about as big as \\(\\sqrt{n}\\).\nIrrespective of the distribution of \\(X_n\\), \\((S_n - n μ)/\\sqrt{n}\\) converges in distribution to a normal distribution with variance \\(σ\\).\n\nWe prove the result below.\n\nTheorem 4.3 (Central Limit Theorem) Let \\(\\{X_n\\}_{n \\ge 1}\\) be i.i.d. random variables with mean \\(μ\\) and finite non-zero variance \\(σ^2\\). Then, their partial sums \\(S_n = X_1 + \\cdots + X_n\\) satisfy \\[\n\\frac{S_n - n μ}{\\sqrt{n}\\, σ}\n\\xrightarrow{D}\n{\\cal N}(0,1).\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof idea is similar to that of Theorem 4.2. Consider the “normalized” sequence of random variables \\(Y_n = (X_n - μ)/σ\\), which have mean zero and unit variance. Let \\(M_Y\\) denote their MGF. Then, the Taylor series expansion of \\(M_Y\\) gives \\[ M_Y(s) = 1 + \\frac{1}{2} s^2 + o(s^2). \\] Moreover, observe that \\[\nZ_n \\coloneqq \\frac{S_n - n μ }{\\sqrt{n}\\, σ}\n= \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n Y_i.\n\\] Therefore, by Theorem 4.1, we have \\[\nM_{Z_n}(s) = M_Y(s/\\sqrt{n})^n\n= \\left(1 + \\frac {s^2}{2n} + o\\left( \\frac {s^2}{n} \\right) \\right)^n\n\\to \\exp(-\\tfrac 12 s^2),\n\\] which is the MGF of \\({\\cal N}(0,1)\\).\n\n\n\nThe central limit theorem is one of the cornerstones of probability theory. The earliest statement of the result goes back to de Moivre (1773), but there was little follow up on that until Laplace Théorie analytique des probabilités, (1812). The term “central limit theorem” is due to Poyla (1920) who called is such because the limit theorem plays a “central role in probability theory”.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "moment-generating-functions.html#moment-generating-function-of-random-vectors",
    "href": "moment-generating-functions.html#moment-generating-function-of-random-vectors",
    "title": "4  Moment Generating Functions",
    "section": "4.3 Moment generating function of random vectors",
    "text": "4.3 Moment generating function of random vectors\nFor random vectors, \\(X \\in \\reals^n\\), the MGF is a function \\(M_X \\colon \\mathbb{C}^n \\to \\mathbb{C}\\) and for any \\(s \\in \\mathbb{C}^n\\) is given by\n\\[M_X(s) = \\EXP[e^{ \\langle s, X \\rangle }]. \\]\nTODO: Multivariate Gaussian. Relationship with correlation and covariance.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Moment Generating Functions</span>"
    ]
  },
  {
    "objectID": "inequalities.html",
    "href": "inequalities.html",
    "title": "5  Probability inequalities",
    "section": "",
    "text": "5.1 Union Bound\nWe already proved this when talking about probability spaces.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#union-bound",
    "href": "inequalities.html#union-bound",
    "title": "5  Probability inequalities",
    "section": "",
    "text": "Union bound\n\n\n\nLet \\(A_1, \\dots, A_n\\) be a collection of events. Then, \\[\n\\PR\\biggl( \\bigcup_{i=1}^n A_i \\biggr)\n\\le\n\\sum_{i=1}^n \\PR(A_n).\n\\]\n\n\n\n\nExample 5.1 Let \\(X_1, \\dots, X_n\\) be i.i.d. random variables with CDF \\(F_X(x)\\). Find an upper bound on the CDF of \\[\nZ_n = \\min(X_1, \\dots, X_n).\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe know that \\[Z_n = \\min(X_1, \\dots, X_n) \\le z\\] if and only if \\[ X_i \\le Z \\quad \\forall i \\in \\{1, \\dots, n\\}.\\] Therefore, \\[\\PR(Z_n \\le z) = \\PR\\biggl( \\bigcup_{i=1}^n \\{X_i \\le z \\} \\biggr)\n\\le \\sum_{i=1}^n \\PR(X_i \\le z) = n F_X(z).\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#cauchy-schwartz-inequality",
    "href": "inequalities.html#cauchy-schwartz-inequality",
    "title": "5  Probability inequalities",
    "section": "5.2 Cauchy-Schwartz inequality",
    "text": "5.2 Cauchy-Schwartz inequality\nThis is a generalization of :Cauchy-Schwartz inequality on inner products to random variables.\n\n\n\n\n\n\nCauchy-Schwartz inequality\n\n\n\nLet \\(X\\) and \\(Y\\) be real-valued random variables. Then, \\[\n\\EXP[XY]^2 \\le \\EXP[X^2] \\EXP[Y^2].\n\\] or equivalently, \\[\n\\COV(X,Y)^2 \\le \\VAR(X) \\VAR(Y).\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nDefine \\(f(s) = \\EXP[ (sX + Y)^2\\) for any \\(s \\in \\reals\\). Then, by linearity of expectation, we have \\[\nf(s) = \\EXP[ (sX + Y)^2 ] = s^2 \\EXP[X^2] + 2s \\EXP[XY] + \\EXP[Y^2].\n\\] We know that \\(f(s) \\ge 0\\). Recall that a quadratic form \\[\nA s^2 + B s + C \\ge 0\n\\] for all values of \\(s\\) if and only it has repeated or complex roots. Thus, the determinant must be non-positive, that is \\[\nΔ = B^2 - 4AC \\le 0.\n\\] The result follows by taking \\(A = \\EXP[X^2]\\), \\(B = 2 \\EXP[XY]\\), and \\(C = \\EXP[Y^2]\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#jensens-inequality",
    "href": "inequalities.html#jensens-inequality",
    "title": "5  Probability inequalities",
    "section": "5.3 Jensen’s inequality",
    "text": "5.3 Jensen’s inequality\nIf we take \\(Y = 1\\) in Cauchy-Schwartz inequality, we get that \\[\n\\EXP[X^2] \\ge \\EXP[X]^2.\n\\] This also follows from the fact that \\(\\VAR(X) \\ge 0\\). Jensen’s inequality may be thought as a generalization of this to convex functions.\n\n\n\n\n\n\nJensen’s inequality\n\n\n\nSuppose \\(X\\) is a real-valued random variable. Then for any convex \\(g \\colon \\reals \\to \\reals\\), we have \\[\n\\EXP[ g(X) ] \\ge g(\\EXP[X]).\n\\]\nMoreover, for any **concave* \\(g \\colon \\reals \\to \\reals\\), we have \\[\n\\EXP[ g(X) ] \\le g(\\EXP[X]).\n\\]\n\n\nFor example, since \\(g(x) = 1/x\\) is convex, we have \\[\n\\EXP\\left[\\frac 1x\\right] \\le \\frac{1}{\\EXP[X]}.\n\\] Moreover, since \\(g(x) = \\log(x)\\) is concave, we have \\[\n\\EXP[ \\log(X)] \\le \\log(\\EXP[X]).\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#markov-inequality",
    "href": "inequalities.html#markov-inequality",
    "title": "5  Probability inequalities",
    "section": "5.4 Markov inequality",
    "text": "5.4 Markov inequality\n\n\n\n\n\n\nMarkov inequality\n\n\n\nFor any non-negative random variable \\(X\\) and a number \\(a &gt; 0\\), \\[\n\\PR(X \\ge a) \\le \\frac{\\EXP[X]}{a}.\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nDefine \\(Z = \\IND\\{X \\ge a\\} a\\), which moves all the “mass” of \\(X\\) to the left.\n\nThus, \\[\n\\EXP[X] \\ge \\EXP[Z] = a \\PR(X \\ge a).\n\\]\n\n\n\n\nExample 5.2 Suppose \\(X \\sim \\text{Unif}(0,1)\\). Verify Markov inequality for \\(\\PR(X \\ge 0.2)\\), \\(\\PR(X \\ge 0.5)\\), and \\(\\PR(X \\ge 0.8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe table below compares the actual tail probability with the bound obtained from Markov inequality.\n\n\n\n\\(a\\)\n\\(\\PR(X \\ge a)\\)\n\\(\\EXP[X]/a\\)\n\n\n\n\n\\(0.2\\)\n\\(0.8\\)\n\\(0.5/0.2 = 2.5\\)\n\n\n\\(0.5\\)\n\\(0.5\\)\n\\(0.5/0.5 = 1\\)\n\n\n\\(0.8\\)\n\\(0.2\\)\n\\(0.5/0.8 = .625\\)\n\n\n\n\n\n\nExample 5.2 shows that the Markov inequality is not tight. Moreover, it gives a vaccous bound for \\(a &lt; μ\\). Thus, it only makes sense to apply the Markov inequality of \\(a \\ge μ\\).\n\n5.4.1 Union bound as a special case of Markov inequality\nNote that it is possible to derive the union bound as a corrollary of Markov inequality. Given events \\(A_1, \\dots, A_n\\), define the random variables \\(X_1, \\dots, X_n\\) as \\[\nX_i(ω) = \\IND_{A_i}(ω) = \\begin{cases}\n1, & ω \\in A_i \\\\\n0, & ω \\not\\in A_i\n\\end{cases}.\n\\] Let \\[ X = X_1 + \\dots + X_n. \\] \\(X_i\\) takes the value \\(1\\) when event \\(A_i\\) occurs. Therefore, the event \\(\\bigcup_{i=1}^n A_i\\) is equal to the event \\(X \\ge 1\\), i.e., \\[\n\\PR\\left(\\bigcup_{i=1}^n A_i \\right)\n= \\PR(X \\ge 1).\n\\] Now, by Markov inequality, we have \\[\n\\PR(X \\ge 1) \\le \\EXP[X] = \\PR(A_1) + \\cdots + \\PR(A_n).\n\\] The union bound follows by combining the above equations.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#chebyshev-inequality",
    "href": "inequalities.html#chebyshev-inequality",
    "title": "5  Probability inequalities",
    "section": "5.5 Chebyshev inequality",
    "text": "5.5 Chebyshev inequality\n\n\n\n\n\n\n\nChebyshev inequality\n\n\n\nLet \\(X\\) be a real-valued random variable with mean \\(μ\\). Then for any \\(a &gt; 0\\), we have \\[\n\\PR( \\ABS{X - μ} \\ge a) \\le \\frac{ \\VAR(X) }{a^2}.\n\\]\nAn alternative form of is as follows. Let \\(a = k σ\\). Then, \\[\n\\PR(\\ABS{X - μ} \\ge k σ) \\le \\frac{1}{k^2}.\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nObserve that \\[\\begin{align*}\n\\PR( \\ABS{X - μ} \\ge a) &= \\PR((X - μ)^2 \\ge a^2)\n\\\\\n&\\stackrel{(a)}\\le \\frac{\\EXP[ (X-μ)^2 ]}{a^2} = \\frac{\\VAR(X)}{a^2}.\n\\end{align*}\\] where \\((a)\\) follows from Markov inequality.\n\n\n\nIn the following example, we compare the strength of Chebyshev inequality compared to that of Markov inequality.\n\nExample 5.3 Let \\(X \\sim \\text{Bin}(n,p)\\) and consider any \\(q \\in (p,1)\\). Use both Markov and Chebyshev inequalities to bound \\(\\PR(X \\ge nq)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall that \\(\\EXP[X] = np\\) and \\(\\VAR(X) = n p(1-p)\\). Therefore, from Markov inequality we have \\[\n\\PR(X \\ge nq) \\le \\frac{np}{nq} = \\frac{p}{q}.\n\\] Therefore, Markov inequality does not suggest any form of concentration with \\(n\\).\nWe now consider Chebyshev inequality. To do so, we first massage the expression a bit \\[\\begin{align*}\n\\PR(X \\ge nq) &= \\PR( X - np \\ge n(q-p) ) \\\\\n&\\le \\PR( \\ABS{X - np} \\ge n(q-p) )\n\\le \\frac{np(1-p)}{n^2(q-p)^2} = \\frac{1}{n} \\cdot \\frac{p(1-p)}{(q-p)^2}\n\\end{align*}\\] which shows that \\(\\PR(X \\ge nq)\\) gets smaller as \\(n\\) increases.\n\n\n\nWe can use Chebyshev inequality to prove the weak law of large numbers.\n\n\n\n\n\n\nWeak law of large numbers\n\n\n\nLet \\(X_1, X_2, \\dots\\) be independent random variables with \\(\\EXP[X_n] = μ\\) and \\(\\VAR(X_n) = σ^2\\). Let \\[\n\\bar X_n = \\frac 1n \\sum_{i=1}^n X_i\n\\] be the sample mean. Then, \\[\\PR(\\ABS{X_n - μ} &gt; ε) \\le \\frac{σ^2}{n ε^2}. \\]\nTherefore, \\(X_n \\xrightarrow{p} μ\\) (which reads as \\(X_n\\) converges in probability to \\(μ\\); we will study convergence in probability in next lecture).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nObserve that \\[\\begin{align*}\n\\EXP[\\bar X_n] &= \\frac{1}{n} \\sum_{i=1}^n \\EXP[X_i] = μ \\\\\n\\VAR(\\bar X_n) &= \\frac{1}{n^2} \\sum_{i=1}^n \\VAR(X_i) = \\frac{σ^2}{n}.\n\\end{align*}\\]\nThen, by Chebyshev inequality, we have \\[\n\\PR(\\ABS{X_n - μ} &gt; ε) \\le \\frac{\\VAR(\\bar X_n)}{ε^2} = \\frac{σ^2}{n ε^2}.\n\\]\n\n\n\nIf we do not know \\(\\VAR(X)\\), we can still use Chebyshev inequality with an upper bound on \\(\\VAR(X)\\). For example, if \\(X \\in [a,b]\\), then we can show that \\[\n\\VAR(X) \\le \\frac{(b-a)^2}{4}.\n\\] So, for any random variable \\(X \\in [a,b]\\), we have that \\[\n\\PR(\\ABS{X - μ} &gt; ε) \\le \\frac{(b-a)^2}{4 ε^2}.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#chernoff-bound",
    "href": "inequalities.html#chernoff-bound",
    "title": "5  Probability inequalities",
    "section": "5.6 Chernoff bound",
    "text": "5.6 Chernoff bound\n\n\n\n\n\n\nChernoff Bound\n\n\n\nLet \\(X\\) be a real-valued random variable. Then, for any \\(a &gt; 0\\), we have \\[\n\\PR(X \\ge a) \\le e^{-φ(a)}\n\\] where \\[\nφ(a) = \\max_{s &gt; 0} \\{ s a - \\log M_X(s) \\}\n\\] is the Lengendre-Fenchel transform of \\(\\log M_X(s)\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof relies on two observations. First, for any \\(s &gt; 0\\), \\(e^s x\\) is an increasing function of \\(x\\). Therefore, \\[\n\\{ ω : X(ω) &gt; a \\} = \\{ ω : e^{sX(ω)} &gt; e^{sa} \\}.\n\\] Hence, \\[\n\\PR(X &gt; a) = \\PR(e^{sX} \\ge e^{sa})\n\\le \\frac{\\EXP[e^{sX}}{e^{sa}}\n= e^{-( sa - \\log M_X(s) ) }\n\\] where the inequality follows from Markov inequality\nSecond, observe that the above inequality holds for every \\(s &gt; 0\\). So, to get the tightest bound, we minimize the RHS over all \\(s &gt; 0\\), i.e., \\[\n\\PR(X &gt; a) = \\PR(e^{sX} \\ge e^{sa})\n\\le \\min_{s &gt; 0} e^{-( sa - \\log M_X(s) ) }\n\\] Since \\(e^x\\) is increasing in \\(x\\), the minimizer above is the same as the maximizer for \\[\nφ(a) = \\max_{s &gt; 0} \\{ sa - \\log M_X(s) \\}.\n\\]\n\n\n\nChernoff bound is much stronger than Markov and Chebyshev inequalities. To see that, let’s revisit the bound of Example 5.3. For a Binomial random variable, \\[\nM_X(s) = (1 - p + p e^s)^n.\n\\] Then, \\[\\PR(X \\ge nq) \\le \\exp\\left( - \\max_{s \\ge 0} \\Big[ sn q - n \\log(1-p + p e^s) \\Big] \\right)\n= \\exp\\left( - n \\max_{s \\ge 0} \\Big[ s q - \\log(1 - p + p^s) \\Big] \\right).\n\\] Even before we go ahead and compute the value in maximum of the term in square brackets, we observe that the result here is qualitatively different from that in Example 5.3. As for Chebyshev inequality, we get that the probability is going to zero, but Chernoff bound shows that the probability is going to zero exponentially fast. A bit of algebra shows that the exact bound is \\[\n\\PR(X \\ge nq) \\le \\exp\\bigl( - n D(p \\| q) \\bigr),\n\\] where \\[\nD(p \\| q) = q \\log \\left( \\frac{q}{p} \\right) + (1-q) \\log \\left( \\frac{1-q}{1 - p} \\right).\n\\]\nWe now present another example to show the tightness of the Chernoff bound.\n\nExample 5.4 Use the Chernoff bound to compute a tail bound on Gaussian random variable, i.e., for any \\(ε &gt; 0\\), bound \\(\\PR(X \\ge 0)\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "inequalities.html#azuma-hoeffding-inequality",
    "href": "inequalities.html#azuma-hoeffding-inequality",
    "title": "5  Probability inequalities",
    "section": "5.7 Azuma-Hoeffding inequality",
    "text": "5.7 Azuma-Hoeffding inequality\nAlthough Chernoff bound is fairly tight, one of the drawbacks is that it requires the knowledge of the MGF of \\(X\\). This is in contrast to Markov and Chebyshev inequalities which only require knowledge of the mean and variance. For sums of i.i.d. random variables, it is possible to get a tight bound that only depends on the mean (and a proxy for variance).\n\n\n\n\n\n\nAzuma-Hoeffding inequality\n\n\n\nLet \\(X_1, \\dots, X_n\\) be i.i.d. random variables such that \\(X_i \\in [a,b]\\) and \\(\\EXP[X_i] = μ\\). Define \\[\n\\bar X_n = \\frac 1n \\sum_{i=1}^n X_i.\n\\] Then, \\[ \\PR( \\bar X_n - μ  &gt; ε ) \\le e ^ {- 2 ε^2 n } \\] and \\[ \\PR( \\ABS{\\bar X_n - μ}  &gt; ε ) \\le 2 e ^ {- 2 ε^2 n } \\]\n\n\nWe will not provide a proof of this inequality. The Azuma-Hoeffding inequality can also be interpreted as follows. For any \\(δ &gt; 0\\), we have that the true mean lies in the interval \\[\n\\left[ \\bar X_n - \\sqrt{ \\frac{1}{2n} \\log \\frac 2 {δ} },\n       \\bar X_n + \\sqrt{ \\frac{1}{2n} \\log \\frac 2 {δ} } \\right]\n\\]\nWe can revisit Example 5.3 using Hoeffding inequality. Recall that a Binomial random variable is the sum of i.i.d. Bernoulli random variables. Therefore, we have \\[\n\\PR( X - np \\ge n (q-p) ) = \\PR( \\bar X_n - p \\ge (q - p) )\n\\le e^{-2 (q - p)^2 n}.\n\\]\nThe bound is weaker than what we obtained via Chernoff bound, but it only requires the first moment and the fact that the random variables are bounded.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability inequalities</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html",
    "href": "convergence-of-random-variables.html",
    "title": "6  Convergence of random variables",
    "section": "",
    "text": "6.1 Different notions of convergence\nSuppose we have an infinite sequence of random variables \\(\\{X_n\\}_{n \\ge 1}\\) defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\). A sequence of random variables, also called a stochastic process, can be thought of a generalization of random vectors. When does this sequence converge?\nRecall that a sequence \\(\\{x_n\\}_{n \\ge 1}\\) of real numbers converges to a limit \\(x^*\\) if for every \\(ε &gt; 0\\), there exists a \\(N\\) such that for all \\(n \\ge N\\), we have that \\[ \\ABS{ x_n - x^* } &lt; ε. \\]\nConvergence of a sequence of random variables is much different. We have already seen one form of convergence: convergence in distribution\nThere are two ways in which this notion can be extended to sequence of random variables:",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#different-notions-of-convergence",
    "href": "convergence-of-random-variables.html#different-notions-of-convergence",
    "title": "6  Convergence of random variables",
    "section": "",
    "text": "Convergence in probability. A sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to a random variale \\(X\\) in probability if \\[\n  \\lim_{n \\to ∞} \\PR( \\ABS{ X_n - X } &gt; ε ) = 0.\n\\] Or, equivalently, for any \\(ε &gt; 0\\) and \\(δ &gt; 0\\), there exists a \\(N\\) such that for all \\(n \\ge N\\), we have \\[\n  \\PR( \\ABS{ X_n - X} &gt; ε ) \\le δ.\n\\] We denote such convergence as \\(X_n \\xrightarrow{p} X\\).\nAlmost sure convergence. A sequence \\(\\{X_n\\}_{n \\ge 1}\\) of random variables converges to a random variable \\(X\\) almost surely if \\[\n  \\PR\\left( \\left\\{ ω : \\lim_{n \\to ∞}\n   X_n(ω) = X(ω) \\right\\} \\right) = 1\n\\] Or, equivalently, for any \\(ε &gt; 0\\), \\[\n  \\PR\\left( \\limsup_{n \\to ∞} \\{ ω : | X_n(ω) - X(ω) | &gt; ε \\}  \\right) = 0\n\\] We denote such convergence as \\(X_n \\xrightarrow{a.s.} X\\).\n\n\n\n\n\n\n\nLim inf and lim sup of sets\n\n\n\n\n\nExplanation adapted from math.stackexchange [1] and [2]\nLimits of sets is easy to describe when we have weakly increasing or weakly decreasing sequence of sets. In particular, if \\(\\{I_n\\}_{n \\ge 1}\\) is a weakly increasing sequence of sets, then \\[\n\\lim_{n \\to ∞} C_n = \\bigcup_{n=1}^{∞} C_n.\n\\] Thus, the limit is the union of all sets. Moreover, if \\(\\{D_n\\}_{n \\ge 1}\\) is weakly decreasing sequence of sets, then \\[\n\\lim_{n \\to ∞} D_n = \\bigcap_{n=1}^{∞} D_n.\n\\] Thus, the limit is the intersection of all sets.\nWhat happens when a sequence \\(\\{A_n\\}_{n \\ge 1}\\) is neither increasing nor decreasing? We can sandwitch it between an increasing sequence \\(\\{C_n\\}_{n \\ge 1}\\) and a decreasing sequence \\(\\{D_n\\}\\) as follows:\n\\[\n\\begin{array}{lcl}\nC_1 = A_1 \\cap A_2 \\cap A_3 \\cap \\dotsb\n&\\quad\\subseteq\\qquad\nA_1\n\\qquad\n&\\subseteq \\qquad\nD_1 = A_1 \\cup A_2 \\cup A_3 \\cup \\dotsb\n\\\\\nC_2 = \\phantom{A_1 \\cap{}} A_2 \\cap A_3 \\cap \\dotsb\n&\\quad\\subseteq\\qquad\nA_2\n\\qquad\n&\\subseteq \\qquad\nD_2 = \\phantom{A_1 \\cup{}} A_2 \\cup A_3 \\cup \\dotsb\n\\\\\nC_3 = \\phantom{A_1 \\cap A_2 \\cap{}} A_3 \\cap \\dotsb\n&\\quad\\subseteq\\qquad\nA_3\n\\qquad\n&\\subseteq \\qquad\nD_3 = \\phantom{A_1 \\cup A_2 \\cup{}} A_3 \\cup \\dotsb\n\\\\\n& \\qquad\\vdots &\n\\end{array}\n\\]\nThe limit of \\(\\{C_n\\}_{n \\ge 1}\\) is called the lim inf of \\(\\{A_n\\}_{n \\ge 1}\\), i.e., \\[\n  \\liminf_{n \\to ∞} A_n = \\lim_{n \\to ∞} C_n = \\bigcup_{n=1}^∞ C_n\n  = \\bigcup_{n=1}^{∞} \\bigcap_{i=n}^{∞} A_i.\n\\] Similarly, the limit of \\(\\{D_n\\}_{n \\ge 1}\\) is call the lim sup of \\(\\{A_n\\}_{n \\ge 1}\\), i.e., \\[\n  \\limsup_{n \\to ∞} A_n = \\lim_{n \\to ∞} D_n = \\bigcap_{n=1}^∞ D_n\n  = \\bigcap_{n=1}^{∞} \\bigcup_{i=n}^{∞} A_i.\n\\] When the two limits are equal, we say that the sequence \\(\\{A_n\\}_{n \\ge 1}\\) has a limit.\n\nAnother way to think about these definitions is as follows. \\[\n  ω \\in \\limsup_{n \\to ∞} A_n \\iff\n  \\limsup_{n \\to ∞} \\IND_{A_n}(ω) = 1\n\\] which holds if and only if the binary sequence \\(\\{\\IND_{A_n}(ω)\\}\\) has infinitely many ones, i.e., \\(ω\\) is a member of infinitely many \\(A_n\\).\nSimilarly, \\[\n  ω \\in \\liminf_{n \\to ∞} A_n \\iff\n  \\liminf_{n \\to ∞} \\IND_{A_n}(ω) = 1\n\\] which holds if and only if the binary sequence \\(\\{\\IND_{A_n}(ω)\\}\\) eventually becomes \\(1\\) forever, i.e., \\(ω\\) is eventually a member of \\(A_n\\) forever.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#examples-of-convergence-in-probability",
    "href": "convergence-of-random-variables.html#examples-of-convergence-in-probability",
    "title": "6  Convergence of random variables",
    "section": "6.2 Examples of convergence in probability",
    "text": "6.2 Examples of convergence in probability\n\nExample 6.1 Consider a probability space \\((Ω, \\ALPHABET F, \\PR)\\) where \\(Ω = [0,1]\\), \\(\\ALPHABET F = \\mathscr{B}(0,1)\\), and \\(\\PR\\) is the uniform distribution on \\(Ω\\). Define \\[\nX_n(ω) = \\begin{cases}\n  1, & ω \\in [0, \\frac{1}{n^2}) \\\\\n  0, & ω \\in [\\frac{1}{n^2}, 1]\n\\end{cases}\n\\] Show that \\(X_n \\xrightarrow{p} 0\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNote that \\[\n      X_n = \\begin{cases}\n        1 & \\text{with probability } \\frac 1{n^2} \\\\\n        0 & \\text{with probability } 1 - \\frac 1{n^2}\n      \\end{cases}\n   \\] Pick any \\(ε &gt; 0\\). If \\(ε &gt; 1\\), then \\(\\PR(X_n &gt; ε) = 0\\). So, we assume that \\(ε \\in (0,1)\\). Then, \\[\n  \\PR(X_n &gt; ε) = \\PR(X_n = 1) = \\frac 1{n^2}\n\\] which converges to \\(0\\) as \\(n \\to ∞\\). Therefore, \\(X_n \\xrightarrow{p} 0\\).\n\n\n\n\nExample 6.2 Consider the same setup as Example 6.1, but the random variable \\(X_n\\) defined as \\[\nX_n(ω) = \\begin{cases}\n  1, & ω \\in [0, \\frac{1}{n}) \\\\\n  0, & ω \\in [\\frac{1}{n}, 1]\n\\end{cases}\n\\] Show that \\(X_n \\xrightarrow{p} 0\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be solved in exactly the same manner as Example 6.1.\n\n\n\n\nExample 6.3 Consider the same setup as Example 6.1, but the random variable \\(X_n\\) defined as \\[\nX_n(ω) = \\begin{cases}\n  n, & ω \\in [0, \\frac{1}{n}) \\\\\n  0, & ω \\in [\\frac{1}{n}, 1]\n\\end{cases}\n\\] Show that \\(X_n \\xrightarrow{p} 0\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be solved in exactly the same manner as Example 6.1.\n\n\n\n\nExample 6.4 Consider an i.i.d. sequence \\(\\{X_n\\}_{n \\ge 1}\\), where \\(X_n \\sim \\text{Uniform}(0,1)\\). Define \\[\n  Y_n = \\min\\{X_1, \\dots, X_n\\}.\n\\] Show that \\(Y_n \\xrightarrow{p} 0\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPick any \\(ε &gt; 0\\). As in Example 6.1, if \\(ε &gt; 1\\), then \\(\\PR(Y_n &gt; ε) = 0\\). So, we assume that \\(ε \\in (0,1)\\). Then, \\[\\begin{align*}\n\\PR(Y_n &gt; ε) &= \\PR\\bigl( \\min\\{ X_1, \\dots, X_n \\} \\ge ε ) \\\\\n&= \\PR( X_1 \\ge ε, X_2 \\ge ε, \\dots, X_n \\ge ε ) \\\\\n&= (1-ε)^n\n\\end{align*}\\] which goes to zero as \\(n \\to ∞\\). Thus, \\(Y_n \\xrightarrow{p} 0\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#examples-of-almost-sure-convergence",
    "href": "convergence-of-random-variables.html#examples-of-almost-sure-convergence",
    "title": "6  Convergence of random variables",
    "section": "6.3 Examples of almost sure convergence",
    "text": "6.3 Examples of almost sure convergence\nWe revisit the examples of previous section.\n\nIn Example 6.1, for any \\(ω \\neq 0\\), the sequence \\(\\{X_n(ω)\\}_{n \\ge 1}\\) is a finite sequence of \\(1\\)’s followed by infinite sequence of \\(0\\)’s. Thus, \\(\\lim_{n\\to ∞} X_n(ω) = 0\\). Thus, \\[\n  \\PR\\Bigl( \\Bigl\\{ ω : \\lim_{n \\to ∞} X_n(ω) = 0 \\Bigr\\} \\Bigr)\n  = \\PR( ω \\in (0,1]) = 1.\n\\] Hence, \\(X_n \\xrightarrow{a.s} 0\\).\nIn Example 6.2, the same argument as above works.\nIn Example 6.3, a slight variation of the above argument works.\nIn Example 6.4, we proceed as follows. Fix \\(ω\\) and consider the sequence \\(\\{Y_n(ω)\\}_{n \\ge 1}\\). Since this is a decreasing sequence, it must have a limit. Denote that limit by \\(Y(ω)\\), i.e., \\[\n  Y(ω) = \\lim_{n \\to ∞} Y_n(ω).\n\\]\nSince \\(\\{Y_n\\}_{n \\ge 1}\\) is a decreasing sequence, we have that \\(Y(ω) \\le Y_n(ω)\\). Hence, for any \\(ε &gt; 0\\), \\[\n      \\PR(Y &gt; ε) \\le \\PR(Y_n &gt; ε) = (1 - ε)^n\n   \\] (where the last inequality follows from the calculation in the solution of Example 6.4).\nThe above inequality holds for every \\(n\\), so we have \\[\n      \\PR(Y &gt; ε) \\le \\lim_{n \\to ∞} (1-ε)^n = 0.\n   \\] Recall that \\(ε &gt; 0\\) was arbitrary. Therefore, we have shown that \\[\n      \\PR\\Bigl( \\lim_{n\\to ∞} Y_n &gt; ε \\Bigr) = 0.\n   \\] Thus, the only possibility is that \\[\n      \\PR\\Bigl( \\lim_{n\\to ∞} Y_n = 0 \\Bigr) = 1.\n   \\] Hence \\(Y_n \\xrightarrow{a.s.} 0\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#almost-sure-convergence-from-convergence-in-probability.",
    "href": "convergence-of-random-variables.html#almost-sure-convergence-from-convergence-in-probability.",
    "title": "6  Convergence of random variables",
    "section": "6.4 Almost sure convergence from convergence in probability.",
    "text": "6.4 Almost sure convergence from convergence in probability.\nIt is possible to infer almost sure convergence from convergence in probability. For that we need to state a result. The proof is not difficult, but is omitted due to time.\n\nLemma 6.1 (Borel Cantelli Lemma) Let \\(\\{A_n\\}_{n \\ge 1}\\) be a sequence of events defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\). If the sum of the probability of the events is finite, i.e., \\[\n\\sum_{n=1}^∞ \\PR(A_n) &lt; ∞,\n\\] then the probability that infinitely many of them occur is zero, i.e., \\[\n  \\PR\\Bigl(\\limsup_{n \\to ∞} A_n \\Bigr) = 0.\n\\]\n\nThere is a partial converse of Borel-Cantelli lemma.\n\nLemma 6.2 (Second Borel Cantelli Lemma) Let \\(\\{A_n\\}_{n \\ge 1}\\) be a sequence of independent events defined on a common probability space \\((Ω, \\ALPHABET F, \\PR)\\). If the sum of the probability of the events is infinite, i.e., \\[\n\\sum_{n=1}^∞ \\PR(A_n) = ∞,\n\\] then the probability that infinitely many of them occur is one, i.e., \\[\n  \\PR\\Bigl(\\limsup_{n \\to ∞} A_n \\Bigr) = 1.\n\\]\n\nAn immediate implication of Borel-Cantelli lemma is the following:\n\nLemma 6.3 Suppose \\(X_n \\xrightarrow{p} X\\) and for any \\(ε &gt; 0\\), we have \\[\n\\sum_{n=1}^{∞} \\PR(\\ABS{X_n - X} &gt; ε) &lt; ∞\n\\] then \\(X_n \\xrightarrow{a.s.} X\\).\n\nIn light of the above result, we revisit some variations of the examples of the previous section.\n\nConsider a variation Example 6.1 where we no longer specify \\(X_n\\) as a function of \\(ω\\) but simply assume that \\[\n   X_n = \\begin{cases}\n     1 & \\text{with probability } \\frac 1{n^2} \\\\\n     0 & \\text{with probability } 1 - \\frac 1{n^2}\n   \\end{cases}\n\\] Then for any \\(ε &gt; 0\\), \\(\\PR(\\ABS{X_n} &gt; ε) = \\PR(X_n &gt; ε) = 1/n^2\\). Therefore, \\(\\sum_{n \\ge 1} \\PR(\\ABS{X_n} &gt; ε) &lt; ∞\\); hence by Lemma 6.3, \\(X_n \\xrightarrow{a.s.} 0\\).\nConsider a variation Example 6.2 where we no longer specify \\(X_n\\) as a function of \\(ω\\) but simply assume that \\[\n   X_n = \\begin{cases}\n     1 & \\text{with probability } \\frac 1{n} \\\\\n     0 & \\text{with probability } 1 - \\frac 1{n}\n   \\end{cases}\n\\] and \\(\\{X_n\\}_{n \\ge 1}\\) are independent.\nThen for any \\(ε &gt; 0\\), \\(\\PR(\\ABS{X_n} &gt; ε) = \\PR(X_n &gt; ε) = 1/n\\). Therefore, \\(\\sum_{n \\ge 1} \\PR(\\ABS{X_n} &gt; ε) = ∞\\); hence by the Second Borel-Cantelli lemma, \\[ \\PR(\\limsup_{n \\to ∞} \\{ \\ABS{X_n} &gt; ε \\}) = 1. \\] So, \\(X_n\\) does not converge almost surely!\nIn Example 6.4, we can directly apply Lemma 6.3 to argue that \\(Y_n \\xrightarrow{a.s.} 0\\).\n\nA variation of Lemma 6.4 is the following:\n\nLemma 6.4 Let \\(\\{X_n\\}_{n \\ge 1}\\) be a sequence of random variables with finite expectations and let \\(X\\) be another random variable. If \\[\n\\sum_{n=1}^{∞} \\EXP[ \\ABS{X_n - X} ]  &lt; ∞\n\\] then \\[\nX_n \\xrightarrow{a.s.} X.\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTo simplify the notation, we assume that \\(X = 0\\).\nPick any \\(ε &gt; 0\\) and define the sequence of events \\[\nA_n = \\bigl\\{ ω : \\ABS{X_i} &gt; ε \\bigr\\},\n\\quad n \\in \\naturalnumbers.\n\\]\nFrom Markov inequality, we have \\[\\PR(A_n) = \\PR\\bigl( \\ABS{X_i} &gt; ε \\bigr)\n\\le \\dfrac{\\EXP[\\ABS{X_i}]}{ε}. \\] Therefore, \\[ \\sum_{n=1}^{∞} \\PR(A_n)\n\\le \\frac{1}{ε} \\sum_{n=1}^{∞} \\EXP[\\ABS{X_i}]\n&lt; ∞\n\\] by the hypothesis of the result. Therefore, by Borel-Cantelli lemma, we have \\[\n\\PR\\Bigl( \\limsup_{n \\to ∞} A_n \\Bigr) = 0.\n\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#some-properties-of-convergence-of-sequence-of-random-variables",
    "href": "convergence-of-random-variables.html#some-properties-of-convergence-of-sequence-of-random-variables",
    "title": "6  Convergence of random variables",
    "section": "6.5 Some properties of convergence of sequence of random variables",
    "text": "6.5 Some properties of convergence of sequence of random variables\nWe now state some properties without proof.\n\nThe three notions of convergence that we have defined are related as follows: \\[\n  [X_n \\xrightarrow{a.s.} X]\n  \\implies\n  [X_n \\xrightarrow{p} X]\n  \\implies\n  [X_n \\xrightarrow{D} X]\n\\]\n\n\n\n\n\n\n\nProof that almost sure convergence implies convergence in probability\n\n\n\n\n\nFix \\(ε &gt; 0\\). Define \\[\nA_n = \\{ ω : \\exists m \\ge n, \\ABS{X_m(ω) - X} \\ge ε \\}.\n\\] Then, \\(\\{A_n\\}_{n \\ge 1}\\) is a decreasing sequence of events. If \\(ω \\in \\bigcap_{n \\ge 1} A_n\\), then \\(X_n(ω) \\not\\xrightarrow{a.s} X(ω)\\). This implies \\[\\PR\\Bigl( \\bigcap_{n \\ge 1} A_n \\Bigr) \\le\n  \\PR\\Bigl( \\Bigl\\{ ω : \\lim_{n \\to ∞}X_n(ω) \\neq X(ω) \\Bigr\\}\\Bigr)\n= 0. \\] By continuity of probability, \\[\n\\lim_{n \\to ∞} \\PR(A_n) = \\PR\\Bigl( \\lim_{n \\to ∞} A_n \\Bigr) = 0.\n\\]\n\n\n\n\n\n\n\n\n\nProof that convergence in probability implies convergence in distribution\n\n\n\n\n\nLet \\(F_n\\) and \\(F\\) denote the CDFs of \\(X_n\\) and \\(X\\), respectively. Fix \\(ε &gt; 0\\), pick \\(x\\) such that \\(F\\) is continuous at \\(x\\), and consider \\[\\begin{align*}\n  F_n(x) &= \\PR(X_n \\le x) = \\PR(X_n \\le x, X \\le x + ε) + \\PR(X_n \\le x, X &gt; x + ε)\n\\\\\n&\\le \\PR(X \\le x + ε) + \\PR(X - X_n &gt; ε)\n\\\\\n&\\le F(x + ε) + \\PR(\\ABS{X_n - X} &gt; ε).\n\\end{align*}\\] Similarly, \\[\\begin{align*}\n  F(x-ε) &= \\PR(X \\le x-ε) = \\PR(X \\le x-ε, X_n \\le x ) + \\PR(X \\le x - ε, X_n &gt; x )\n\\\\\n&\\le \\PR(X_n \\le x) + \\PR(X_n - X &gt; ε)\n\\\\\n&\\le F_n(x) + \\PR(\\ABS{X_n - X} &gt; ε).\n\\end{align*}\\]\nThus, \\[\nF(x-ε) - \\PR(\\ABS{X_n - X} &gt; ε) \\le F_n(x) \\le F(x+ε) + \\PR(\\ABS{X_n - X} &gt; ε).\n\\] Taking \\(n \\to ∞\\), we have \\[\nF(x-ε) \\le \\liminf_{x \\to ∞} F_n(x) \\le \\limsup_{x \\to ∞} F_n(x) \\le F(x + ε).\n\\] The result is true for all \\(ε &gt; 0\\). Since \\(F\\) is continuous at \\(x\\), when we take \\(ε \\downarrow 0\\), we have \\[\nF(x - ε) \\uparrow F(x)\n\\quad\\text{and}\\quad\nF(x + ε) \\downarrow F(x)\n\\] which implies that \\(F_n(x) \\to F(x)\\).\n\n\n\n\nThere are partial converss. For any constant \\(c\\) \\[\n  [X_n \\xrightarrow{D} c]\n  \\implies\n  [X_n \\xrightarrow{p} c].\n\\] If \\(\\{X_n\\}_{n \\ge 1}\\) is a strictly decreasing sequence then \\[\n  [X_n \\xrightarrow{p} c]\n  \\implies\n  [X_n \\xrightarrow{a.s.} c].\n  \\]\nIf \\(X_n \\xrightarrow{p} X\\), then there exists a subsequence \\(\\{n_k : k \\in \\naturalnumbers \\}\\) such that \\(\\{X_{n_k}\\}_{k \\ge 1}\\) converges almost surely to \\(X\\).\n\\(X_n \\xrightarrow{p} X\\) if and only if every subsequence \\(\\{n_k : k \\in \\naturalnumbers \\}\\) has a sub-subsequence \\(\\{n_{k_m} : m \\in \\naturalnumbers \\}\\) such that \\(\\{X_{n_{k_m}} \\}_{m \\ge 1}\\) that converges to \\(X\\) almost surely.\nSkorokhod’s representation theorem. If \\(X_n \\xrightarrow{D} X\\), then there exists a sequence \\(\\{Y_n\\}_{n \\ge 1}\\) that is identically distributed to \\(\\{X_n\\}_{n \\ge 1}\\) such that \\(Y_n \\xrightarrow Y\\), where \\(Y\\) is identically distributed to \\(X\\).\nContinuous mapping theorems. Let \\(g \\colon \\reals \\to \\reals\\) be a continuous function. Then,\n\n\\(X_n \\xrightarrow{a.s.} X\\) implies \\(g(X_n) \\xrightarrow{a.s.} g(X)\\).\n\\(X_n \\xrightarrow{p} X\\) implies \\(g(X_n) \\xrightarrow{p} g(X)\\).\n\\(X_n \\xrightarrow{D} X\\) implies \\(g(X_n) \\xrightarrow{D} g(X)\\).\n\nConvergence of sums.\n\nIf \\(X_n \\xrightarrow{a.s.} X\\) and \\(Y_n \\xrightarrow{a.s.} Y\\), then \\(X_n + Y_n \\xrightarrow{a.s.} X + Y\\).\nIf \\(X_n \\xrightarrow{p} X\\) and \\(Y_n \\xrightarrow{p} Y\\), then \\(X_n + Y_n \\xrightarrow{p} X + Y\\).\nIt is not true in general that \\(X_n + Y_n \\xrightarrow{D} X + Y\\) whenever \\(X_n \\xrightarrow{p} X\\) and \\(Y_n \\xrightarrow{p} Y\\). The result is true when \\(X\\) or \\(Y\\) is a constant.\n\n\nIf \\(X_n \\xrightarrow{p} X\\) and \\(Y_n \\xrightarrow{p} Y\\), then \\(X_n + Y_n \\to X + Y\\) and \\(X_n Y_n \\xrightarrow X Y\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#strong-law-of-large-numbers",
    "href": "convergence-of-random-variables.html#strong-law-of-large-numbers",
    "title": "6  Convergence of random variables",
    "section": "6.6 Strong law of large numbers",
    "text": "6.6 Strong law of large numbers\n\nTheorem 6.1 Let \\(\\{X_n\\}_{n \\ge 1}\\) be an i.i.d. sequence of random variables with mean \\(μ\\) and variance \\(σ^2\\). Let \\[\n  \\bar X_n = \\frac 1n \\sum_{i=1}^n X_i\n\\] be the sameple average. Then, \\(\\bar X_n \\xrightarrow{a.s} μ\\), i.e., \\[\n\\PR\\Bigl( ω : \\lim_{n \\to ∞} X_n(ω) = μ \\Bigr) = 1.\n\\]\n\nWe provide a proof under the assumption that the fouth moment’s exist.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe assume that \\(μ = 0\\) (this is just for notational simplicity) and \\(\\EXP[X^4] = γ &lt; ∞\\) (this is a strong assumption).\nSince we know that the forth moment exist, we can use a forth moment version of Chebyshev inequality: \\[ \\PR \\ABS{\\bar X_n} \\ge ε) \\le \\frac{ \\EXP[ \\bar X_n^4]}{ε^2}. \\]\nThen, by the multinomial theorem, we have \\[ \\EXP{\\bar X_n^4} = \\frac{1}{4} \\EXP\\biggl[\n\\sum_{i} X_i^4 + \\binom{4}{1,3} \\sum_{i \\neq j} X_i X_j^3 + \\binom{4}{2,2} \\sum_{i \\neq j} X_i^2 X_j^2\n+ \\binom{4}{1,1,2} \\sum_{i \\neq j \\neq k} X_i X_j X_k^2 + \\sum_{i \\neq j \\neq k \\neq \\ell} X_i X_j X_k X_{\\ell} \\biggr].\n\\]\nSince the \\(\\{X_i\\}_{i \\ge 1}\\) are independent and zero mean, we have\n\n\\(\\EXP[X_i X_j^3] = \\EXP[X_i] \\EXP[X_j^3] = 0\\).\n= = 0$.\n\nTherefore, \\[\n\\EXP[\\bar X_n^4= \\frac{1}{n^4} \\bigl[ n \\EXP[X_i^4] + 3 n(n-1) \\EXP[ X_i^2 X_j^2] \\bigr]\n\\le \\frac{M_4}{n_3} + \\frac{3 σ^4}{n^2}\n\\] where \\(M_4\\) is the forth moment. Now, from the forth moment version of Chebyshev inequality, we have \\[ \\PR \\ABS{\\bar X_n} \\ge ε) \\le \\frac{ \\frac{M_4}{n_3} + \\frac{3 σ^4}{n^2} }{ε^2}. \\] This implies that \\[\n\\sum_{n=1}^{∞} \\PR(\\ABS{X_n} \\ge ε) &lt; ∞.\n\\] Thus, from Lemma 6.3, we have that \\(X_n \\xrightarrow{a.s.} 0\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "convergence-of-random-variables.html#theres-more",
    "href": "convergence-of-random-variables.html#theres-more",
    "title": "6  Convergence of random variables",
    "section": "6.7 There’s more",
    "text": "6.7 There’s more\nThere’s another type of convergence commonly used in engineering: convergence in mean-squared sense. In the interest of time, we will not study this notion in class.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Convergence of random variables</span>"
    ]
  },
  {
    "objectID": "markov-chains.html",
    "href": "markov-chains.html",
    "title": "7  Markov chains",
    "section": "",
    "text": "7.1 Time-homogeneous Markov chains\nLet \\(\\ALPHABET X\\) be a finite set. A stochastic process \\(\\{X_n\\}_{n \\ge 0}\\), \\(X_n \\in \\ALPHABET X\\), is called a Markov chain if it satisfies the Markov property: for any \\(n \\in \\integers_{\\ge 0}\\) and any \\(x_{1:n+1} \\in \\ALPHABET X^{n+1}\\), we have \\[\\begin{equation}\\tag{Markov property}\\label{eq:Markov}\n  \\PR(X_{n+1} = x_{n+1} \\mid X_{1:n} = x_{1:n})\n  = \\PR(X_{n+1} = x_{n+1} \\mid X_n = x_n).\n\\end{equation}\\]\nThe variable \\(X_n\\) is called the state of the Markov chain at time \\(n\\); the set \\(\\ALPHABET X\\) is called state space. The \\(\\eqref{eq:Markov}\\) implies that the current state captures all the information from the past that is relevant for the future. Stated differently, conditioned on the present, the past is independent of the future.\nIndependence is a symmetric relationship. Thus, we expect the Markov property to also hold if time is reversed! Exercise 7.1 asks you to formally prove that.\nWe now present some examples of Markov chains arising in different applications.\nSee the following video from Veritasium for an excellent history of Markov chains and its applications\nIn this course, we will focus on time-homogeneous Markov chain. The Markov chain is called time-homogeneous if the right hand side of \\(\\eqref{eq:Markov}\\) does not depend on \\(n\\). In this case, we describe the Markov chain by a state transition matrix \\(P\\), where \\(P_{ij} = \\PR(X_{n+1} = j | X_n = i)\\). Such Markov chains can also be visualized using state transition diagrams as we illustrate in the examples below.\nIn spite of its simplicity, such simple on-off Markov chains are used in various applications including telecommunication systems, network traffic modeling, machine failure-repair models, gene activation, and others. Some properties of the on-off Markov chain are as follows:\nNote that the gambler’s fortune in Example 7.3 is a random walk on non-negative integers \\(\\{0,1,2, \\dots\\}\\) with absorption at \\(0\\). In the variation where the gambler stops when his fortune reaches $\\(K\\), it is a random walk over \\(\\{0,1,\\dots,K\\}\\) with absorption at both ends: \\(0\\) and \\(K\\).\nTODO: Add other examples",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#time-homogeneous-markov-chains",
    "href": "markov-chains.html#time-homogeneous-markov-chains",
    "title": "7  Markov chains",
    "section": "",
    "text": "Example 7.4 (On-Off Markov chain) The On-Off Markov chain discussed earlier can be modelled with \\(\\ALPHABET X = \\{0, 1\\}\\) and general transition probability matrix of the form. \\[\n  P = \\MATRIX{ 1 - a & a \\\\ b & 1 - b }.\n\\] The transition matrix can be visualized as follows.\n\n\n\nOn-Off Markov chain\n\n\n\n\n\nIf \\(a + b = 1\\), then both rows of the transition matrix are identical. Therefore, the Markov chain has no memory and is equivalent to a Bernoulli process with success probability \\(a = 1-b\\).\nWhen \\(a\\) or \\(b\\) are small, the corresponding state is “sticky”, i.e., when the Markov chain enters a sticky state, it stays there for a long time.\n\n\nExample 7.5 The Ehrenfest model of diffusion presented in Example 7.2 can be modelled as a Markov chain with state space \\(\\{0, 1, \\dots, K\\}\\) and transition probability \\[\n  P_{ij} = \\begin{cases}\n    i/K     & j = i-1 \\\\\n    (K-i)/K & j = i + 1 \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n\\] The transition matrix can be visualized as follows.\n\n\n\nEhrenfest model of diffusion\n\n\n\n\nExample 7.6 (Random walk in one dimension) Imagine a particle which moves in a straight line in unit steps. Each step is one unit to the right with probability \\(p\\) or one unit to the left with probabity \\(q = 1-p\\). It moves until it reaches one of two extreme points, which are called boundary points. The behavior of the particle at the boundary determines several different possibilities.\nWe will consider the case where the state space is \\(\\ALPHABET X = \\{-2, -1, 0, 1, 2\\}\\) and the process starts in state \\(0\\).\n\nAbsorbing random walk: Assume that when the particle reaches a boundary state, it stays there from that time on. We may visualize the Markov chain as follows.\n\n\n\nAbsorbing random walk\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 1 & 0 & 0 & 0 & 0 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                0 & 0 & 0 & 0 & 1}.\n\\]\nReflected random walk: Assume that when the particle reaches a boundary states, it is reflected and returns to the point it came from. We may visualize the Markov chain as follows.\n\n\n\nReflected random walk\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 0 & 1 & 0 & 0 & 0 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                0 & 0 & 0 & 1 & 0}.\n\\]\nRandom walk with restart: Assume that when the particle reaches a boundary state, it restarts in the initial state. We may visualize the Markov chain as follows.\n\n\n\nRandom walk with restart\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 0 & 0 & 1 & 0 & 0 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                0 & 0 & 1 & 0 & 0}.\n\\]\nRandom walk with periodic boundary: Assume that when the particle reaches a boundary state, it moves to the other boundary. We may visualize the Markov chain as follows.\n\n\n\nRandom walk with periodic boundary\n\n\nIn this case, the transition matrix is given by \\[\n   P = \\MATRIX{ 0 & 0 & 0 & 0 & 1 \\\\\n                q & 0 & p & 0 & 0 \\\\\n                0 & q & 0 & p & 0 \\\\\n                0 & 0 & q & 0 & p \\\\\n                1 & 0 & 0 & 0 & 0}.\n\\]\n\n\n\n\n\nSuccess runs\n\n\n7.1.1 Properties of interest\nDepending on the application, we are typically interested in the following properties of a Markov chain:\n\nIf the chain starts in state \\(i\\), what is the probability that after \\(n\\) steps it is in state \\(j\\)?\nIf the chain starts in state \\(i\\), what is the expected number of visits to state \\(j\\) in \\(n\\) steps?\nWhat is the expected number of steps that it takes for a chain starting in state \\(i\\) to visit state \\(j\\) for the first time?\nWhat is the average number of times that the chain is in state \\(i\\)? How does this depend on the initial state?\n\nIn the rest of this section, we will present results in Markov chain theory that answer the above questions.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#state-occupancy-probabilities",
    "href": "markov-chains.html#state-occupancy-probabilities",
    "title": "7  Markov chains",
    "section": "7.2 State occupancy probabilities",
    "text": "7.2 State occupancy probabilities\nLet \\(μ^{(n)}\\) denote the PMF of the state of the Markov chain at time \\(n\\). This is also called the state occupancy probababilites. We will think of of \\(μ^{(n)}\\) as a row vector. Then, by the law of total probability, we have \\[\n  \\PR(X_n = j) = \\sum_{i \\in \\ALPHABET X} \\PR(X_{n-1} = i) \\PR(X_n = j | X_{n-1} = i)\n\\] or, equivalently, \\[\n  μ^{(n)}_j = \\sum_{i \\in \\ALPHABET X} μ^{(n-1)}_i P_{ij}\n\\] which can be written in matrix form as \\[ μ^{(n)} = μ^{(n-1)} P \\] and, by recusively expanding the right hand side, we have \\[ μ^{(n)} = μ^{(0)} P^n. \\]\nWe will abbreviate \\([P^n]_{ij}\\) as \\(P^{(n)}_{ij}\\).\n\nExample 7.7 Numerically compute the state occupancy probabilities for the different examples of random walk presented in Example 7.6 when \\(p = q = \\tfrac 12\\) for \\(n \\in \\{1, \\dots, 8\\}\\). In all cases, the initial probability \\[\n  μ^{(0)} = \\MATRIX{0 & 0 & 1 & 0 & 0 }.\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAbsorbing random walk: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & 0 & \\frac{1}{4} & 0 & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & \\frac{1}{8} & 0 & \\frac{1}{8} & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & 0 & \\frac{1}{8} & 0 & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & \\frac{1}{16} & 0 & \\frac{1}{16} & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{15}{32} & 0 & \\frac{1}{16} & 0 & \\frac{15}{32} \\\\\n\\end{array}\n\\right]\\)\n\nReflected random walk: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\nRandom walk with restart: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{8} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{8} & \\frac{1}{8} & \\frac{1}{2} & \\frac{1}{8} & \\frac{1}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{16} & \\frac{1}{4} & \\frac{3}{8} & \\frac{1}{4} & \\frac{1}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{8} & \\frac{3}{16} & \\frac{3}{8} & \\frac{3}{16} & \\frac{1}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{32} & \\frac{3}{16} & \\frac{7}{16} & \\frac{3}{16} & \\frac{3}{32} \\\\\n\\end{array}\n\\right]\\)\n\nRandom walk with periodic boundary: In this case, we have\n\n\\(μ^{(0)} = \\left[\n\\begin{array}{ccccc}\n0 & 0 & 1 & 0 & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(1)} = \\left[\n\\begin{array}{ccccc}\n0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(2)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(3)} = \\left[\n\\begin{array}{ccccc}\n\\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(4)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & 0 & \\frac{1}{4} & 0 & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(5)} = \\left[\n\\begin{array}{ccccc}\n\\frac{3}{8} & \\frac{1}{8} & 0 & \\frac{1}{8} & \\frac{3}{8} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(6)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & 0 & \\frac{1}{8} & 0 & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(7)} = \\left[\n\\begin{array}{ccccc}\n\\frac{7}{16} & \\frac{1}{16} & 0 & \\frac{1}{16} & \\frac{7}{16} \\\\\n\\end{array}\n\\right]\\)\n\\(μ^{(8)} = \\left[\n\\begin{array}{ccccc}\n\\frac{15}{32} & 0 & \\frac{1}{16} & 0 & \\frac{15}{32} \\\\\n\\end{array}\n\\right]\\)\n\n\n\n\n\n\nExample 7.8 Analytically compute the state occupancy probabilites for the on-off Markov chain of Example 7.4 when it starts from the initial probability distribution \\(μ^{(0)}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince \\(μ^{(n+1)} = μ^{(n)} P\\), we have \\[\\begin{align*}\n  μ^{(n+1)}_0 &= μ^{(n)}_0 (1-a) + μ^{(n)}_1 b\n  \\\\\n  &= μ^{(n)}_0 (1-a) + (1 - μ^{(n)}_0) b\n  \\\\\n  &= μ^{(n)}_0 (1-a-b) + b.\n\\end{align*}\\] If \\(a = b = 0\\), then \\(μ^{(n+1)}_0 = μ^{(n)_0 = \\cdots = μ^{(0)}_0\\). If not, we exploit the fact that \\[\n  b = \\frac{b}{a+b} - \\frac{b}{a+b}(1-a-b)  \n\\] to recursively write \\[\\begin{align*}\n  μ^{(1)}_0 &= μ^{(0)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b) + \\frac{b}{a+b}\n\\end{align*}\\] and \\[\\begin{align*}\n  μ^{(2)}_0 &= μ^{(1)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^2 + \\frac{b}{a+b}(1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^2 + \\frac{b}{a+b}\n\\end{align*}\\] and, so on, to get \\[\\begin{align*}\n  μ^{(n)}_0 &= μ^{(n-1)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^n + \\frac{b}{a+b}(1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^n + \\frac{b}{a+b}.\n\\end{align*}\\] Therefore, \\[\n  μ^{(n)}_1 = 1 - μ^{(n)}_0\n  = \\left(μ^{(1)}_0 - \\frac{a}{a+b}\\right)(1-a-b)^n + \\frac{a}{a+b}.\n\\]\n\n\n\n\n\n\n\n\n\nHow did we figure out the above calculation?\n\n\n\n\n\nThe above analysis appears to be a bit of black magic. To understand what is going on, note that we are interested in computing \\(μ^{(0)} P^n\\). What is an efficient way to compute \\(P^n\\)? Eigen decomposition!. Since \\(P\\) is a row stochastic matrix, we have \\(P \\mathbf{1} = 1.\\) Thus, \\(λ_1 = 1\\) is always an eigenvector of any transition matrix with eigenvector \\(\\mathbf{1}\\).\nFor Example 7.4, we can explicitly compute all eigenvalues by finding the roots of the characteristic equation \\[\\begin{align*}\n\\det(λI - P) &= \\DET{λ - 1 + a & - a \\\\ -b & λ - 1 + b} \\\\\n&= (λ-1 +a)(λ-1+b) - ab \\\\\n&= (λ-1)^2 + (a+b)(λ-1) = (λ-1)(λ-1 + a + b).\n\\end{align*}\\] Thus, the eigenvalues are \\(λ_1 = 1\\) and \\(λ_2 = 1 - a - b\\).\nFor the special case of \\(2 × 2\\) transition matrices, we can find the second eigenvalue by observing that \\(λ_1 = 1\\) is always an eigenvalue and \\(\\TR(P) = λ_1 + λ_2 = 1 + λ_2\\) or \\(\\det(P) = λ_1 λ_2 = λ_2\\).\nTo find the eigenvector, we find a vector \\(v\\) such that \\((λI - P)v = 0\\).\n\nFor \\(λ_1 = 1\\), we already know that \\(v_1 = [1; 1]\\) is an eigenvector.\nFor \\(λ_2 = (1-a-b)\\), we have \\[ λ_2I - P = \\MATRIX{-b & -a \\\\ -b & -a}. \\] Therefore, one possible eigenvector is \\([a; -b]\\).\n\nThen, from spectral decomposition, we know that \\[\n  P = V Λ V^{-1}\n\\] where \\(V = [v_1 v_2] = [1, a; 1, -b]\\) and \\(Λ = \\diag(1, 1-a-b)\\). Therefore, \\[\\begin{align*}\n  μ^{(n)} &= μ^{(0)} P^n = μ^{(0)} V Λ^n V^{-1} \\\\\n  &= \\MATRIX{ μ^{(0)}_0 & 1 - μ^{(0)}_0 }\n     \\MATRIX{1 & a \\\\ 1 & -b }\n     \\MATRIX{1 & 0 \\\\ 0 & 1 - a -b}\n     \\frac{1}{a+b}\n     \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &= \\MATRIX{\\dfrac{1}{a+b} & μ^{(0)}_0 - \\dfrac{b}{a+b} }\n     \\MATRIX{1 & 0 \\\\ 0 & 1 - a -b}\n     \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &=\\MATRIX{\\dfrac{1}{a+b} & 0 \\\\ 0 & \\left(μ^{(0)}_0 - \\dfrac{b}{a+b}\\right)(1-a-b)^n }\n   \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &= \\frac{b}{a+b} + \\left(μ^{(0)}_0 - \\dfrac{b}{a+b}\\right)(1-a-b)^n\n\\end{align*}\\]\n\n\n\nTODO: Add more examples!",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#class-structure",
    "href": "markov-chains.html#class-structure",
    "title": "7  Markov chains",
    "section": "7.3 Class structure",
    "text": "7.3 Class structure\n\nWe say that a state \\(j\\) is accessible from state \\(i\\) (abbreviated as \\(i \\rightsquigarrow j\\)) if there is exists an \\(m \\in \\integers_{\\ge 0}\\) (which may depend on \\(i\\) and \\(j\\)) such that \\([P^m]_{ij} &gt; 0\\). The fact that \\(P^{(m)}_{ij} &gt; 0\\) implies that there exists an ordered sequence of states \\((i_0, \\dots, i_m)\\) such that \\(i_0 = i\\) and \\(i_m = j\\) such that \\(P_{i_k i_{k+1}} &gt; 0\\); thus, there is a path of positive probability from state \\(i\\) to state \\(j\\).\nAccessibility is an transitive relationship, i.e., if \\(i \\rightsquigarrow j\\) and \\(j \\rightsquigarrow k\\) implies that \\(i \\rightsquigarrow k\\).\n\nExample 7.9 Consider the Markov chain shown below.\n\n\nIdentify all states that are accessible from state \\(1\\).\nIdentify all states from which state \\(1\\) is accessible.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nStates accessible from state \\(1\\) are \\(\\{1,2,3\\}\\).\nStates from which state \\(1\\) is accessible are \\(\\{1,2,3,4,5,6\\}\\).\n\n\n\n\nTwo distinct states \\(i\\) and \\(j\\) are said to communicate (abbreviated to \\(i \\leftrightsquigarrow j\\)) if \\(i\\) is accessible from \\(j\\) (i.e., \\(j \\rightsquigarrow i\\)) and \\(j\\) is accessible from \\(i\\) (\\(i \\rightsquigarrow j\\)). Alternatively, we say that \\(i\\) and \\(j\\) communicate if there exist \\(m, m' \\in \\integers_{\\ge 0}\\) such that \\(P^{(m)}_{ij} &gt; 0\\) and \\(P^{(m')}_{ji} &gt; 0\\).\nFor instance, in Example 7.9, state \\(1\\) communicates with state \\(2\\) but does not communicate with state \\(5\\).\nCommunication is an equivalence relationship, i.e., it is reflexive (\\(i \\leftrightsquigarrow i\\)), symmetric (\\(i \\leftrightsquigarrow j\\) if and only if \\(j \\leftrightsquigarrow i\\)), and transitive (\\(i \\leftrightsquigarrow j\\) and \\(j \\leftrightsquigarrow k\\) implies \\(i \\leftrightsquigarrow k\\)).\nThe states in a finite-state Markov chain can be partitioned into two sets: recurrent states and transient states. A state is recurrent if it is accessible from all states that are accessible from it (i.e., \\(i\\) is recurrent if \\(i \\rightsquigarrow j\\) implies that \\(j \\rightsquigarrow i\\)). States that are not recurrent are transient.\nIt can be shown that a state \\(i\\) is recurrent if and only if \\[\\sum_{m=1}^{\\infty} P^{(m)}_{ii} = \\infty.\\]\nStates \\(i\\) and \\(j\\) are said to belong to the same communicating class if \\(i\\) and \\(j\\) communicate. Communicating classes form a partition the state space. Within a communicating class, all states are of the same type, i.e., either all states are recurrent (in which case the class is called a recurrent class) or all states are transient (in which case the class is called a transient class).\nFor example, in Example 7.9, there are two communication classes: \\(\\{1,2,3\\}\\) and \\(\\{4,5,6\\}\\). The communication class \\(\\{4,5,6\\}\\) is transient while the communication class \\(\\{1,2,3\\}\\) is recurrent.\nA communicating class \\(C\\) is said to be closed if \\[\n    i \\in C \\text{ and } i \\rightsquigarrow j \\implies j \\in C.\n\\] Thus, there is no escape from a closed class. For finite state spaces, a recurrent class is always closed and a transient class is never closed. But this is not the case for countable state Markov chains.\nA state \\(i\\) is called absorbing if \\(\\{i\\}\\) is a closed class, i.e., if \\(P_{ii} = 1\\).\nA Markov chain with a single communicating class (thus, all states communicate with each other and are, therefore, recurrent) is called irreducible.\nThe period of a state \\(i\\), denoted by \\(d(i)\\), is defined as \\[d(i) = \\gcd\\{ t \\in \\integers_{\\ge 1} : [P^t]_{ii} &gt; 0 \\}.\\] If the period is \\(1\\), the state is aperiodic, and if the period is \\(2\\) or more, the state is periodic. It can be shown that all states in the same class have the same period.\nA Markov chain is aperiodic, if all states are aperiodic. A simple sufficient (but not necessary) condition for an irreducible Markov chain to be aperiodic is that there exists a state \\(i\\) such that \\(P_{ii} &gt; 0\\). In general, for a finite and aperiodic Markov chain, there exists a positive integer \\(M\\) such that \\[ P^{(m)}_{ii} &gt; 0,\n        \\quad \\forall m \\ge M, i \\in \\ALPHABET X.\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#hitting-times",
    "href": "markov-chains.html#hitting-times",
    "title": "7  Markov chains",
    "section": "7.4 Hitting times",
    "text": "7.4 Hitting times\n\nWe use the following notation:\n\nFor any event \\(E\\), \\(P_i(E)\\) denotes \\(\\PR(E \\mid X_0 = i)\\)\nFor any random variable \\(Y\\), \\(\\EXP_i[Y]\\) denotes \\(\\EXP[Y \\mid X_0 = i]\\).\n\nLet \\(A\\) be a subset of \\(\\ALPHABET X\\). The hitting time \\(H^A\\) of \\(A\\) is a random variable \\(H^A \\colon \\ALPHABET X \\to \\{0, 1, \\dots \\} \\cup \\{∞\\}\\) given by \\[\n  H^A(ω) = \\min\\{n \\ge 0 : X_n(ω) \\in A\\}.\n\\] The standard convention is that \\(H^A\\) is taken to be \\(∞\\) if \\(X_n \\neq A\\) for any \\(n &gt; 0\\). For a state \\(j \\in \\ALPHABET X\\), we use the short-hand \\(H^j\\) to denote \\(H^{\\{j\\}}\\).\nThe probability that starting from state \\(i\\) the Markov chain ever hits \\(A\\) is then given by \\[\n   h^A_i = P_i(H^A &lt; ∞).\n\\] This is called hitting probability. When \\(A\\) is a closed class, \\(h^A_i\\) is called the absorption probability.\nThe mean-time taken for the Markov chain to reach \\(A\\) is given by \\[\n  m^A_i = \\EXP_i[H^A] = \\sum_{n=0}^{∞} n \\PR_i(H^A = n).\n\\] This is called the mean hitting time.\n\nA remarkable property of Markov chain is that these quantities can be computed by solving a system of linear equations associated with the transition probability matrix \\(P\\).\n\nTheorem 7.1 (Hitting probabilities) The hitting probabilities \\(\\{h^A_i\\}_{i \\in \\ALPHABET X}\\) satisfies the following system of linear equations: \\[\n  h^A_i = \\begin{cases}\n    1, & i \\in A \\\\\n    \\sum_{j \\in \\ALPHABET X} P_{ij} h^A_j, & i \\not\\in A\n  \\end{cases}\n\\] When \\(\\ALPHABET X\\) is finite, the above system has a unique solution; when \\(\\ALPHABET X\\) is countable, the above system may have multiple solutions and the hitting probabilities correspond to the minimal non-negative solution.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWhen \\(X_0 = i \\in A\\), the hitting time \\(H^A = 0\\), so \\(h^A_i = 1\\). This proves the first part of the formula.\nFor the second part, consider \\(X_0 = i \\not\\in A\\). Then \\(H^A_i \\ge 1\\). By the Markov property, we have \\[\n\\PR_i(H^A &lt; ∞ \\mid X_1 = j) = \\PR_j(H^A &lt; ∞) = h^A_j.\n\\] Moreover, by the law of total probability, we have \\[\\begin{align*}\nh^A_i &= \\PR_i(H^A &lt; ∞) = \\sum_{j \\in \\ALPHABET X} \\PR_i(H^A &lt; ∞, X_1 = j) \\\\\n&= \\sum_{j \\in \\ALPHABET X} \\PR_i(H^A &lt; ∞ \\mid X_1 = j) \\PR_i(X_1 = j) \\\\\n&= \\sum_{j \\in \\ALPHABET X} P_{ij} h^A_j.\n\\end{align*}\\]\n\n\n\n\nExample 7.10 Consider a gambler’s ruin problem, where the gambler starts with $\\(1\\) and stops either when he is ruined or when his fortune reaches $\\(K\\).\nFind the probability of ruin (i.e., the fortune gets absorbed in state \\(0\\) rather than state \\(K\\)).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor the ease of notation, we use \\(h_i\\) as a short-form for \\(h^{\\{0\\}}_i\\). The hitting probabilities satisy the linear system of equations:\n\\[\\begin{align*}\n  h_0 &= 1 \\\\\n  h_i &= ph_{i+1} + q h_{i-1}, \\quad i \\in \\{1,\\dots,K-1\\} \\\\\n  h_K &= 0,\n\\end{align*}\\] where \\(q = 1-p\\).\nThe characteristic equation associated with the linear recurrence relationship is \\[\n  λ = p λ^2 + q\n\\] which has two distinct roots, \\(λ_1 = 1\\) and \\(λ_2 = q/p\\) if \\(p \\neq q\\) and a double root at \\(λ_1 = 1\\) if \\(p = q = \\frac 12\\). Therefore, the general solution is of the form \\[\n  h_i = a λ_1^i + b λ_2^i = a + b\\Bigl(\\tfrac {q}{p} \\Bigr)^i\n\\] where we determine the coefficients \\(a\\) and \\(b\\) from the boundary conditions \\(h_0 = 1\\) and \\(h_K = 0\\). Solving for \\(a\\) and \\(b\\), we get that for \\(p \\neq q\\), we have \\[\n  h_i = \\frac{1 - \\Bigl(\\frac qp\\Bigr)^i}{1 - \\Bigl(\\frac qp\\Bigr)^K}\n\\] and for \\(p = q = \\frac 12\\), we have \\[\n  h_i = \\frac{i}{K}.\n\\]\n\n\n\n\nTheorem 7.2 (Mean hitting times) The mean hitting times \\(\\{m^A_i\\}_{i \\in \\ALPHABET X}\\) satisfies the following system of linear equations: \\[\n  m^A_i = \\begin{cases}\n    0, & i \\in A \\\\\n    1 + \\sum_{j \\not\\in A} P_{ij} m^A_j, & i \\not\\in A\n  \\end{cases}\n\\] When \\(\\ALPHABET X\\) is finite, the above system has a unique solution; when \\(\\ALPHABET X\\) is countable, the above system may have multiple solutions and the mean hitting times correspond to the minimal non-negative solution.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe proof is similar to the proof of Theorem 7.1. When \\(X_0 = i \\in A\\), the hitting time \\(H^A = 0\\), so \\(m^A_i = 0\\). This proves the first part of the formula.\nFor the second part, consider \\(X_0 = i \\not\\in A\\). Then \\(H^A_i \\ge 1\\). By the Markov property, we have \\[\n  \\EXP_i[ H^A \\mid X_1 = j] = 1 + \\EXP_j[ H^A ] = 1 + m^A_j.\n\\] Moreover, by the law of total probability, we have \\[\\begin{align*}\n  m^A_i &= \\EXP_i[ H^A ]\n  = \\sum_{j \\in \\ALPHABET X} \\EXP_i[ H^A \\mid X_1 = j ] \\PR_i(X_1 = j) \\\\\n  &= \\sum_{j \\in \\ALPHABET X} \\EXP_i[ H^A \\mid X_1 = j ] \\PR_i(X_1 = j) \\\\\n  &= \\sum_{j \\in \\ALPHABET X} P_{ij} \\bigl[ 1 + m^A_j \\bigr]\n  \\\\\n  &= 1 + \\sum_{j \\not\\in A} P_{ij} m^A_j.\n\\end{align*}\\]",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#first-passage-time-and-strong-markov-property",
    "href": "markov-chains.html#first-passage-time-and-strong-markov-property",
    "title": "7  Markov chains",
    "section": "7.5 First passage time and strong Markov property",
    "text": "7.5 First passage time and strong Markov property\n\nDefine the first passage time \\[\n  f^{(n)}_{ij} \\coloneqq P_i(T_j = n) = P_i(X_1 \\neq j, \\dots, X_{n-1} \\neq j, X_n = j).\n\\]\n\\(f^{(n)}_{ij}\\) satisfies the following recursion.\n\n\\(f^{(1)}_{ij} = P_i(X_1 = j) = P_{ij}\\).\nAnd for \\(n &gt; 1\\), \\(f^{(n+1)}_{ij} = \\sum_{k \\neq j} P_{ik} f^{(n)}_{kj}\\).\n\nLet \\[\n  f_{ij} = \\sum_{n=1}^∞ f^{(n)}_{ij} = P_i(T_j &lt; ∞)\n\\] denotes the probability that a chain starting in \\(i\\) eventually visits \\(j\\). In particular \\(f_{jj}\\) denotes the probability that a chain starting in \\(j\\) will return to \\(j\\).\n\\(f_{ij}\\) satisfy the following property.\n\\[\\displaystyle P^{(n)}_{ij} = \\sum_{m=1}^n  f^{(m)}_{ij} P^{(n-m)}_{jj}.\\]\nAn immediate implication of the above is that if \\(j\\) is an absorbing state then \\(P^{(n)}_{ij} = \\sum_{m=1}^n f^{(m)}_{ij} = P_i(T_j \\le n).\\)\nLet \\(N^{(n)}_j = \\sum_{m=1}^{n} \\IND\\{X_m = j\\}\\) denote the number of visits to state \\(j\\) in \\(n\\) steps. Define \\[\n  G_{ij}^{(n)} = \\EXP_i[ N^{(n)}_j ] = \\sum_{m=1}^n P^{(m)}_{ij}\n\\] to be the expected number of visits to state \\(j\\) in \\(n\\) steps, starting in \\(i\\).\nLet \\(N_j = \\lim_{n \\to ∞} N^{(n)}_j \\sum_{m=1}^∞ \\IND\\{X_m = j \\}\\) denote the number of visits to state \\(j\\). Similarly, define \\[\n   G_{ij} = \\EXP_{i}[N_j] = \\lim_{n \\to ∞} G_{ij}^{(n)}\n   = \\sum_{m=1}^{∞} P^{(m)}_{ij}\n\\] to denote the expected number of visits to state \\(j\\) for a chain starting in \\(i\\).\nA state \\(j\\) is recurrent if \\(f_{jj} = 1\\) and transient if \\(f_{jj} &lt; 1\\).\nA state is called periodic if \\(f_{ii}^(n)\\) is non-zero only for multiples of some smallest integer \\(d\\), \\(d &gt; 1\\).\nFor every transient state \\(j\\), we have for every \\(i\\), \\(P_i(N_j &lt; ∞) = 1\\) and \\[ G_{ij} = \\dfrac{f_{ij}}{1 - f_{jj}}.\\] On the other hand, if \\(j\\) is recurrent, then \\(P_j(N_j = ∞) = 1\\) and \\(G_{jj} = ∞\\). Moreover, \\[ P_i(N_j = ∞) = P_i(T_j &lt; ∞) = f_{ij}. \\] So, if \\(f_{ij} = 0\\), then \\(G_{ij} = 0\\) while if \\(f_{ij} = ∞\\) then \\(G_{ij} = ∞\\).\nThus, a state \\(i\\) is recurrent if and only if \\[G_{ii} = \\sum_{n=1}^∞ P^{(n)}_{ii} = ∞. \\]\nA state \\(j\\) is said to be accessible from \\(i\\) (abbreviated as \\(i \\rightsquigarrow j\\)) if there is an ordered string of notes \\((i_0, \\dots, i_m)\\) such that \\(i_0 = i\\) and \\(i_m = j\\) and \\(P_{i_k i_{k+1}} &gt; 0\\). Equivalently, \\(i \\rightsquigarrow j\\) if there exists a \\(m\\) such that \\(P^{(m)}_{ij} &gt; 0\\).\nAccessibility is an transitive relationship, i.e., if \\(i \\rightsquigarrow j\\) and \\(j \\rightsquigarrow k\\) implies that \\(i \\rightsquigarrow k\\).\nIf \\(f_{ij} &gt; 0\\) but \\(f_{ji} &lt; 1\\), then \\(i\\) is transient.\nIf \\(i\\) is recurrent and \\(i \\rightsquigarrow j\\). Then, \\(j\\) is also recurrent and \\(f_{ij} = f_{ji} = 1\\).\nA subset \\(C\\) of \\(\\ALPHABET X\\) is said to be closed if no state inside \\(C\\) can lead to any state outside \\(C\\), i.e., \\[\n    f_{ij} = 0, \\quad \\forall i \\in C \\text{ and } j \\not\\in C.\n\\]\nA closed set \\(C\\) is called irreducible if \\(i \\rightsquigarrow j\\) for all \\(i,j \\in C\\). Thus, if \\(C\\) is an irreducible set, then all states in \\(C\\) are either recurrent or transient.\nConsequently, if \\(C\\) is an irreducible and closed set of recurrent states. Then for all \\(i,j \\in C\\),\n\n\\(f_{ij} = 1\\)\n\\(P_i(N_j = ∞) = 1\\)\n\\(G_{ij} = ∞\\)\n\nIf \\(C\\) is a finite irreducible closed set of states. Then every state in \\(C\\) is recurrent.\nLet \\(\\ALPHABET X_T\\) and \\(\\ALPHABET X_R\\) denote the set of transient and recurrent states. The set \\(\\ALPHABET X_R\\) can be paritioned into a finite or countable number of irreducible closed sets \\(C_1\\), \\(C_2\\), \\(\\dots\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#expected-duration-of-play",
    "href": "markov-chains.html#expected-duration-of-play",
    "title": "7  Markov chains",
    "section": "7.6 Expected duration of play",
    "text": "7.6 Expected duration of play\nLet’s start with a simple example. Suppose we toss a coin multiple times and stop at a heads. What are the expected number of tosses until stopping?\nFrom elementary probability we know that the number of tosses until stopping is a geometric random variable. However, we will model this using a Markov chain where the state denotes the number of consecutive heads so far. Let \\(p\\) denote the probability of heads and \\(q = 1-p\\) denote the probability of tails. Then, the Markov chain model is as follows.\n\n\n\nMarkov chain for coin tossing until one head\n\n\nLet \\(v_i\\) denote the expected number of tosses until stopping when starting at state \\(i\\). Then, we have \\[\\begin{align*}\n  v_0 &= 1 + q v_0 + p v_1, \\\\\n  v_1 &= 0.\n\\end{align*}\\] Solving this system of equations, we get \\(v_0 = 1/(1-q) = 1/p\\).\nNow, let’s try a variation of the above model. Suppose we toss a coin multiple times and stop at two heads. What are the expected number of tosses until stopping.\nWe can model this in the same manner as the before, where the state denotes the number of consecutive heads so far. The Markov chain is as follows:\n\n\n\nMarkov chain for coin tossing until two heads\n\n\nAs before, let \\(v_i\\) denote the expected number of tosses until stopping when starting at state \\(i\\). Then, we have \\[\\begin{align*}\n  v_0 &= 1 + q v_0 + p v_1, \\\\\n  v_1 &= 1 + q v_0 + p v_2, \\\\\n  v_2 &= 0.\n\\end{align*}\\] Solving this system of equations, we get \\(v_0 = 1/(1-p)\\).\nWe can generalize these ideas to find time of hitting a state.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#stationary-distribution",
    "href": "markov-chains.html#stationary-distribution",
    "title": "7  Markov chains",
    "section": "7.7 Stationary distribution",
    "text": "7.7 Stationary distribution\n\nA distribution \\(π\\) is said the be a stationary distribution if \\[\n   π = π P.\n\\]\nStationary distributions can be computed by solving balance equations.\nLet \\(C\\) be an irreducible class of a Markov chain. Then, there is a unique stationary distribution \\(π\\) that assigns positive probability only to states in \\(C\\).\nIf \\(π_1\\) and \\(π_2\\) are stationary distributions of a Markov chain and \\(α \\in (0,1)\\), then \\(α π_1 + (1-α) π_2\\) is also a stationary distribution.\nThus, if a Markov chain has a single irreducible class, then it has a unique stationary distribution; if it has multiple irreducible classes, then it has uncountable number of stationary distributions.",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#limiting-distribution",
    "href": "markov-chains.html#limiting-distribution",
    "title": "7  Markov chains",
    "section": "7.8 Limiting distribution",
    "text": "7.8 Limiting distribution\n\nSuppose \\(j\\) is a transient state. Then, we know that \\(N_j &lt; ∞\\) and \\(G_{ij} &lt; ∞\\). Therefore, \\[\n   \\lim_{n \\to ∞}\n   \\frac{N^{(n)}_{j}}{n} = 0, a.s.,\n   \\quad\\text{and}\\quad\n   \\lim_{n \\to ∞}\n   \\frac{G^{(n)}_{ij}}{n} = 0, \\forall i \\in \\ALPHABET X.\n\\]\nSuppose \\(j\\) is a recurrent state. Then, \\[\n   \\lim_{n \\to ∞}\n   \\frac{N^{(n)}_{j}}{n} = μ_j, a.s.,\n   \\quad\\text{and}\\quad\n   \\lim_{n \\to ∞}\n   \\frac{G^{(n)}_{ij}}{n} = μ_j, \\forall i \\in \\ALPHABET X\n\\] where \\(μ_j = \\EXP_{j}[T_j]\\) is the mean return time to state \\(j\\).\n\nA Markov chain is said to have a steady state distribution if \\(π_n\\) converges to a limit as \\(n \\to ∞\\) and the limit does not depend on the initial distribution \\(π_0\\).\nA Markov chain has a steady state distribution if it is ergodic. We can find the steady state distribution by solving the balance equation: \\(π = π P\\).",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  },
  {
    "objectID": "markov-chains.html#exercises",
    "href": "markov-chains.html#exercises",
    "title": "7  Markov chains",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 7.1 (Time reversal of Markov chains) Let \\(\\{X_n\\}_{n \\ge 1}\\) is a Markov chain. Show that for any \\(N &gt; n\\), \\[\n  \\PR(X_n = x_n \\mid X_{n+1:N} = x_{n+1:N})\n  = \\PR(X_n = x_n \\mid X_{n+1} = x_{n+1}).\n\\] Thus, a time reversed Markov chain is also Markov.\n\n\nExercise 7.2 Suppose \\(\\{X_n\\}_{n \\ge 0}\\) is a Markov chain with transition matrix \\(P\\). For a fixed positive integer \\(k\\), define \\(Y_n = X_{kn}\\). Show that \\(\\{Y_n\\}_{n \\ge 0}\\) is a Markov chain with transition matrix \\(P^k\\).\n\n\nExercise 7.3 Suppose a (6-sided) die is ‘fixed’ so that two consecutive rolls cannot have the same outcome. In particular, if the outcome of a roll is \\(i\\), then the next roll cannot be \\(i\\); all \\(5\\) other outcomes are equally likely.\n\nModel the above as a Markov chain.\nIf the outcome of the first roll is \\(1\\), what is the probability that the outcome of the \\(n\\)th roll is also \\(1\\)?",
    "crumbs": [
      "Probability and Random Signals II",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Markov chains</span>"
    ]
  }
]