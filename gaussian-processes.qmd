---
title: Gaussian processes
execute:
  echo: false
---

## Gaussian processes

1. Recall that a discrete-time stochastic process is a collection $\{X_n\}_{n \ge 0}$ of random variables defined on a common probability space where we specify the joint distribution for any collection $\{n_1, \dots, n_k\}$ of time indices. 

2. A discrete-time stochastic process is called **Gaussian** if for any collection $\{n_1, \dots, n_k\}$ of time indices, the random vector $\{X_{n_1}, \dots, X_{n_k}\}$ has a multivariate Guassian distribution. 

     Recall that the definition of multivariable Gaussian random variables means we can equivalently define a discrete-time stochastic process $\{X_n\}_{n \ge 0}$ to be a Gaussian process if every finite linear combination of the random variables $X_n$ is a Gaussian random variable.

3. Since a multivariate Gaussian distribution is completely characterized by its mean vector and covariance matrix, a Gaussian process is completely characterized by:
   - **Mean function:** $μ_X(n) = \EXP[X_n]$
   - **Covariance function:** $R_X(m,n) = \COV(X_m, X_n) = \EXP[(X_m - μ_X(m))(X_n - μ_X(n))]$

4. Recall that a stochastic process is **strict sense stationary** if for any finite collection of time indices $\{n_1, n_2, \dots, n_k\}$ with $n_i \ge 0$, the random vector $(X_{n_1}, X_{n_2}, \dots, X_{n_k})$ has the same distribution as the random vector $(X_{n_1 + h}, X_{n_2 + h}, \dots, X_{n_k + h})$ for any $h \ge 0$. For a Gaussian process, this is equivalent to the mean function being constant and the covariance function depending only on the time difference, i.e.,
$$
μ_X(n) = μ_X \quad \text{for all } n \ge 0, \quad R_X(m,n) = R_X(|n-m|) \quad \text{for all } m, n \ge 0.
$$
An implication is that the variance of $X_n$ does not depend on $n$.

## Properties of covariance functions

1. To fix ideas, consider three time indices $(\ell,n,m)$. Then, the random vector $\{X_\ell, X_n, X_m\}$ has a multivariate Gaussian distribution given by
   $$ 
     \MATRIX{X_{\ell}\\ X_n\\ X_m} \sim \mathcal{N}\left( \MATRIX{\mu_X(\ell)\\ \mu_X(n)\\ \mu_X(m)}, \MATRIX{R_X(\ell,\ell) & R_X(\ell,n) & R_X(\ell,m) \\ R_X(n,\ell) & R_X(n,n) & R_X(n,m) \\ R_X(m,\ell) & R_X(m,n) & R_X(m,m)} \right)
   $$
   The covariance matrix above must be symmetric, non-negative definite, and satisfy the Cauchy-Schwarz inequality, i.e., $\COV(X_m, X_n)^2 \le \COV(X_m, X_m) \COV(X_n, X_n)$. These properties are true in general as well. 

2. **Symmetry:** $R_X(m,n) = R_X(n,m)$ for all $m, n \ge 0$.

   :::{.callout-note collapse="true"}
   #### Proof
   $R_X(m,n) = \COV(X_m, X_n) = \COV(X_n, X_m) = R_X(n,m)$ by the symmetry of the covariance operator.
   :::

3. **Non-negative definiteness:** For any finite collection $\{n_1, \dots, n_k\}$ with $n_i \ge 0$ and any real numbers $a_1, \dots, a_k$, we have
   $$
   \sum_{i=1}^k \sum_{j=1}^k a_i a_j R_X(n_i, n_j) \ge 0.
   $$

   :::{.callout-note collapse="false"}
   #### Proof
   Let $\Sigma$ denote the covariance matrix of the random vector $\{X_{n_1}, \dots, X_{n_k}\}$. Then, we have
   $$
   \Sigma = \MATRIX{R_X(n_1,n_1) & R_X(n_1,n_2) & \dots & R_X(n_1,n_k) \\ R_X(n_2,n_1) & R_X(n_2,n_2) & \dots & R_X(n_2,n_k) \\ \vdots & \vdots & \ddots & \vdots \\ R_X(n_k,n_1) & R_X(n_k,n_2) & \dots & R_X(n_k,n_k)}
   $$
   Since $\Sigma$ is symmetric, non-negative definite, we have $\Sigma \succeq 0$. Therefore,
   $$
   \sum_{i=1}^k \sum_{j=1}^k a_i a_j R_X(n_i, n_j) = a^T \Sigma a \ge 0.
   $$
   :::

4. **Cauchy-Schwarz inequality:** $|R_X(m,n)|^2 \le R_X(m,m) R_X(n,n)$.

   :::{.callout-note collapse="false"}
   #### Proof
   This is a direct consequence of the [Cauchy-Schwarz inequality](inequalities.qmd@cauchy-schwarz-inequality) for random variables:
   $$
   |\COV(X_m, X_n)|^2 \leq \COV(X_m, X_m) \COV(X_n, X_n).
   $$
   :::

5. The variance function is $R_X(n,n) = \VAR(X_n) \ge 0$.

## Examples of Gaussian processes

```{ojs}
// Standard Normal variate using Box-Muller transform.
function gaussianRandom(mean=0, stdev=1) {
    const u = 1 - Math.random();
    const v = Math.random();
    const z = Math.sqrt( -2.0 * Math.log( u ) ) * Math.cos( 2.0 * Math.PI * v );
    return z * stdev + mean;
}
```

:::{#exm-cosine-gaussian-phase}
#### Cosine with Gaussian amplitude

Consider a process $\{X_n\}_{n \ge 0}$ defined as:
$$
X_n = A \cos(\Omega n) + B \sin(\Omega n)
$$
where $\Omega \in \mathbb{R}$ is a constant frequency, and $A$ and $B$ are independent Gaussian random variables with $A, B \sim \mathcal{N}(0, σ^2)$.

A sample path of this process is shown below.

```{ojs}
viewof rerun_cosine_gauss = Inputs.button("Generate sample path")

points_cosine_gauss = {
    rerun_cosine_gauss
    const N = 120
    const sigma = 1.0
    const f = 0.1
    const A = gaussianRandom(0, sigma)
    const B = gaussianRandom(0, sigma)
    var points = new Array(N+1)
    for(var n=0; n <= N; n++) {
      var val = A * Math.cos(2 * Math.PI * f * n) + B * Math.sin(2 * Math.PI * f * n)
      points[n] = { n: n, X_n: val }
    }
    return points
}
```

```{ojs}
Plot.plot({
  grid: true,
  height: 150,
  y: {domain: [-2, 2]},
  marks: [
    Plot.ruleX([0]),
    Plot.ruleY([0]),
    Plot.line(points_cosine_gauss, {x: "n", y: "X_n", strokeWidth: 1.5}),
    Plot.dot(points_cosine_gauss, {x: "n", y: "X_n", fill: "currentColor", r: 2}),
  ]
})
```

Find the mean and covariance function of the process. Is the process stationary?
:::

:::{.callout-note collapse="true"}
#### Solution

**Mean function:**

\begin{align*}
\mu_X(n) &= \EXP[X_n] = \EXP[A \cos(\Omega n) + B \sin(\Omega n)] \\
&= \EXP[A] \cos(\Omega n) + \EXP[B] \sin(\Omega n) \\
&= 0
\end{align*}
since $\EXP[A] = \EXP[B] = 0$.

**Covariance function:**

\begin{align*}
R_X(m,n) &= \EXP[X_m X_n] \\
&= \EXP[(A \cos(\Omega m) + B \sin(\Omega m))(A \cos(\Omega n) + B \sin(\Omega n))] \\
&= \EXP[A^2] \cos(\Omega m) \cos(\Omega n) + \EXP[AB] \cos(\Omega m) \sin(\Omega n) \\
&\quad + \EXP[BA] \sin(\Omega m) \cos(\Omega n) + \EXP[B^2] \sin(\Omega m) \sin(\Omega n)
\end{align*}

Since $A$ and $B$ are independent with $\EXP[A] = \EXP[B] = 0$, we have:
- $\EXP[A^2] = \VAR(A) = σ^2$
- $\EXP[B^2] = \VAR(B) = σ^2$
- $\EXP[AB] = \EXP[A]\EXP[B] = 0$

Therefore:
\begin{align*}
R_X(m,n) &= σ^2 [\cos(\Omega m) \cos(\Omega n) + \sin(\Omega m) \sin(\Omega n)] \\
&= σ^2 \cos(\Omega (m-n))
\end{align*}
using the identity $\cos(a-b) = \cos(a)\cos(b) + \sin(a)\sin(b)$.

Since cosine is even:
$$
R_X(m,n) = σ^2 \cos(\Omega |m-n|)
$$

Thus, the process is sationary.
:::



:::{#exm-white-noise}
#### White noise process (i.i.d. Gaussian)

A **white noise process** $\{W_n\}_{n \ge 0}$ is a Gaussian process where $\{W_n\}_{n \ge 0}$ is an i.i.d. sequence with $W_n \sim \mathcal{N}(0, σ^2)$ for some $σ > 0$.

A sample path of white noise is shown below.

```{ojs}
viewof rerun_white = Inputs.button("Generate sample path")

points_white = {
    rerun_white
    const N = 120
    const sigma = 1.0
    var points = new Array(N)
    for(var n=0; n < N; n++) {
      var val = gaussianRandom(0, sigma)
      points[n] = { n: n, W_n: val }
    }
    return points
}
```

```{ojs}
Plot.plot({
  grid: true,
  height: 150,
  marks: [
    Plot.ruleX([0]),
    Plot.ruleY([0]),
    Plot.line(points_white, {x: "n", y: "W_n", strokeWidth: 1}),
    Plot.dot(points_white, {x: "n", y: "W_n", fill: "currentColor", r: 2}),
  ]
})
```

Find the mean and covariance function of the process. Is the process stationary?

:::{.callout-note collapse="false"}
#### Solution

**Mean function:**

Since $W_n \sim \mathcal{N}(0, σ^2)$ for all $n \ge 0$, we have:
$$
\mu_W(n) = \EXP[W_n] = 0 \quad \text{for all } n \ge 0
$$

**Covariance function:**

For $m, n \ge 0$:

- If $m = n$: $R_W(m,n) = \COV(W_m, W_n) = \VAR(W_n) = σ^2$
- If $m \neq n$: Since $W_m$ and $W_n$ are independent, $R_W(m,n) = \COV(W_m, W_n) = 0$

Therefore:
$$
R_W(m,n) = σ^2 \IND_{\{m=n\}} \quad \text{for } m, n \ge 0
$$
where $\IND_{\{m=n\}}$ is the indicator function that equals 1 when $m = n$ and 0 otherwise.
:::
:::

:::{#exm-random-walk}
#### Gaussian random walk

A **Gaussian random walk** $\{X_n\}_{n \ge 0}$ is a Gaussian process defined by:
$$
X_0 = 0, \quad X_{n+1} = X_n + W_n, \quad n \ge 0
$$
where $\{W_n\}_{n \ge 0}$ is an i.i.d. sequence with $W_n \sim \mathcal{N}(0, σ^2)$ for some $σ > 0$.

A sample path of a Gaussian random walk is shown below.

```{ojs}
viewof rerun_rw = Inputs.button("Generate sample path")

points_rw = {
    rerun_rw
    const N = 120
    const sigma = 1.0
    var points = new Array(N+1)
    var X = 0
    points[0] = { n: 0, X_n: 0 }
    for(var n=1; n <= N; n++) {
      var W = gaussianRandom(0, sigma)
      X = X + W
      points[n] = { n: n, X_n: X }
    }
    return points
}
```

```{ojs}
Plot.plot({
  grid: true,
  height: 150,
  y: {domain: [-20, 20]},
  marks: [
    Plot.ruleX([0]),
    Plot.ruleY([0]),
    Plot.line(points_rw, {x: "n", y: "X_n", strokeWidth: 1.5}),
    Plot.dot(points_rw, {x: "n", y: "X_n", fill: "currentColor", r: 2}),
  ]
})
```

Assuming $X_n \sim \mathcal{N}(0, σ^2)$, find the mean and covariance function of the process. Is the process sationary?
:::

:::{.callout-note collapse="false"}
#### Solution

Observe that the process satisfies the Markov property. So, it is also called a **Gauss Markov process**. Since the marginal distribution of $X_n$ is Gaussian, we can write recursive formulas for the mean and covariance function (which will be the analog of the $\mu^{(n+1)} = \mu^{(n)} P$ type recursions for Markov chains). 
\begin{align*}
  \mu_X(n+1) &= \mu_X(n) + \EXP[W_{n+1}] = \mu_X(n), \\
  R_X(n+1,n+1) &= \EXP[ (X_n + W_n)(X_n + W_n)] \\
  &= R_X(n,n) + \EXP[W_n^2] = R_X(n,n) + σ^2.
\end{align*}

Thus, $\mu_X(n) = \mu_X(0) = 0$ and $R_X(n,n) = σ^2 n$ for all $n \ge 0$. 

For computing $R_X(n,m)$ for $n \neq m$, we use the fact that (assuming $n < m$) the increment $X_{m} - X_{n} = W_{n} + W_{n+1} + \cdots + W_{m-1}$ is independent of $X_n$. Therefore
\begin{align*}
  R_X(n,m) &= \EXP[X_n X_m] \\ 
  &= \EXP[X_n (X_{n} + W_{n} + W_{n+1} + \cdots + W_{m-1})] \\
  &= R_X(n,n).
\end{align*}

Combining the above we have $R_X(n,m) = σ^2 \min(n,m)$ for all $n, m \ge 0$. Thus, the process is not sationary.
:::

:::{#exm-AR1-process}
#### AR(1) process 

An **AR(1) process** (also called discrete-time **Ornstein-Uhlenbeck** process) $\{X_n\}_{n \ge 0}$ is a Gaussian process defined by the recursion:
$$
X_{n+1} = a X_n + W_n, \quad n \ge 0
$$
where $|a| < 1$ and $\{W_n\}_{n \ge 0}$ is an i.i.d. sequence with $W_n \sim \mathcal{N}(0, σ^2)$. 

A sample path of an AR(1) process is shown below.

```{ojs}
viewof rerun_ar1 = Inputs.button("Generate sample path")

points_ar1 = {
    rerun_ar1
    const N = 120
    const sigma = 1.0
    const a = 0.8
    const var0 = sigma*sigma / (1 - a*a)
    var points = new Array(N+1)
    var X = gaussianRandom(0, Math.sqrt(var0))
    points[0] = { n: 0, X_n: X }
    for(var n=1; n <= N; n++) {
      var W = gaussianRandom(0, sigma)
      X = a * X + W
      points[n] = { n: n, X_n: X }
    }
    return points
}
```

```{ojs}
Plot.plot({
  grid: true,
  height: 150,
  y: {domain: [-5, 5]},
  marks: [
    Plot.ruleX([0]),
    Plot.ruleY([0]),
    Plot.line(points_ar1, {x: "n", y: "X_n", strokeWidth: 1.5}),
    Plot.dot(points_ar1, {x: "n", y: "X_n", fill: "currentColor", r: 2}),
  ]
})
```

Assuming $X_0 \sim \mathcal{N}(0, σ^2)$, find the mean and covariance function of the process. Is the process sationary?
:::

:::{.callout-note collapse="false"}
#### Solution

As was the case for @exm-random-walk, AR(1) processes are also Gauss Markov processes. As before, we can write recursive formulas for the mean and covariance function: 
\begin{align*}
  \mu_X(n+1) &= a \mu_X(n) + \EXP[W_{n+1}] = a \mu_X(n), \\
  R_X(n+1,n+1) &= \EXP[ (a X_n + W_n)(a X_n + W_n)] \\
  &= a^2 R_X(n,n) + \EXP[W_n^2] = a^2 R_X(n,n) + σ^2.
\end{align*}

Thus, $\mu_X(n) = \mu_X(0) = 0$ and $R_X(n,n) = σ^2 \sum_{k=0}^{n-1} a^{2k}$ for all $n \ge 0$. 

For computing $R_X(n,m)$ for $n \neq m$, we use the fact that (assuming $n < m$) 
\begin{align*}
X_{m} &= a X_{m-1} + W_{m-1} \\
&= a (a X_{m-2} + W_{m-2}) + W_{m-1} \\
&= a^2 X_{m-2} + a W_{m-2} + W_{m-1} \\
&= \cdots \\
&= a^{m-n} X_{n} + a^{m-n-1} W_{n+1} + \cdots + W_{m-1}.
\end{align*}
Note that the increment $a^{m-n-1} W_{n+1} + \cdots + W_{m-1}$ is independent of $X_n$. Therefore
\begin{align*}
  R_X(n,m) &= \EXP[X_n X_m] \\
  &= \EXP[X_n (a^{m-n} X_{n} + a^{m-n-1} W_{n+1} + \cdots + W_{m-1})] \\
  &= a^{m-n} R_X(n,n).
\end{align*}  

Combining the above we have 
$$R_X(n,m) = σ^2 a^{|m-n|} \frac{1 - a^{2\min(n,m)}}{1 - a^2}, \quad \forall n, m \ge 0.$$
where we have used the geometric series formula $\sum_{k=0}^{n-1} a^{2k} = \frac{1 - a^{2n}}{1 - a^2}$ (for $|a| < 1$).

Thus, the process is not stationary.
:::

:::{.callout-tip}
#### Invariant distribution and stationarity

From the results of @exm-AR1-process, observe that 
$$
   \lim_{n \to \infty} \mu_X(n) = 0 \quad \text{and} \quad \lim_{n \to \infty} R_X(n,n) = \frac{σ^2}{1-a^2}
$$
for $m, n \ge 0$.  The above limits show that the distribution of $X_n$ converges to an invariant distribution with mean 0 and variance $\frac{σ^2}{1-a^2}$. 

As was the case for Markov chains, if we start with the invariant distribution, i.e., $X_0 \sim \mathcal{N}(0, \frac{σ^2}{1-a^2})$, the process will remain in the invariant distribution for all time because
$$
  \mu_X(n+1) = a \mu_X(n) = 0
$$
and
$$
R_X(1,1) = σ^2 + a^2 R_X(0,0) = σ^2 + a^2 \frac{σ^2}{1-a^2} = σ^2 \frac{1 + a^2}{1-a^2},
$$
and simlarly for $R_X(n,n)$ for all $n \ge 0$.

Furthermore, the process is a strict sense stationary process because in addition to $\mu_X(n) = 0$, we have
$$
R_X(n, m) = a^{|n-m|} R_X(n,n) = a^{|n-m|} \frac{σ^2}{1-a^2}.
$$
which is a function of $|n-m|$ only.
:::

The above ideas can be generalized to vector valued processes as well. 

1. In general, let $\{W_n\}_{n \ge 0}$ be a vector valued white noise process, where $W_n \sim \mathcal{N}(0, Σ_W)$. Then $μ_X(n) = 0$ and $R_X(n,m) = Σ_W \IND\{n = m\}$. The process is called a **standard White noise** process if $Σ_W = I$. 

2. Consider a stochastic process $\{X_n\}_{n \ge 0}$ defined as
   $$
     X_{n+1} = A_n X_n + B_n W_n.
   $$
   where $A_n$ and $B_n$ are deterministic matrices and $\{W_n\}_{n \ge 0}$ is standard White noise. Then, $\{X_n\}_{n \ge 0}$ is **Gauss Markov process** and it can be shown that for any $\ell < n < m$, we have
   $$
     R_X(\ell, m) = R_X(\ell,n) R_X(n,n)^{-1} R_X(n,m).
   $$

3. The process is stationary when $A_n$ and $B_n$ do not depend on $n$. 

<!--
:::{#exm-moving-average}
#### Moving average process

A **moving average process** $\{X_n\}_{n \ge 0}$ is defined as:
$$
X_n = \sum_{k=0}^{\min(n,q)} b_k W_{n-k}
$$
where $\{W_n\}_{n \ge 0}$ is white noise with $W_n \sim \mathcal{N}(0, σ^2)$ and $b_0, \dots, b_q$ are constants.

A sample path of a moving average process (with $q=2$ and $b_0=1, b_1=0.5, b_2=0.25$) is shown below.

```{ojs}
viewof rerun_ma = Inputs.button("Generate sample path")

points_ma = {
    rerun_ma
    const N = 120
    const sigma = 1.0
    const q = 2
    const b = [1, 0.5, 0.25]
    var W_history = []
    var points = new Array(N+1)
    for(var n=0; n <= N; n++) {
      var W = gaussianRandom(0, sigma)
      W_history.unshift(W)
      if(W_history.length > q+1) {
        W_history.pop()
      }
      var X = 0
      for(var k=0; k <= Math.min(n, q); k++) {
        if(k < W_history.length) {
          X += b[k] * W_history[k]
        }
      }
      points[n] = { n: n, X_n: X }
    }
    return points
}
```

```{ojs}
Plot.plot({
  grid: true,
  height: 150,
  marks: [
    Plot.ruleX([0]),
    Plot.ruleY([0]),
    Plot.line(points_ma, {x: "n", y: "X_n", strokeWidth: 1.5}),
    Plot.dot(points_ma, {x: "n", y: "X_n", fill: "currentColor", r: 2}),
  ]
})
```

Find the mean and covariance function of the process.

:::{.callout-note collapse="true"}
#### Solution

**Mean function:**

Since $W_n \sim \mathcal{N}(0, σ^2)$ for all $n \ge 0$, we have:
$$
\mu_X(n) = \EXP[X_n] = \EXP\left[\sum_{k=0}^{\min(n,q)} b_k W_{n-k}\right] = \sum_{k=0}^{\min(n,q)} b_k \EXP[W_{n-k}] = 0 \quad \text{for all } n \ge 0
$$

**Covariance function:**

For $m, n \ge 0$:
\begin{align*}
R_X(m,n) &= \COV(X_m, X_n) = \EXP[X_m X_n] \\
&= \EXP\left[\left(\sum_{i=0}^{\min(m,q)} b_i W_{m-i}\right)\left(\sum_{j=0}^{\min(n,q)} b_j W_{n-j}\right)\right] \\
&= \sum_{i=0}^{\min(m,q)} \sum_{j=0}^{\min(n,q)} b_i b_j \EXP[W_{m-i} W_{n-j}]
\end{align*}

Since $W_k$ are independent with $\EXP[W_k] = 0$ and $\VAR(W_k) = σ^2$, we have $\EXP[W_{m-i} W_{n-j}] = σ^2$ if $m-i = n-j$ (i.e., $n-m = j-i$), and $0$ otherwise.

Let $d = n-m$. Then:
$$
R_X(m,n) = σ^2 \sum_{k=0}^{q} b_k b_{k+|d|}
$$
where we define $b_k = 0$ for $k < 0$ or $k > q$.

For the case when $n \ge m$ (so $d \ge 0$), we need $j = i + d$:
$$
R_X(m,n) = σ^2 \sum_{i=0}^{q} b_i b_{i+d}
$$

Since the covariance function is symmetric, for general $m, n \ge 0$:
$$
R_X(m,n) = σ^2 \sum_{k=0}^{q} b_k b_{k+|n-m|}
$$
:::
:::
-->

## Jointly Gaussian stochastic processes

1. Consider two stochastic processes defined on a common probability space: $\{X_n\}_{n \ge 0}$ and $\{Y_n\}_{n \ge 0}$. Suppose for any collection $\{n_1, \dots, n_k\}$ of time indices, the random vector $\{X_{n_1}, \dots, X_{n_k}, Y_{n_1}, \dots, Y_{n_k}\}$ has a multivariate Gaussian distribution. Then, the processes are called **jointly Gaussian**.

2. The joint process $\{X_n, Y_n\}_{n \ge 0}$ is a Gaussian process with mean $\mu_{X,Y}(n) = \MATRIX{\mu_X(n)\\ \mu_Y(n)}$ and covariance $R_{X,Y}(n,m) = \MATRIX{R_X(n,m) & R_{XY}(n,m) \\ R_{YX}(n,m) & R_Y(n,m)}$.

3. The function $R_{XY}(n,m) = COV(X_n, Y_m)$ is called the **cross-covariance function** of the processes (and sometimes the processes $R_X(n,m)$ and $R_Y(n,m)$ are called the **auto-covariance functions** of the processes). 

4. The cross-covariance function satisfies $R_{XY}(n,m) = \COV(X_n, Y_m) = R_{YX}(m,n)$ for all $n, m \ge 0$. 

5. The processes are **uncorrelated** if $R_{XY}(n,m) = 0$ for all $n, m \ge 0$. Since the joint process is Gaussian, uncorrelated implies independent.

## Review of discrete-time LTI systems

1. Consider an LTI system with impulse resposnse $\{h_k\}_{k \ge 0}$. The transfer function of the system is given by the Z-transform of the impulse response:
   $$
   H(z) = \sum_{k=0}^∞ h_k z^{-k}
   $$

2. The system is stable if 
   $$ \sum_{k=0}^∞ |h_k| < ∞, $$
   Equivalently, the ROC of $H(z)$ contains the unit circle $|z| = 1$, i.e., all poles of $H(z)$ lie inside the unit circle. 

3. The Z-transform evaluated on the unit circle is called the **Fourier transform**, i.e.,
   $$
      H(e^{j\omega}) = \sum_{k=-\infty}^∞ h_k e^{-j\omega k}
   $$
   Note that we have a clash of notation. In Signals and Systems, $\omega$ is used to denote the frequency in radians per second, while in probabiblity $\omega$ is used to denote events in the sample space. To avoid confusion, we we will work use $\Omega$ to denote the frequency (it still clashes with the notation for sample space, but the meaning should be clear from the context). Thus, the Fourier transform is given by
   $$
      H(e^{j\Omega}) = \sum_{k=-\infty}^∞ h_k e^{-j\Omega k}
   $$

4. The inverse Fourier transform is given by:
   $$
      h_k = \frac{1}{2\pi} \int_{-\pi}^{\pi} H(e^{j\Omega}) e^{j\Omega k} d\Omega
   $$


## Stationary Gaussian processes through LTI systems

1. When a stationary Gaussian process is passed through a linear time-invariant (LTI) system, the output is also a stationary Gaussian process. This is a fundamental result in signal processing and stochastic control.

5. Suppose a stationary Gaussian process $\{X_n\}_{n ≥ 0}$ is passed through an LTI system $H(z)$. For every $ω ∈ Ω$, the output process is given by
  $$
    Y_n(ω) = \sum_{k=0}^∞ h_k X_{n-k}(ω)
  $$
  which is a sum of Gaussian random variables, and hence is Gaussian.

6. The output $\{Y_n\}_{n ≥ 0}$ is a stationary Gaussian process. 

   :::{.callout-note}
   #### Proof

   The mean function of the output process is given by
   $$
     μ_Y(n) = \EXP[Y_n] = \EXP\left[\sum_{k=0}^∞ h_k X_{n-k}\right] = \sum_{k=0}^∞ h_k \EXP[X_{n-k}] = \sum_{k=0}^∞ h_k μ_X = μ_X \sum_{k=0}^∞ h_k = μ_X H(1)
   $$
   which does not depend on $n$. 

   The covariance function of the output process is given by
   $$
     \begin{align*}
       R_Y(n,m) 
         &= \EXP[Y_n Y_m] \\
         &= \EXP\left[\sum_{k=0}^\infty h_k X_{n-k} \sum_{j=0}^\infty h_j X_{m-j}\right] \\
         &= \sum_{k=0}^\infty \sum_{j=0}^\infty h_k h_j \EXP[X_{n-k} X_{m-j}] \\
         &= \sum_{k=0}^\infty \sum_{j=0}^\infty h_k h_j R_X(|(n-k)-(m-j)|) \\
         &= \sum_{k=0}^\infty \sum_{j=0}^\infty h_k h_j R_X(|(n-m)-(k-j)|)
     \end{align*}
   $$
   which only depends on $n-m$.

   Therefore, the output process is a stationary Gaussian process.
   :::

7. For a stationary Gaussian process $\{X_n\}_{n \ge 0}$ with covariance function $R_X(k) = R_X(|n-m|)$ where $k = |n-m|$, the **power spectral density** (PSD) $S_X(\omega)$ is defined as the discrete-time Fourier transform (DTFT) of the covariance function:
   $$
   S_X(\omega) = \sum_{n=-\infty}^{\infty} R_X(n) e^{-j\omega n}, \quad \omega \in [-\pi, \pi]
   $$
   where $j = \sqrt{-1}$ and $R_X(-n) = R_X(n)$ by symmetry of the covariance function. For this infinite sum to converge, we require that $\sum_{n=-\infty}^{\infty} |R_X(n)| < \infty$ (absolute summability of the covariance function).

8. The covariance function can be recovered from the power spectral density via the inverse DTFT:
   $$
   R_X(n) = \frac{1}{2\pi} \int_{-\pi}^{\pi} S_X(\omega) e^{j\omega n} \, d\omega, \quad n \in \mathbb{Z}
   $$
   This establishes a Fourier transform pair between the covariance function in the time domain and the power spectral density in the frequency domain.

9. **Properties of power spectral density:**
   - **Non-negativity:** $S_X(\omega) \ge 0$ for all $\omega \in [-\pi, \pi]$
   - **Symmetry:** $S_X(\omega) = S_X(-\omega)$ (for real-valued processes)
   - **Total power:** The variance of the process is given by
     $$
     \VAR(X_n) = R_X(0) = \frac{1}{2\pi} \int_{-\pi}^{\pi} S_X(\omega) \, d\omega
     $$

10. **Linear filtering:** If $\{Y_n\}_{n \ge 0}$ is the output of a stable LTI system with frequency response $H(\omega)$ and input $\{X_n\}_{n \ge 0}$, then
    $$
    S_Y(\omega) = |H(\omega)|^2 S_X(\omega)
    $$
    where $H(\omega) = \sum_{n=-\infty}^{\infty} h_n e^{-j\omega n}$ is the DTFT of the impulse response $\{h_n\}_{n=-\infty}^{\infty}$.

    :::{.callout-note collapse="true"}
    #### Proof

    The output process is given by the convolution:
    $$
    Y_n = \sum_{k=-\infty}^{\infty} h_k X_{n-k}
    $$

    The covariance function of the output is:
    $$
    \begin{align*}
    R_Y(n) &= \COV(Y_0, Y_n) = \EXP[Y_0 Y_n] - \EXP[Y_0]\EXP[Y_n] \\
    &= \EXP\left[\sum_{k=-\infty}^{\infty} h_k X_{-k} \sum_{j=-\infty}^{\infty} h_j X_{n-j}\right] - \mu_Y^2 \\
    &= \sum_{k=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} h_k h_j \EXP[X_{-k} X_{n-j}] - \mu_Y^2
    \end{align*}
    $$

    Since $\{X_n\}$ is stationary with mean $\mu_X$ and covariance $R_X(m)$, we have:
    $$
    \EXP[X_{-k} X_{n-j}] = R_X((n-j) - (-k)) + \mu_X^2 = R_X(n - j + k) + \mu_X^2
    $$

    Therefore:
    $$
    \begin{align*}
    R_Y(n) &= \sum_{k=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} h_k h_j (R_X(n - j + k) + \mu_X^2) - \mu_Y^2 \\
    &= \sum_{k=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} h_k h_j R_X(n - j + k) + \mu_X^2 \sum_{k=-\infty}^{\infty} h_k \sum_{j=-\infty}^{\infty} h_j - \mu_Y^2
    \end{align*}
    $$

    Since $\mu_Y = \mu_X H(1)$ and $H(1) = \sum_{k=-\infty}^{\infty} h_k$, the constant terms cancel, leaving:
    $$
    R_Y(n) = \sum_{k=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} h_k h_j R_X(n - j + k)
    $$

    Making the substitution $\ell = j - k$, we get:
    $$
    R_Y(n) = \sum_{k=-\infty}^{\infty} \sum_{\ell=-\infty}^{\infty} h_k h_{k+\ell} R_X(n - \ell) = \sum_{\ell=-\infty}^{\infty} R_X(n - \ell) \sum_{k=-\infty}^{\infty} h_k h_{k+\ell}
    $$

    The inner sum $\sum_{k=-\infty}^{\infty} h_k h_{k+\ell}$ is the autocorrelation of the impulse response, which we denote as $r_h(\ell)$. This is the convolution of $h_k$ with $h_{-k}$:
    $$
    r_h(\ell) = \sum_{k=-\infty}^{\infty} h_k h_{k+\ell} = (h * \tilde{h})(\ell)
    $$
    where $\tilde{h}_k = h_{-k}$ is the time-reversed impulse response.

    Therefore:
    $$
    R_Y(n) = \sum_{\ell=-\infty}^{\infty} r_h(\ell) R_X(n - \ell) = (r_h * R_X)(n)
    $$

    Taking the DTFT of both sides and using the convolution property of the DTFT:
    $$
    S_Y(\omega) = \mathcal{F}\{r_h\}(\omega) \cdot S_X(\omega)
    $$

    where $\mathcal{F}\{r_h\}(\omega)$ is the DTFT of $r_h(\ell)$. The DTFT of the autocorrelation $r_h(\ell)$ is:
    $$
    \mathcal{F}\{r_h\}(\omega) = \sum_{\ell=-\infty}^{\infty} r_h(\ell) e^{-j\omega \ell} = \sum_{\ell=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} h_k h_{k+\ell} e^{-j\omega \ell}
    $$

    Making the substitution $m = k + \ell$:
    $$
    \begin{align*}
    \mathcal{F}\{r_h\}(\omega) &= \sum_{k=-\infty}^{\infty} \sum_{m=-\infty}^{\infty} h_k h_m e^{-j\omega (m-k)} \\
    &= \sum_{k=-\infty}^{\infty} h_k e^{j\omega k} \sum_{m=-\infty}^{\infty} h_m e^{-j\omega m} \\
    &= H^*(\omega) H(\omega) = |H(\omega)|^2
    \end{align*}
    $$

    where $H^*(\omega)$ is the complex conjugate of $H(\omega)$.

    Therefore:
    $$
    S_Y(\omega) = |H(\omega)|^2 S_X(\omega)
    $$

    This shows that the power spectral density of the output is the power spectral density of the input multiplied by the squared magnitude of the frequency response.
    :::

11. **Interpretation:** The power spectral density describes how the power (variance) of a stationary process is distributed across different frequencies. It is a fundamental tool in signal processing for analyzing the frequency content of random signals.

## Note on non-Gaussian processes

Since all the properties discussed above (mean function, covariance function, linear transformations, etc.) depend only on the first two moments (mean and covariance), they apply to stochastic processes beyond Gaussian processes, as long as we restrict our attention to first and second moments.

However, it is important to distinguish between two notions of stationarity:

1. A stochastic process $\{X_n\}_{n \ge 0}$ is called **wide-sense stationary** (or **weakly stationary**) if:

   - The mean function is constant: $μ_X(n) = μ_X$ for all $n \ge 0$
   - The covariance function depends only on the time difference: $R_X(m,n) = R_X(|n-m|)$ for some function $R_X$

2. A stochastic process $\{X_n\}_{n \ge 0}$ is called **strict-sense stationary** (or **strongly stationary**) if for any finite collection of time indices $\{n_1, \dots, n_k\}$ and any integer $h \ge 0$, the joint distribution of $(X_{n_1}, \dots, X_{n_k})$ is the same as the joint distribution of $(X_{n_1+h}, \dots, X_{n_k+h})$.

For any stochastic process, strict-sense stationarity implies wide-sense stationarity. However, the converse is not true in general. For **Gaussian processes**, since they are completely determined by their mean and covariance, wide-sense stationarity implies strict-sense stationarity. Thus, for Gaussian processes, the two notions are equivalent.

## Exercises{-}

:::{#exr-verify-covariance}
#### Verifying covariance functions

Show that the following functions are valid covariance functions for a discrete-time Gaussian process:

a. $R_X(m,n) = σ^2 \min(m,n)$ for $m, n \ge 0$
b. $R_X(m,n) = ρ^{|n-m|}$ for $m, n \ge 0$ and $|ρ| < 1$
c. $R_X(m,n) = \frac{σ^2}{1-a^2} a^{|n-m|}$ for $m, n \ge 0$ and $|a| < 1$
:::

:::{#exr-random-walk-properties}
#### Properties of Gaussian random walk

Let $\{W_n\}_{n \ge 0}$ be a Gaussian random walk with $R_W(m,n) = σ^2 \min(m,n)$. Show that:

a. For $0 \le m < n$, $W_n - W_m \sim \mathcal{N}(0, σ^2(n-m))$
b. For $0 \le n_1 < n_2 < n_3 < n_4$, the increments $W_{n_2} - W_{n_1}$ and $W_{n_4} - W_{n_3}$ are independent
c. $\EXP[W_m W_n] = σ^2 \min(m,n)$
:::

:::{#exr-conditional-random-walk}
#### Conditional distribution of Gaussian random walk

For a Gaussian random walk $\{W_n\}_{n \ge 0}$ with $R_W(m,n) = σ^2 \min(m,n)$, compute the conditional distribution of $W_n$ given $W_5 = 1$ for:
a. $n = 3$
b. $n = 7$

Interpret the results.
:::

:::{#exr-GP-regression}
#### Gaussian process regression

Consider a Gaussian process $\{X_n\}_{n \ge 0}$ with mean function $μ_X(n) = 0$ and covariance function $R_X(m,n) = ρ^{|n-m|}$ where $ρ = 0.8$.

Given observations $X_2 = 1$ and $X_8 = -0.5$:

a. Compute the conditional mean and variance of $X_5$.
b. Compute the conditional mean and variance of $X_9$.
c. Which prediction has higher uncertainty? Why?
:::

:::{#exr-sum-GP}
#### Sum of Gaussian processes

Let $\{X_n\}_{n \ge 0}$ and $\{Y_n\}_{n \ge 0}$ be independent Gaussian processes with mean functions $μ_X(n)$, $μ_Y(n)$ and covariance functions $R_X(m,n)$, $R_Y(m,n)$ respectively.

Show that $\{X_n + Y_n\}_{n \ge 0}$ is a Gaussian process and find its mean and covariance functions.
:::

:::{#exr-stationary-GP}
#### Stationary Gaussian process

A Gaussian process $\{X_n\}_{n \ge 0}$ is wide-sense stationary with covariance function $R_X(k) = ρ^{k}$ for $k \ge 0$ and $|ρ| < 1$.

a. Find the mean function $μ_X(n)$.
b. Show that $R_X(k)$ is a valid covariance function.
c. Compute $\VAR(X_n)$.
d. For what values of $k$ is the correlation between $X_n$ and $X_{n+k}$ less than 0.1?
:::

## Further Reading {-}

1. Gubner, Chapter 15.

2. Grimmett and Stirzaker, Chapter 13.

3. [Rasmussen and Williams, _Gaussian Processes for Machine Learning_](https://gaussianprocess.org/gpml/), MIT Press, 2006. Available online.

4. [Adler and Taylor, _Random Fields and Geometry_](https://www.springer.com/gp/book/9780387481168), Springer, 2007.
