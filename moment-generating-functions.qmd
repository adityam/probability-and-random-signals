---
title: Moment Generating Functions
---

## Moment Generating Functions

The **moment generating function (MGF)** of a random variable $X$ is defined as
$$ 
M_X(s) = \EXP[e^{sX}]
$$
provided the expectation exists.

- When $X$ is discrete, we have
  $$ M_X(s) = \sum_{x \in \text{range}(X)} e^{sx} p_X(x). $$

- When $X$ is continuous, we have
  $$ M_X(s) = \int_{-∞}^∞ e^{sx} f_X(x) \, dx. $$

:::{.callout-important}
### Relationship to Laplace transforms

Although most texts (including the textbook) restrict $s$ to be real, my personal view is that one should really interpret $s$ as a complex number. If we do so, then we have the following:

- $M_X(-s)$ is the Laplace transform of the PDF. 
- $M_X(-j ω)$ is the Fourier transform of the PDF, which is called the **characteristic function** of $X$.

Therefore, we can recover the PDF by taking the inverse Laplace transform of MGF. Thus, specifying the MGF of a random variable is equivalent to specifying the PDF.

Historically, MGF is defined for $s \in \reals$ and there are distributions (e.g., Cauchy) for which MGF does not exist for any $s \neq 0$. In avoid such situations, one uses the characteristic function because the characteristic function always exists. However, if we view the domain of MGF to be $\mathbb{C}$, then there is no need for a distinction between MGF and characteristic function. 
:::

:::{#exm-MGF-discrete}
Suppose $X$ is a random variable which takes values $\{0, 1, 2\}$ with probabilities $\{\frac 12, \frac 13, \frac 16\}$. Then,
\begin{align*}
M_X(s) &= \EXP[e^{sX}] \\
&= \frac 12 e^{s 0} + \frac 13 e^{s 1} + \frac 16 e^{s 2} \\
&= \frac 12 + \frac 13 e^{s} + \frac 16 e^{2s}.
\end{align*}
:::

:::{#exm-MGF-Poisson}
Find the MGF of a Poisson random variable with parameter $λ$. 
:::

:::{.callout-note collapse="true"}
#### Solution
\begin{align*}
M_X(s) &= \EXP[e^{sX}] = \sum_{k=0}^{∞}e^{ks} \frac{λ^k e^{-λ}}{k!} \\
&= e^{-λ} \sum_{k=0}^{∞} \frac{(λe^s)^k}{k!} \\
&= e^{-λ} e^{λ e^{s}}.
\end{align*}
:::

:::{#exm-MGF-exponential}
Find the MGF of an exponential random variable with parameter $λ$. 
:::

:::{.callout-note collapse="true"}
#### Solution
\begin{align*}
M_X(s) &= \EXP[e^{sX}] \\
&= \int_{0}^∞ e^{sx} λ e^{-λx} \, dx \\
&= λ \int_{0}^∞ e^{(s-λ)x} \, dx \\
&= \frac{λ}{λ-s}.
\end{align*}

Note that we could have looked up this result from the Laplace transform tables which show that
$$
e^{at} \xleftrightarrow{\hskip 0.5em \mathcal{L}\hskip 0.5em } \frac{1}{s-a}
$$
:::

The MGF of common random variables is shown in @tbl-MGF.

| **Random variable** | **Parameter(s)** | **MGF** |
|:---------------:|:------------:|:----:|
| Bernoulli       | $p$          | $1 - p + p e^s$ | 
| Binomial        | $(n,p)$      | $(1-p + p e^s)^n$ | 
| Geometric       | $p$          | $\dfrac{p e^s}{1 - (1-p)e^s}$ |
| Poisson         | $λ$          | $\exp(λ e^s - 1)$ |
| Uniform         | $(a,b)$      | $\dfrac{e^{sb} - e^{sa}}{s(b-a)}$ |
| Exponential     | $λ$          | $\dfrac{λ}{λ-s}$ |
| Gaussian        | $(μ,σ)$      | $\exp\bigl(μ s + \frac 12 σ^2 s^2 \bigr)$ |


: MGF of common random vables {#tbl-MGF}


If there exist a neighborhood around origin where $M_X(s)$ is well-defined. Then, we can use the MGF to "generate the moments" of $X$ as follows:

1. $M_X(0) = 1$

2. $\dfrac{d}{ds} M_X(s) \biggr|_{s=0} = \EXP[X]$.

3. $\dfrac{d^2}{ds^2} M_X(s) \biggr|_{s=0} = \EXP[X^2]$.

4. and in general 
   $\dfrac{d^k}{ds^k} M_X(s) \biggr|_{s=0} = \EXP[X^k]$.

5. Hence, by Taylor series expansion of $M_X(s)$ within the radius of convergence, we get
   $$
    M_X(s) = \sum_{k=0}^∞ \frac{\EXP[X^k]}{k!} s^k.
   $$
   Thus, when if the MGF is well defined in a neighborhood around origin, knowing all the moments of a distribution is sufficient to construct the MGF. We already saw that the MGF is sufficient to constrct the PDF/PMF of the distribution. Thus, **the distribution of a random variable is completely characterized by its moments.**


:::{.callout-note collapse="true"}
#### Proof

The first property follows from definition:
$$ 
M_X(0) = \EXP[e^{0 X}] = \EXP[1] = 1.
$$

For the general derivative, we have 
\begin{align*}
\frac{d^k}{ds^k} M_X(s) 
&= \int_{-∞}^∞ \frac{d^k}{ds^k} e^{sx} f_X(x) \, dx \\
&= \int_{-∞}^∞ x^k e^{sx} f_X(x) \, dx.
\end{align*}

Therefore
$$
\frac{d^k}{ds^k} M_X(s) \biggr|_{s=0} = \int_{-∞}^∞ x^k f_X(x) \, dx. 
$$
:::


:::{#exm-MGF-Bernoulli}
Use the MGF of Bernoulli to find its first all the moments of $X$.
:::

:::{.callout-note collapse="true"}
#### Solution

From @tbl-MGF, we see that 
$$ M_X(s) = 1 - p + p e^{s}. $$
Therefore,

- $\dfrac{d}{ds} M_X(s) = p e^s$. 

- $\dfrac{d^2}{ds^2} M_X(s) = p e^s$. 

- and in general
  $\dfrac{d^k}{ds^k} M_X(s) = p e^s$. 

Thus,

- $\EXP[X] = \dfrac{d}{ds} M_X(s) \biggr|_{s=0} = p$. 

- $\EXP[X^2] = \dfrac{d^2}{ds^2} M_X(s) \biggr|_{s=0} = p$. 

- and in general
  $\EXP[X^k] = \dfrac{d^k}{ds^k} M_X(s) \biggr|_{s=0} = p$. 

:::

### Moment generating functions and sums of independent random variables

:::{#thm-sum-of-iid}
Suppose $X_1, X_2, \dots, X_n$ are independent random variables defined on the same probability space. Let
$$
  Z = X_1 + X_2 + \dots + X_n
$$
Then,
$$
  M_Z(s) = M_{X_1}(s) M_{X_2}(s) \cdots M_{X_n}(s).
$$

Furthermore, if the variables are identically distributed, then
$$
  M_Z(s) = (M_{X_1}(s))^n.
$$

:::

:::{.callout-note collapse="true"}
#### Proof

The proof follows immediately from properties of independent random variables. We prove the result for $n=2$.
$$
M_Z(s) = \EXP[e^{sX_1} e^{sX_2}] 
= \EXP[e^{sX_1} e^{sX_2}] = M_{X_1}(s) M_{X_2}(s). 
$$
:::

@thm-sum-of-iid is a very useful result. An immediate implication of the result is that following:

1. **Sum of i.i.d. Bernoulli random variables is a Binomial random variable**

    Let $X_i \sim \text{Ber}(p)$. Then $M_{X_i}(s) = (1 - p + pe^s)$.

    Let $Z = \sum_{i=1}^n X_i$. Then $M_Z(s) = (1 - p + pe^s)^n$. 

2. **Sum of independent Binomial random variables with the same $p$ is a Binomial random variable.**

   Let $X_i \sim \text{Binom}(m_i,p)$. Then $M_{X_i}(s) = (1 - p + p e^s)^{m_i}$.

   Let $Z = \sum_{i=1}^n X_i$. Then $M_Z(s) = (1 - p + p e^s)^M$, where $M = \sum_{i=1}^n m_i$.

3. **Sum of independent Poisson random variables is Poisson.**

    Let $X_i \sim \text{Pois}(λ_i)$. Then $M_{X_i}(s) = e^{λ_i(e^s - 1)}$.

    Let $Z = \sum_{i=1}^n X_i$. Then $M_Z(s) = e^{λ(e^s - 1)}$, where $λ = \sum_{i=1}^n λ_i$. 

4. **Sum of Gaussian random variables is Gaussian.**

    Let $X_i \sim \mathcal N(μ_i, σ_i)$. Then, $M_{X_i}(s) = \exp( μ_i s + \frac 12 σ_i^2 s^2)$.

    Let $Z = \sum_{i=1}^n X_i$. Then $M_Z(s) = \exp(μ s + \frac 12 σ^2 s^2)$, where
    $$
      μ = \sum_{i=1}^n μ_i
      \quad\text{and}\quad
      σ^2 = \sum_{i=1}^n σ_i^2.
    $$

## Approximations via moment generating functions

One of the ways in which MGFs are useful is that they allow us to understand the limiting behavior of sum of i.i.d. random variables. We will study such a behavior in more detail later on, but we provide an illustration of the results here. 

Let $\{X_i\}_{i \ge 1}$ be a sequence of i.i.d. $\text{Ber}(p)$ random variables. Define 
$$
  S_n = X_1 + \cdots + X_n.
$$
As we saw above, 
$$
  M_{S_n}(s) = (1 - p + p e^s)^n
$$
which is the MGF of $\text{Ber}(n,p)$ random variable. 

Write $q$ for $1-p$ and $σ_n^2$ for $npq$ (which is the variance of Bernoulli random variable). Define
$$
  Z_n = \frac{S_n - np}{σ_n}
$$
We know that
\begin{align*}
  M_{Z_n}(s) &= \EXP[ e^{s (S_n - np)/σ_n^2} ]
  = e^{-s n p/σ_n} \EXP[ e^{s S_n/σ_n}] 
  = e^{-s n p/σ_n} M_{S_n}(s/σ_n).
  \\
  &= e^{-s n p/σ_n} (q + p e^{s/σ_n})^n 
  = (q e^{-s p/σ_n} + p e^{s q / σ_n})^n
\end{align*}
Let's consider the power series expansion of $q e^{-sp/σ_n} + pe^{sq/σ_n}$:
\begin{align*}
&q\left(1 - \frac{ps}{σ_n} + \frac{p^2 s^2}{2! σ_n^2} - \frac{p^3 s^3}{3! σ_n^3} + \cdots \right)
+
p\left(1 + \frac{qs}{σ_n} + \frac{q^2 s^2}{2! σ_n^2} + \frac{q^3 s^3}{3! σ_n^3} + \cdots \right)
\\
& \quad= 1 + \frac{pq s^2}{2 σ_n^2} + \frac{pq(p-q) s^3}{6 σ_n^2} + \cdots 
\end{align*}
Thus, for large values of $n$, we have $\log(1 + z)^n = n(z - z^2/2 + \cdots)$. Therefore, 
$$
\log M_{Z_n}(s) = \frac{s^2}{2} + \frac{(q-p) s^3}{6 \sqrt{n p q}}
+ \text{terms of order $\dfrac 1n$ or smaller}
$$
Thus, the logarithm of the MGF of $Z_n$ matches that of the standard normal until the $s^2/2$ term. As $n$ tends to infinity, the remainder terms tend to zero. 

This convergence of $M_{Z_n}(s)$ to $e^{s^2/2}$ can be used to rigorously prove that the distribution of "standard Binomial" converges to standard normal as $n$ tends to infinity. 
