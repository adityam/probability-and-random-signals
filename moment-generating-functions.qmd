---
title: Moment Generating Functions
---

## Moment Generating Functions

The **moment generating function (MGF)** of a random variable $X$ is defined as
$$ 
M_X(s) = \EXP[e^{sX}]
$$
provided the expectation exists.

- When $X$ is discrete, we have
  $$ M_X(s) = \sum_{x \in \text{range}(X)} e^{sx} p_X(x). $$

- When $X$ is continuous, we have
  $$ M_X(s) = \int_{-∞}^∞ e^{sx} f_X(x) \, dx. $$

:::{.callout-important}
### Relationship to Laplace transforms

Although most texts (including the textbook) restrict $s$ to be real, my personal view is that one should really interpret $s$ as a complex number. If we do so, then we have the following:

- $M_X(-s)$ is the Laplace transform of the PDF. 
- $M_X(-j ω)$ is the Fourier transform of the PDF, which is called the **characteristic function** of $X$.

Therefore, we can recover the PDF by taking the inverse Laplace transform of MGF. Thus, specifying the MGF of a random variable is equivalent to specifying the PDF.

Historically, MGF is defined for $s \in \reals$ and there are distributions (e.g., Cauchy) for which MGF does not exist for any $s \neq 0$. In avoid such situations, one uses the characteristic function because the characteristic function always exists. However, if we view the domain of MGF to be $\mathbb{C}$, then there is no need for a distinction between MGF and characteristic function. 
:::

:::{#exm-MGF-discrete}
Suppose $X$ is a random variable which takes values $\{0, 1, 2\}$ with probabilities $\{\frac 12, \frac 13, \frac 16\}$. Then,
\begin{align*}
M_X(s) &= \EXP[e^{sX}] \\
&= \frac 12 e^{s 0} + \frac 13 e^{s 1} + \frac 16 e^{s 2} \\
&= \frac 12 + \frac 13 e^{s} + \frac 16 e^{2s}.
\end{align*}
:::

:::{#exm-MGF-Poisson}
Find the MGF of a Poisson random variable with parameter $λ$. 
:::

:::{.callout-note collapse="true"}
#### Solution
\begin{align*}
M_X(s) &= \EXP[e^{sX}] = \sum_{k=0}^{∞}e^{ks} \frac{λ^k e^{-λ}}{k!} \\
&= e^{-λ} \sum_{k=0}^{∞} \frac{(λe^s)^k}{k!} \\
&= e^{-λ} e^{λ e^{s}}.
\end{align*}
:::

:::{#exm-MGF-exponential}
Find the MGF of an exponential random variable with parameter $λ$. 
:::

:::{.callout-note collapse="true"}
#### Solution
\begin{align*}
M_X(s) &= \EXP[e^{sX}] \\
&= \int_{0}^∞ e^{sx} λ e^{-λx} \, dx \\
&= λ \int_{0}^∞ e^{(s-λ)x} \, dx \\
&= \frac{λ}{λ-s}.
\end{align*}

Note that we could have looked up this result from the Laplace transform tables which show that
$$
e^{at} \xleftrightarrow{\hskip 0.5em \mathcal{L}\hskip 0.5em } \frac{1}{s-a}
$$
:::

The MGF of common random variables is shown in @tbl-MGF.

| **Random variable** | **Parameter(s)** | **MGF** |
|:---------------:|:------------:|:----:|
| Bernoulli       | $p$          | $1 - p + p e^s$ | 
| Binomial        | $(n,p)$      | $(1-p + p e^s)^n$ | 
| Geometric       | $p$          | $\dfrac{p e^s}{1 - (1-p)e^s}$ |
| Poisson         | $λ$          | $\exp(λ e^s - 1)$ |
| Uniform         | $(a,b)$      | $\dfrac{e^{sb} - e^{sa}}{s(b-a)}$ |
| Exponential     | $λ$          | $\dfrac{λ}{λ-s}$ |
| Gaussian        | $(μ,σ)$      | $\exp\bigl(μ s + \frac 12 σ^2 s^2 \bigr)$ |


: MGF of common random vables {#tbl-MGF}


If there exist a neighborhood around origin where $M_X(s)$ is well-defined. Then, we can use the MGF to "generate the moments" of $X$ as follows:

1. $M_X(0) = 1$

2. $\dfrac{d}{ds} M_X(s) \biggr|_{s=0} = \EXP[X]$.

3. $\dfrac{d^2}{ds^2} M_X(s) \biggr|_{s=0} = \EXP[X^2]$.

4. and in general 
   $\dfrac{d^k}{ds^k} M_X(s) \biggr|_{s=0} = \EXP[X^k]$.

5. Hence, by Taylor series expansion of $M_X(s)$ within the radius of convergence, we get
   $$
    M_X(s) = \sum_{k=0}^∞ \frac{\EXP[X^k]}{k!} s^k.
   $$
   Thus, when if the MGF is well defined in a neighborhood around origin, knowing all the moments of a distribution is sufficient to construct the MGF. We already saw that the MGF is sufficient to constrct the PDF/PMF of the distribution. Thus, **the distribution of a random variable is completely characterized by its moments.**


:::{.callout-note collapse="true"}
#### Proof

The first property follows from definition:
$$ 
M_X(0) = \EXP[e^{0 X}] = \EXP[1] = 1.
$$

For the general derivative, we have 
\begin{align*}
\frac{d^k}{ds^k} M_X(s) 
&= \int_{-∞}^∞ \frac{d^k}{ds^k} e^{sx} f_X(x) \, dx \\
&= \int_{-∞}^∞ x^k e^{sx} f_X(x) \, dx.
\end{align*}

Therefore
$$
\frac{d^k}{ds^k} M_X(s) \biggr|_{s=0} = \int_{-∞}^∞ x^k f_X(x) \, dx. 
$$
:::


:::{#exm-MGF-Bernoulli}
Use the MGF of Bernoulli to find its first all the moments of $X$.
:::

:::{.callout-note collapse="true"}
#### Solution

From @tbl-MGF, we see that 
$$ M_X(s) = 1 - p + p e^{s}. $$
Therefore,

- $\dfrac{d}{ds} M_X(s) = p e^s$. 

- $\dfrac{d^2}{ds^2} M_X(s) = p e^s$. 

- and in general
  $\dfrac{d^k}{ds^k} M_X(s) = p e^s$. 

Thus,

- $\EXP[X] = \dfrac{d}{ds} M_X(s) \biggr|_{s=0} = p$. 

- $\EXP[X^2] = \dfrac{d^2}{ds^2} M_X(s) \biggr|_{s=0} = p$. 

- and in general
  $\EXP[X^k] = \dfrac{d^k}{ds^k} M_X(s) \biggr|_{s=0} = p$. 

:::

### Moment generating functions and sums of independent random variables

:::{#thm-sum-of-iid}
Suppose $X_1, X_2, \dots, X_n$ are independent random variables defined on the same probability space. Let
$$
  Z = X_1 + X_2 + \dots + X_n
$$
Then,
$$
  M_Z(s) = M_{X_1}(s) M_{X_2}(s) \cdots M_{X_n}(s).
$$

Furthermore, if the variables are identically distributed, then
$$
  M_Z(s) = (M_{X_1}(s))^n.
$$

:::

:::{.callout-note collapse="true"}
#### Proof

The proof follows immediately from properties of independent random variables. We prove the result for $n=2$.
$$
M_Z(s) = \EXP[e^{sX_1} e^{sX_2}] 
= \EXP[e^{sX_1} e^{sX_2}] = M_{X_1}(s) M_{X_2}(s). 
$$
:::

@thm-sum-of-iid is a very useful result. An immediate implication of the result is that following:

1. **Sum of i.i.d. Bernoulli random variables is a Binomial random variable**

    Let $X_i \sim \text{Ber}(p)$. Then $M_{X_i}(s) = (1 - p + pe^s)$.

    Let $Z = \sum_{i=1}^n X_i$. Then $M_Z(s) = (1 - p + pe^s)^n$. 

2. **Sum of independent Binomial random variables with the same $p$ is a Binomial random variable.**

   Let $X_i \sim \text{Binom}(m_i,p)$. Then $M_{X_i}(s) = (1 - p + p e^s)^{m_i}$.

   Let $Z = \sum_{i=1}^n X_i$. Then $M_Z(s) = (1 - p + p e^s)^M$, where $M = \sum_{i=1}^n m_i$.

3. **Sum of independent Poisson random variables is Poisson.**

    Let $X_i \sim \text{Pois}(λ_i)$. Then $M_{X_i}(s) = e^{λ_i(e^s - 1)}$.

    Let $Z = \sum_{i=1}^n X_i$. Then $M_Z(s) = e^{λ(e^s - 1)}$, where $λ = \sum_{i=1}^n λ_i$. 

4. **Sum of Gaussian random variables is Gaussian.**

    Let $X_i \sim \mathcal N(μ_i, σ_i)$. Then, $M_{X_i}(s) = \exp( μ_i s + \frac 12 σ_i^2 s^2)$.

    Let $Z = \sum_{i=1}^n X_i$. Then $M_Z(s) = \exp(μ s + \frac 12 σ^2 s^2)$, where
    $$
      μ = \sum_{i=1}^n μ_i
      \quad\text{and}\quad
      σ^2 = \sum_{i=1}^n σ_i^2.
    $$
