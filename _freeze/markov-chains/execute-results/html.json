{
  "hash": "97dce00c0dcc7615c17022f8400c8c1f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Markov chains\n\nengine: jupyter\nexecute:\n  echo: false\n  cache: true\n  freeze: true\n---\n\nLet $\\ALPHABET X$ be a finite set. A stochastic process $\\{X_n\\}_{n \\ge 0}$, $X_n \\in \\ALPHABET X$, is called a **Markov chain** if it satisfies the _Markov property_: for any $n \\in \\integers_{\\ge 0}$ and any $x_{1:n+1} \\in \\ALPHABET X^{n+1}$, we have\n\\begin{equation}\\tag{Markov property}\\label{eq:Markov}\n  \\PR(X_{n+1} = x_{n+1} \\mid X_{1:n} = x_{1:n}) \n  = \\PR(X_{n+1} = x_{n+1} \\mid X_n = x_n).\n\\end{equation}\n\nThe variable $X_n$ is called the **state** of the Markov chain at time $n$; the set $\\ALPHABET X$ is called **state space**. The \\eqref{eq:Markov} implies that the current state captures all the information from the past that is relevant for the future. Stated differently, **conditioned on the present, the past is independent of the future**.\n\nIndependence is a symmetric relationship. Thus, we expect the Markov property to also hold if time is reversed! The next exercise asks you to formally prove that. \n\n:::{#exr-time-reversal}\n#### Time reversal of Markov chains\n\nLet $\\{X_n\\}_{n \\ge 1}$ is a Markov chain. Show that for any $N > n$,\n$$\n  \\PR(X_n = x_n \\mid X_{n+1:N} = x_{n+1:N})\n  = \\PR(X_n = x_n \\mid X_{n+1} = x_{n+1}).\n$$\nThus, a time reversed Markov chain is also Markov.\n:::\n\n## Time-homogeneous Markov chains\n\nIn this course, we will focus on time-homogeneous Markov chain.\nThe Markov chain is called **time-homogeneous** if the right hand side of \\eqref{eq:Markov} does not depend on $n$. In this case, we describe the Markov chain by a **state transition matrix** $P$, where $P_{ij} = \\PR(X_{n+1} = j | X_n = i)$. Such Markov chains can also be visualized using **state transition diagrams** as we illustrate in the example below. \n\n:::{#exm-random-walk}\n#### Random walk in one dimension\n\nImagine a particle which moves in a straight line in unit steps. Each step is one unit to the right with probability $p$ or one unit to the left with probabity $q = 1-p$. It moves until it reaches one of two extreme points, which are called **boundary points**. The behavior of the particle at the boundary determines several different possibilities.\n\nWe will consider the case where the state space is $\\ALPHABET X = \\{-2, -1, 0, 1, 2\\}$ and the process starts in state $0$.\n\n1. **Absorbing random walk:** Assume that when the particle reaches a boundary state, it stays there from that time on. We may visualize the Markov chain as follows.\n\n   ![Absorbing random walk](figures/svg/random-walk1.svg)\n\n   In this case, the transition matrix is given by\n   $$\n      P = \\MATRIX{ 1 & 0 & 0 & 0 & 0 \\\\\n                   q & 0 & p & 0 & 0 \\\\\n                   0 & q & 0 & p & 0 \\\\\n                   0 & 0 & q & 0 & p \\\\\n                   0 & 0 & 0 & 0 & 1}.\n   $$\n\n2. **Reflected random walk:** Assume that when the particle reaches a boundary states, it is _reflected_ and returns to the point it came from. We may visualize the Markov chain as follows.\n\n   ![Reflected random walk](figures/svg/random-walk2.svg)\n\n   In this case, the transition matrix is given by\n   $$\n      P = \\MATRIX{ 0 & 1 & 0 & 0 & 0 \\\\\n                   q & 0 & p & 0 & 0 \\\\\n                   0 & q & 0 & p & 0 \\\\\n                   0 & 0 & q & 0 & p \\\\\n                   0 & 0 & 0 & 1 & 0}.\n   $$\n\n3. **Random walk with restart:** Assume that when the particle reaches a boundary state, it restarts in the initial state. We may visualize the Markov chain as follows.\n\n   ![Random walk with restart](figures/svg/random-walk3.svg)\n\n   In this case, the transition matrix is given by\n   $$\n      P = \\MATRIX{ 0 & 0 & 1 & 0 & 0 \\\\\n                   q & 0 & p & 0 & 0 \\\\\n                   0 & q & 0 & p & 0 \\\\\n                   0 & 0 & q & 0 & p \\\\\n                   0 & 0 & 1 & 0 & 0}.\n   $$\n\n4. **Random walk with periodic boundary:** Assume that when the particle reaches a boundary state, it moves to the other boundary. We may visualize the Markov chain as follows.\n\n   ![Random walk with periodic boundary](figures/svg/random-walk4.svg)\n\n   In this case, the transition matrix is given by\n   $$\n      P = \\MATRIX{ 0 & 0 & 0 & 0 & 1 \\\\\n                   q & 0 & p & 0 & 0 \\\\\n                   0 & q & 0 & p & 0 \\\\\n                   0 & 0 & q & 0 & p \\\\\n                   1 & 0 & 0 & 0 & 0}.\n   $$\n:::\n\n### State occupancy probabilities\n\nLet $μ^{(n)}$ denote the PMF of the state of the Markov chain at time $n$. This is also called the **state occupancy probababilites**. We will think of of $μ^{(n)}$ as a row vector. Then, by the law of total probability, we have\n$$\n  \\PR(X_n = j) = \\sum_{i \\in \\ALPHABET X} \\PR(X_{n-1} = i) \\PR(X_n = j | X_{n-1} = i)\n$$\nor, equivalently,\n$$\n  μ^{(n)}_j = \\sum_{i \\in \\ALPHABET X} μ^{(n-1)}_i P_{ij}\n$$\nwhich can be written in matrix form as\n$$ μ^{(n)} = μ^{(n-1)} P $$\nand, by recusively expanding the right hand side, we have\n$$ μ^{(n)} = μ^{(0)} P^n. $$\n\nWe will abbreviate $[P^n]_{ij}$ as $P^{(n)}_{ij}$. \n\n:::{#exm-random-walk-occupancy}\nWe now compute the state occupancy probabilities for the different examples of random walk presented in @exm-random-walk when $p = q = \\tfrac 12$ for $n \\in \\{1, \\dots, 8\\}$. In all cases, the initial probability \n$$\n  μ^{(0)} = \\MATRIX{0 & 0 & 1 & 0 & 0 }.\n$$\n:::\n\n:::{.callout-note collapse=\"true\"}\n### Solution\n\n\n\n1. **Absorbing random walk:** \n   In this case, we have\n\n\n    \n    - $μ^{(0)} = \\left[\n    \\begin{array}{ccccc}\n    0 & 0 & 1 & 0 & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(1)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(2)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(3)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(4)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{3}{8} & 0 & \\frac{1}{4} & 0 & \\frac{3}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(5)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{3}{8} & \\frac{1}{8} & 0 & \\frac{1}{8} & \\frac{3}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(6)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{7}{16} & 0 & \\frac{1}{8} & 0 & \\frac{7}{16} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(7)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{7}{16} & \\frac{1}{16} & 0 & \\frac{1}{16} & \\frac{7}{16} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(8)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{15}{32} & 0 & \\frac{1}{16} & 0 & \\frac{15}{32} \\\\\n    \\end{array}\n    \\right]$\n    \n    \n    \n2. **Reflected random walk:** \n   In this case, we have\n\n\n    \n    - $μ^{(0)} = \\left[\n    \\begin{array}{ccccc}\n    0 & 0 & 1 & 0 & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(1)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(2)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(3)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(4)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(5)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(6)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(7)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(8)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    \n    \n    \n3. **Random walk with restart:** \n   In this case, we have\n\n\n    \n    - $μ^{(0)} = \\left[\n    \\begin{array}{ccccc}\n    0 & 0 & 1 & 0 & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(1)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(2)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(3)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(4)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{8} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(5)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{8} & \\frac{1}{8} & \\frac{1}{2} & \\frac{1}{8} & \\frac{1}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(6)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{16} & \\frac{1}{4} & \\frac{3}{8} & \\frac{1}{4} & \\frac{1}{16} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(7)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{8} & \\frac{3}{16} & \\frac{3}{8} & \\frac{3}{16} & \\frac{1}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(8)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{3}{32} & \\frac{3}{16} & \\frac{7}{16} & \\frac{3}{16} & \\frac{3}{32} \\\\\n    \\end{array}\n    \\right]$\n    \n    \n    \n3. **Random walk with periodic boundary:** \n   In this case, we have\n\n\n    \n    - $μ^{(0)} = \\left[\n    \\begin{array}{ccccc}\n    0 & 0 & 1 & 0 & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(1)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(2)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(3)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(4)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{3}{8} & 0 & \\frac{1}{4} & 0 & \\frac{3}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(5)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{3}{8} & \\frac{1}{8} & 0 & \\frac{1}{8} & \\frac{3}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(6)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{7}{16} & 0 & \\frac{1}{8} & 0 & \\frac{7}{16} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(7)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{7}{16} & \\frac{1}{16} & 0 & \\frac{1}{16} & \\frac{7}{16} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(8)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{15}{32} & 0 & \\frac{1}{16} & 0 & \\frac{15}{32} \\\\\n    \\end{array}\n    \\right]$\n    \n    \n    \n:::\n\n### Properties of interest\n\nDepending on the application, we are typically interested in the following properties of a Markov chain:\n\n- If the chain starts in state $i$, what is the probability that after $n$ steps it is in state $j$?\n\n- If the chain starts in state $i$, what is the expected number of visits to state $j$ in $n$ steps?\n\n- What is the expected number of steps that it takes for a chain starting in state $i$ to visit state $j$ for the first time?\n\n- What is the average number of times that the chain is in state $i$? How does this depend on the initial state?\n\nIn the rest of this section, we will present results in Markov chain theory that answer the above questions. \n\n## Hitting times and expected number of visits\n\n1. We use the following notation:\n\n    - $P_i(A)$ denotes $\\PR(A \\mid X_0 = i)$ and $\n    - $\\EXP_i[Y]$ denotes $\\EXP[Y \\mid X_0 = i]$.\n\n2. Let $A$ be a subset of $\\ALPHABET X$. The **hitting time** $T_A$ of $A$ is defined by\n   $$\n     T_A = \\min\\{n > 0 : X_n \\in A\\}.\n   $$\n   The standard convention is that $T_A$ is taken to be $∞$ if $X_n \\neq A$ for any $n > 0$. For a state $j \\in \\ALPHABET X$, we use the short-hand $T_j$ to denote $T_{\\{j\\}}$. \n\n3. Define \n   $$\n     f^{(n)}_{ij} \\coloneqq P_i(T_j = n) = P_i(X_1 \\neq j, \\dots, X_{n-1} \\neq j, X_n = j).\n   $$\n\n4. $f^{(n)}_{ij}$ satisfies the following recursion.\n\n   - $f^{(1)}_{ij} = P_i(X_1 = j) = P_{ij}$.\n   - And for $n > 1$, $f^{(n+1)}_{ij} = \\sum_{k \\neq j} P_{ik} f^{(n)}_{kj}$.\n\n5. Let\n   $$\n     f_{ij} = \\sum_{n=1}^∞ f^{(n)}_{ij} = P_i(T_j < ∞)\n   $$\n   denotes the probability that a chain starting in $i$ eventually visits $j$. In particular $f_{jj}$ denotes the probability that a chain starting in $j$ will return to $j$. \n\n6. $f_{ij}$ satisfy the following property.  \n   $$\\displaystyle P^{(n)}_{ij} = \\sum_{m=1}^n  f^{(m)}_{ij} P^{(n-m)}_{jj}.$$\n\n      An immediate implication of the above is that if $j$ is an absorbing state then \n      $P^{(n)}_{ij} = \\sum_{m=1}^n f^{(m)}_{ij} = P_i(T_j \\le n).$\n\n\n7. Let $N^{(n)}_j = \\sum_{m=1}^{n} \\IND\\{X_m = j\\}$ denote the number of visits to state $j$ in $n$ steps. Define\n   $$\n     G_{ij}^{(n)} = \\EXP_i[ N^{(n)}_j ] = \\sum_{m=1}^n P^{(m)}_{ij}\n   $$\n   to be the expected number of visits to state $j$ in $n$ steps, starting in $i$.\n\n8. Let $N_j = \\lim_{n \\to ∞} N^{(n)}_j \\sum_{m=1}^∞ \\IND\\{X_m = j \\}$ denote the number of visits to state $j$. Similarly, define\n   $$\n      G_{ij} = \\EXP_{i}[N_j] = \\lim_{n \\to ∞} G_{ij}^{(n)}\n      = \\sum_{m=1}^{∞} P^{(m)}_{ij}\n   $$\n   to denote the expected number of visits to state $j$ for a chain starting in $i$.\n\n8. A state $j$ is **recurrent** if $f_{jj} = 1$ and **transient** if $f_{jj} < 1$. \n\n9. A state is called periodic if $f_{ii}^(n)$ is non-zero only for multiples of some smallest integer $d$, $d > 1$. \n\n9. For every transient state $j$, we have for every $i$, $P_i(N_j < ∞) = 1$ and \n   $$ G_{ij} = \\dfrac{f_{ij}}{1 - f_{jj}}.$$\n   On the other hand, if $j$ is recurrent, then $P_j(N_j = ∞) = 1$ and $G_{jj} = ∞$. Moreover,\n   $$ P_i(N_j = ∞) = P_i(T_j < ∞) = f_{ij}. $$\n   So, if $f_{ij} = 0$, then $G_{ij} = 0$ while if $f_{ij} = ∞$ then $G_{ij} = ∞$. \n\n10. Thus, a state $i$ is recurrent if and only if \n    $$G_{ii} = \\sum_{n=1}^∞ P^{(n)}_{ii} = ∞. $$\n\n11. A state $j$ is said to be **accessible from** $i$ (abbreviated as $i \\rightsquigarrow j$) if there is an ordered string of notes $(i_0, \\dots, i_m)$ such that $i_0 = i$ and $i_m = j$ and $P_{i_k i_{k+1}} > 0$. Equivalently, $i \\rightsquigarrow j$ if there exists a $m$ such that $P^{(m)}_{ij} > 0$. \n\n12. Accessibility is an transitive relationship, i.e., if $i \\rightsquigarrow j$ and $j \\rightsquigarrow k$ implies that $i \\rightsquigarrow k$. \n\n#. If $f_{ij} > 0$ but $f_{ji} < 1$, then $i$ is transient. \n\n13. If $i$ is recurrent and $i \\rightsquigarrow j$. Then, $j$ is also recurrent and $f_{ij} = f_{ji} = 1$. \n\n14. A subset $C$ of $\\ALPHABET X$ is said to be **closed** if no state inside $C$ can lead to any state outside $C$, i.e.,\n    $$\n        f_{ij} = 0, \\quad \\forall i \\in C \\text{ and } j \\not\\in C.\n    $$\n\n15. A closed set $C$ is called **irreducible** if $i \\rightsquigarrow j$ for all $i,j \\in C$. Thus, if $C$ is an irreducible set, then all states in $C$ are either recurrent or transient. \n\n16. Consequently, if $C$ is an irreducible and closed set of recurrent states. Then for all $i,j \\in C$, \n  \n      - $f_{ij} = 1$\n      - $P_i(N_j = ∞) = 1$\n      - $G_{ij} = ∞$\n\n17. If $C$ is a finite irreducible closed set of states. Then every state in $C$ is recurrent.\n\n18. Let $\\ALPHABET X_T$ and $\\ALPHABET X_R$ denote the set of transient and recurrent states. The set $\\ALPHABET X_R$ can be paritioned into a finite or countable number of irreducible closed sets $C_1$, $C_2$, $\\dots$. \n\n## Absorption probabilities\n\n1. Let $C$ be one of the irreducible closed sets of recurrent states. Define $f_{iC} = P_i(T_C < ∞)$ to be the probability that a chain starting at $i$ eventually hits $C$ (and is _absorbed_ in $C$). \n\n2. Clearly, $f_{iC} = 1$ for $i \\in C$ and $f_{iC} = 0$ if $i$ is a recurrent state not in $C$. \n\n3. Suppose $i \\in \\ALPHABET X_T$. Then, $f_{iC}$ satisfies the following:\n   $$\n      f_{iC} = \\sum_{j \\in C} P_{ij} \n      + \\sum_{j \\in \\ALPHABET X_T} P_{ij} f_{jC}.\n   $$\n\n    When $\\ALPHABET X_T$ is finite, the above system has a unique solution.\n\n:::{#exm-gambler-ruin}\nConsider a gambler's ruin problem, where we start at state $1$ and stop if we hit state $0$ or $3$.\n\n![A gambler's ruin problem](figures/svg/markov-chains3.svg)\n\nFind the probability of getting absorbed in state $0$ before state $3$. \n\n:::\n\n:::{.callout-warning collapse=\"true\"}\n### Solution\nLet $f_{i0}$ denote the probability of getting absorbed in state $0$ before $3$.\nThen, we can write the following system of equations to describe the\nabsorption probabilities:\n\\begin{align*}\n  f_{10} &= q + p f_{20} \\\\\n  f_{20} &= q f_{10}. \n\\end{align*}\nWe can solve this system of equations to find $f_{10}$ and $f_{20}$.\n:::\n\n\n## Expected duration of play\n\nLet's start with a simple example. Suppose we toss a coin multiple times and\nstop at a heads. What are the expected number of tosses until stopping?\n\nFrom elementary probability we know that the number of tosses until stopping\nis a geometric random variable. However, we will model this using a Markov\nchain where the state denotes the number of consecutive heads so far. Let $p$\ndenote the probability of heads and $q = 1-p$ denote the probability of tails.\nThen, the Markov chain model is as follows.\n\n![Markov chain for coin tossing until one head](figures/svg/markov-chains1.svg)\n\n\nLet $v_i$ denote the expected number of tosses until stopping when starting at\nstate $i$. Then, we have\n\\begin{align*}\n  v_0 &= 1 + q v_0 + p v_1, \\\\\n  v_1 &= 0.\n\\end{align*}\nSolving this system of equations, we get $v_0 = 1/(1-q) = 1/p$. \n\nNow, let's try a variation of the above model. Suppose we toss a coin multiple\ntimes and stop at two heads. What are the expected number of tosses until\nstopping. \n\nWe can model this in the same manner as the before, where the state denotes\nthe number of consecutive heads so far. The Markov chain is as follows:\n\n![Markov chain for coin tossing until two heads](figures/svg/markov-chains2.svg)\n\nAs before, let $v_i$ denote the expected number of tosses until stopping when\nstarting at state $i$. Then, we have\n\\begin{align*}\n  v_0 &= 1 + q v_0 + p v_1, \\\\\n  v_1 &= 1 + q v_0 + p v_2, \\\\\n  v_2 &= 0.\n\\end{align*}\nSolving this system of equations, we get $v_0 = 1/(1-p)$. \n\nWe can generalize these ideas to find time of hitting a state.\n\n\n## Stationary distribution\n\n1. A distribution $π$ is said the be a **stationary distribution** if \n    $$\n      π = π P.\n    $$\n\n2. Stationary distributions can be computed by solving **balance equations.**\n\n3. Let $C$ be an irreducible class of a Markov chain. Then, there is a unique stationary distribution $π$ that assigns positive probability only to states in $C$. \n\n4. If $π_1$ and $π_2$ are stationary distributions of a Markov chain and $α \\in (0,1)$, then $α π_1 + (1-α) π_2$ is also a stationary distribution. \n\n5. Thus, if a Markov chain has a single irreducible class, then it has a unique stationary distribution; if it has multiple irreducible classes, then it has uncountable number of stationary distributions.\n\n## Limiting distribution\n\n1. Suppose $j$ is a transient state. Then, we know that $N_j < ∞$ and $G_{ij} < ∞$. Therefore,\n   $$\n      \\lim_{n \\to ∞}\n      \\frac{N^{(n)}_{j}}{n} = 0, a.s., \n      \\quad\\text{and}\\quad\n      \\lim_{n \\to ∞}\n      \\frac{G^{(n)}_{ij}}{n} = 0, \\forall i \\in \\ALPHABET X.\n   $$\n\n2. Suppose $j$ is a recurrent state. Then, \n   $$\n      \\lim_{n \\to ∞}\n      \\frac{N^{(n)}_{j}}{n} = μ_j, a.s., \n      \\quad\\text{and}\\quad\n      \\lim_{n \\to ∞}\n      \\frac{G^{(n)}_{ij}}{n} = μ_j, \\forall i \\in \\ALPHABET X\n   $$\n   where $μ_j = \\EXP_{j}[T_j]$ is the mean return time to state $j$. \n\n\nA Markov chain is said to have a _steady state distribution_ if $π_n$\nconverges to a limit as $n \\to ∞$ and the limit does not depend on the initial\ndistribution $π_0$. \n\nA Markov chain has a steady state distribution if it is ergodic. We can find\nthe steady state distribution by solving the _balance equation_: $π = π P$. \n\n",
    "supporting": [
      "markov-chains_files"
    ],
    "filters": [],
    "includes": {}
  }
}