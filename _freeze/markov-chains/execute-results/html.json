{
  "hash": "0c5712d6af24518b7ac62747a9dbcac9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Markov chains\n\nengine: jupyter\nexecute:\n  echo: false\n  cache: true\n  freeze: true\n---\n\nLet $\\ALPHABET X$ be a finite set. A stochastic process $\\{X_n\\}_{n \\ge 0}$, $X_n \\in \\ALPHABET X$, is called a **Markov chain** if it satisfies the _Markov property_: for any $n \\in \\integers_{\\ge 0}$ and any $x_{1:n+1} \\in \\ALPHABET X^{n+1}$, we have\n\\begin{equation}\\tag{Markov property}\\label{eq:Markov}\n  \\PR(X_{n+1} = x_{n+1} \\mid X_{1:n} = x_{1:n}) \n  = \\PR(X_{n+1} = x_{n+1} \\mid X_n = x_n).\n\\end{equation}\n\nThe variable $X_n$ is called the **state** of the Markov chain at time $n$; the set $\\ALPHABET X$ is called **state space**. The \\eqref{eq:Markov} implies that the current state captures all the information from the past that is relevant for the future. Stated differently, **conditioned on the present, the past is independent of the future**.\n\nIndependence is a symmetric relationship. Thus, we expect the Markov property to also hold if time is reversed! @exr-time-reversal asks you to formally prove that. \n\nWe now present some examples of Markov chains arising in different applications.\n\n:::{#exm-Gilbert-Elliot-channel}\n#### Gilbert Elliot channel model for burst erasures\n\n:::\n\n:::{#exm-Ehrenfest-diffusion}\n#### Ehrenfest model of diffusion\n\nThe following simplified model of diffusion through a porous membrane was proposed by @Ehrenfest1907 to describe the exchange of heat between two systems at different temperature. \n\nThere are $K$ particles that can either be in compartment $A$ or compartment $B$. At each time, a particle is picked at random and moved from its compartment to the other. Thus, if there are $X_n = i$ particles in compartment $A$ at time $n$, then the next state $X_{n+1}$ is either $i-1$ (a particle was displaced from compartment $A$ to $B$) with probability $i/K$, or $i+1$ (a particle was displaced from compartment $B$ to $A$) with probability $(K-i)/K$. \n:::\n\n:::{#exm-gamblers-ruin}\n#### Gambler's ruin\n\nConsider a gambler who is playing in a casino. The gambler starts with an initial fortune of \\$$x_0$. At each time, the gambler places a bet where he wins \\$$1$ with probability $p$ and loses \\$$1$ with probability $1-p$. Thus, his fortune evolves as \n$$\n  X_{n+1} = X_n + Z_n\n$$\nwhere $\\{Z_n\\}_{n \\ge 0}$ is an i.i.d.\\ sequence which takes values $+1$ with probability $p$ and $-1$ with probability $1-p$. \n\nThe gambler stops when his fortune is ruined, i.e., $X_n = 0$. In some variations, the gambler may also stop when his fortune reaches a pre-specified amount of \\$$K$.\n:::\n\n<!--\n**TODO:** Add examples and non-examples of Markov processes. Not necessarily the mathematical description, but simply models that are Markov\n\n- iid coin tosses\n- Success runs\n- On off Markov chain (Gilbert Elliot)\n- Gambler's ruin\n\n-->\n\n## Time-homogeneous Markov chains\n\nIn this course, we will focus on time-homogeneous Markov chain.\nThe Markov chain is called **time-homogeneous** if the right hand side of \\eqref{eq:Markov} does not depend on $n$. In this case, we describe the Markov chain by a **state transition matrix** $P$, where $P_{ij} = \\PR(X_{n+1} = j | X_n = i)$. Such Markov chains can also be visualized using **state transition diagrams** as we illustrate in the examples below. \n\n:::{#exm-on-off}\n#### On-Off Markov chain\n\nThe On-Off Markov chain discussed earlier can be modelled with $\\ALPHABET X = \\{0, 1\\}$ and general transition probability matrix of the form.\n$$\n  P = \\MATRIX{ 1 - a & a \\\\ b & 1 - b }.\n$$\nThe transition matrix can be visualized as follows.\n\n![On-Off Markov chain](figures/svg/markov-examples1.svg)\n:::\n\nIn spite of its simplicity, such simple on-off Markov chains are used in various applications including telecommunication systems, network traffic modeling, machine failure-repair models, gene activation, and others. Some properties of the on-off Markov chain are as follows:\n\n- If $a + b = 1$, then both rows of the transition matrix are identical. Therefore, the Markov chain _has no memory_ and is equivalent to a Bernoulli process with success probability $a = 1-b$.\n\n- When $a$ or $b$ are small, the corresponding state is \"sticky\", i.e., when the Markov chain enters a sticky state, it stays there for a long time.\n\n:::{#exm-Ehrenfest-diffusion-model}\nThe Ehrenfest model of diffusion presented in @exm-Ehrenfest-diffusion can be modelled as a Markov chain with state space $\\{0, 1, \\dots, K\\}$ and transition probability\n$$\n  P_{ij} = \\begin{cases}\n    i/K     & j = i-1 \\\\\n    (K-i)/K & j = i + 1 \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n$$\nThe transition matrix can be visualized as follows.\n\n![Ehrenfest model of diffusion](figures/svg/markov-examples2.svg)\n\n:::\n\n:::{#exm-random-walk}\n#### Random walk in one dimension\n\nImagine a particle which moves in a straight line in unit steps. Each step is one unit to the right with probability $p$ or one unit to the left with probabity $q = 1-p$. It moves until it reaches one of two extreme points, which are called **boundary points**. The behavior of the particle at the boundary determines several different possibilities.\n\nWe will consider the case where the state space is $\\ALPHABET X = \\{-2, -1, 0, 1, 2\\}$ and the process starts in state $0$.\n\n1. **Absorbing random walk:** Assume that when the particle reaches a boundary state, it stays there from that time on. We may visualize the Markov chain as follows.\n\n   ![Absorbing random walk](figures/svg/random-walk1.svg)\n\n   In this case, the transition matrix is given by\n   $$\n      P = \\MATRIX{ 1 & 0 & 0 & 0 & 0 \\\\\n                   q & 0 & p & 0 & 0 \\\\\n                   0 & q & 0 & p & 0 \\\\\n                   0 & 0 & q & 0 & p \\\\\n                   0 & 0 & 0 & 0 & 1}.\n   $$\n\n2. **Reflected random walk:** Assume that when the particle reaches a boundary states, it is _reflected_ and returns to the point it came from. We may visualize the Markov chain as follows.\n\n   ![Reflected random walk](figures/svg/random-walk2.svg)\n\n   In this case, the transition matrix is given by\n   $$\n      P = \\MATRIX{ 0 & 1 & 0 & 0 & 0 \\\\\n                   q & 0 & p & 0 & 0 \\\\\n                   0 & q & 0 & p & 0 \\\\\n                   0 & 0 & q & 0 & p \\\\\n                   0 & 0 & 0 & 1 & 0}.\n   $$\n\n3. **Random walk with restart:** Assume that when the particle reaches a boundary state, it restarts in the initial state. We may visualize the Markov chain as follows.\n\n   ![Random walk with restart](figures/svg/random-walk3.svg)\n\n   In this case, the transition matrix is given by\n   $$\n      P = \\MATRIX{ 0 & 0 & 1 & 0 & 0 \\\\\n                   q & 0 & p & 0 & 0 \\\\\n                   0 & q & 0 & p & 0 \\\\\n                   0 & 0 & q & 0 & p \\\\\n                   0 & 0 & 1 & 0 & 0}.\n   $$\n\n4. **Random walk with periodic boundary:** Assume that when the particle reaches a boundary state, it moves to the other boundary. We may visualize the Markov chain as follows.\n\n   ![Random walk with periodic boundary](figures/svg/random-walk4.svg)\n\n   In this case, the transition matrix is given by\n   $$\n      P = \\MATRIX{ 0 & 0 & 0 & 0 & 1 \\\\\n                   q & 0 & p & 0 & 0 \\\\\n                   0 & q & 0 & p & 0 \\\\\n                   0 & 0 & q & 0 & p \\\\\n                   1 & 0 & 0 & 0 & 0}.\n   $$\n:::\n\nNote that the gambler's fortune in @exm-gamblers-ruin is a random walk on non-negative integers $\\{0,1,2, \\dots\\}$ with absorption at $0$. In the variation where the gambler stops when his fortune reaches \\$$K$, it is a random walk over $\\{0,1,\\dots,K\\}$ with absorption at both ends: $0$ and $K$. \n\n**TODO**: Add other examples\n\n- Success runs\n\n### Properties of interest\n\nDepending on the application, we are typically interested in the following properties of a Markov chain:\n\n- If the chain starts in state $i$, what is the probability that after $n$ steps it is in state $j$?\n\n- If the chain starts in state $i$, what is the expected number of visits to state $j$ in $n$ steps?\n\n- What is the expected number of steps that it takes for a chain starting in state $i$ to visit state $j$ for the first time?\n\n- What is the average number of times that the chain is in state $i$? How does this depend on the initial state?\n\nIn the rest of this section, we will present results in Markov chain theory that answer the above questions. \n\n## State occupancy probabilities\n\nLet $μ^{(n)}$ denote the PMF of the state of the Markov chain at time $n$. This is also called the **state occupancy probababilites**. We will think of of $μ^{(n)}$ as a row vector. Then, by the law of total probability, we have\n$$\n  \\PR(X_n = j) = \\sum_{i \\in \\ALPHABET X} \\PR(X_{n-1} = i) \\PR(X_n = j | X_{n-1} = i)\n$$\nor, equivalently,\n$$\n  μ^{(n)}_j = \\sum_{i \\in \\ALPHABET X} μ^{(n-1)}_i P_{ij}\n$$\nwhich can be written in matrix form as\n$$ μ^{(n)} = μ^{(n-1)} P $$\nand, by recusively expanding the right hand side, we have\n$$ μ^{(n)} = μ^{(0)} P^n. $$\n\nWe will abbreviate $[P^n]_{ij}$ as $P^{(n)}_{ij}$. \n\n:::{#exm-random-walk-occupancy}\nNumerically compute the state occupancy probabilities for the different examples of random walk presented in @exm-random-walk when $p = q = \\tfrac 12$ for $n \\in \\{1, \\dots, 8\\}$. In all cases, the initial probability \n$$\n  μ^{(0)} = \\MATRIX{0 & 0 & 1 & 0 & 0 }.\n$$\n:::\n\n:::{.callout-note collapse=\"true\"}\n### Solution\n\n\n\n1. **Absorbing random walk:** \n   In this case, we have\n\n\n    \n    - $μ^{(0)} = \\left[\n    \\begin{array}{ccccc}\n    0 & 0 & 1 & 0 & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(1)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(2)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(3)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(4)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{3}{8} & 0 & \\frac{1}{4} & 0 & \\frac{3}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(5)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{3}{8} & \\frac{1}{8} & 0 & \\frac{1}{8} & \\frac{3}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(6)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{7}{16} & 0 & \\frac{1}{8} & 0 & \\frac{7}{16} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(7)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{7}{16} & \\frac{1}{16} & 0 & \\frac{1}{16} & \\frac{7}{16} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(8)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{15}{32} & 0 & \\frac{1}{16} & 0 & \\frac{15}{32} \\\\\n    \\end{array}\n    \\right]$\n    \n    \n    \n2. **Reflected random walk:** \n   In this case, we have\n\n\n    \n    - $μ^{(0)} = \\left[\n    \\begin{array}{ccccc}\n    0 & 0 & 1 & 0 & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(1)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(2)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(3)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(4)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(5)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(6)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(7)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(8)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    \n    \n    \n3. **Random walk with restart:** \n   In this case, we have\n\n\n    \n    - $μ^{(0)} = \\left[\n    \\begin{array}{ccccc}\n    0 & 0 & 1 & 0 & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(1)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(2)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(3)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{4} & \\frac{1}{2} & \\frac{1}{4} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(4)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{8} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(5)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{8} & \\frac{1}{8} & \\frac{1}{2} & \\frac{1}{8} & \\frac{1}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(6)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{16} & \\frac{1}{4} & \\frac{3}{8} & \\frac{1}{4} & \\frac{1}{16} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(7)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{8} & \\frac{3}{16} & \\frac{3}{8} & \\frac{3}{16} & \\frac{1}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(8)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{3}{32} & \\frac{3}{16} & \\frac{7}{16} & \\frac{3}{16} & \\frac{3}{32} \\\\\n    \\end{array}\n    \\right]$\n    \n    \n    \n3. **Random walk with periodic boundary:** \n   In this case, we have\n\n\n    \n    - $μ^{(0)} = \\left[\n    \\begin{array}{ccccc}\n    0 & 0 & 1 & 0 & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(1)} = \\left[\n    \\begin{array}{ccccc}\n    0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(2)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(3)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{1}{4} & \\frac{1}{4} & 0 & \\frac{1}{4} & \\frac{1}{4} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(4)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{3}{8} & 0 & \\frac{1}{4} & 0 & \\frac{3}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(5)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{3}{8} & \\frac{1}{8} & 0 & \\frac{1}{8} & \\frac{3}{8} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(6)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{7}{16} & 0 & \\frac{1}{8} & 0 & \\frac{7}{16} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(7)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{7}{16} & \\frac{1}{16} & 0 & \\frac{1}{16} & \\frac{7}{16} \\\\\n    \\end{array}\n    \\right]$\n    - $μ^{(8)} = \\left[\n    \\begin{array}{ccccc}\n    \\frac{15}{32} & 0 & \\frac{1}{16} & 0 & \\frac{15}{32} \\\\\n    \\end{array}\n    \\right]$\n    \n    \n    \n:::\n\n:::{#exm-on-off-occupancy}\nAnalytically compute the state occupancy probabilites for the on-off Markov chain of @exm-on-off when it starts from the initial probability distribution $μ^{(0)}$. \n:::\n:::{.callout-note collapse=\"true\"}\n#### Solution\n\nSince $μ^{(n+1)} = μ^{(n)} P$, we have\n\\begin{align*}\n  μ^{(n+1)}_0 &= μ^{(n)}_0 (1-a) + μ^{(n)}_1 b\n  \\\\\n  &= μ^{(n)}_0 (1-a) + (1 - μ^{(n)}_0) b\n  \\\\\n  &= μ^{(n)}_0 (1-a-b) + b.\n\\end{align*}\nIf $a = b = 0$, then $μ^{(n+1)}_0 = μ^{(n)_0 = \\cdots = μ^{(0)}_0$. \nIf not, we exploit the fact that \n$$\n  b = \\frac{b}{a+b} - \\frac{b}{a+b}(1-a-b)  \n$$\nto recursively write\n\\begin{align*}\n  μ^{(1)}_0 &= μ^{(0)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b) + \\frac{b}{a+b}\n\\end{align*}\nand\n\\begin{align*}\n  μ^{(2)}_0 &= μ^{(1)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^2 + \\frac{b}{a+b}(1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^2 + \\frac{b}{a+b}\n\\end{align*}\nand, so on, to get\n\\begin{align*}\n  μ^{(n)}_0 &= μ^{(n-1)}_0 (1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^n + \\frac{b}{a+b}(1-a-b) + b \\\\\n  &= \\left(μ^{(0)}_0 - \\frac{b}{a+b}\\right)(1-a-b)^n + \\frac{b}{a+b}.\n\\end{align*}\nTherefore,\n$$\n  μ^{(n)}_1 = 1 - μ^{(n)}_0\n  = \\left(μ^{(1)}_0 - \\frac{a}{a+b}\\right)(1-a-b)^n + \\frac{a}{a+b}.\n$$\n:::\n\n:::{.callout-warning collapse=\"true\"}\n#### How did we figure out the above calculation?\n\nThe above analysis appears to be a bit of black magic. To understand what is going on, note that we are interested in computing $μ^{(0)} P^n$. What is an efficient way to compute $P^n$? **Eigen decomposition!**. Since $P$ is a row stochastic matrix, we have $P \\mathbf{1} = 1.$ Thus, $λ_1 = 1$ is always an eigenvector of _any_ transition matrix with eigenvector $\\mathbf{1}$. \n\nFor @exm-on-off, we can explicitly compute all eigenvalues by finding the roots of the characteristic equation\n\\begin{align*}\n\\det(λI - P) &= \\DET{λ - 1 + a & - a \\\\ -b & λ - 1 + b} \\\\\n&= (λ-1 +a)(λ-1+b) - ab \\\\\n&= (λ-1)^2 + (a+b)(λ-1) = (λ-1)(λ-1 + a + b).\n\\end{align*}\nThus, the eigenvalues are $λ_1 = 1$ and $λ_2 = 1 - a - b$. \n\n[For the special case of $2 × 2$ transition matrices, we can find the second eigenvalue by observing that $λ_1 = 1$ is always an eigenvalue and $\\TR(P) = λ_1 + λ_2 = 1 + λ_2$ or $\\det(P) = λ_1 λ_2 = λ_2$.]{.text-secondary}\n\nTo find the eigenvector, we find a vector $v$ such that $(λI - P)v = 0$. \n\n- For $λ_1 = 1$, we already know that $v_1 = [1; 1]$ is an eigenvector. \n- For $λ_2 = (1-a-b)$, we have \n  $$ λ_2I - P = \\MATRIX{-b & -a \\\\ -b & -a}. $$\n  Therefore, one possible eigenvector is $[a; -b]$. \n\nThen, from spectral decomposition, we know that\n$$\n  P = V Λ V^{-1}\n$$\nwhere $V = [v_1 v_2] = [1, a; 1, -b]$ and $Λ = \\diag(1, 1-a-b)$. Therefore,\n\\begin{align*}\n  μ^{(n)} &= μ^{(0)} P^n = μ^{(0)} V Λ^n V^{-1} \\\\\n  &= \\MATRIX{ μ^{(0)}_0 & 1 - μ^{(0)}_0 }\n     \\MATRIX{1 & a \\\\ 1 & -b }\n     \\MATRIX{1 & 0 \\\\ 0 & 1 - a -b}\n     \\frac{1}{a+b} \n     \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &= \\MATRIX{\\dfrac{1}{a+b} & μ^{(0)}_0 - \\dfrac{b}{a+b} }\n     \\MATRIX{1 & 0 \\\\ 0 & 1 - a -b}\n     \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &=\\MATRIX{\\dfrac{1}{a+b} & 0 \\\\ 0 & \\left(μ^{(0)}_0 - \\dfrac{b}{a+b}\\right)(1-a-b)^n }\n   \\MATRIX{b & a \\\\ 1 & -1 }\n  \\\\[5pt]\n  &= \\frac{b}{a+b} + \\left(μ^{(0)}_0 - \\dfrac{b}{a+b}\\right)(1-a-b)^n \n\\end{align*}\n:::\n\n\n**TODO:** Add more examples!\n\n## Class structure\n\n\n\n1.  We say that a state $j$ is **accessible from** state $i$ (abbreviated as $i \\rightsquigarrow j$) if there is exists an $m \\in \\integers_{\\ge 0}$ (which may depend on $i$ and $j$) such that $[P^m]_{ij} > 0$. The fact that $P^{(m)}_{ij} > 0$ implies that there exists an ordered sequence of states $(i_0, \\dots, i_m)$ such that $i_0 = i$ and $i_m = j$ such that $P_{i_k i_{k+1}} > 0$; thus, there is a path of positive probability from state $i$ to state $j$.\n\n    Accessibility is an transitive relationship, i.e., if $i \\rightsquigarrow j$ and $j \\rightsquigarrow k$ implies that $i \\rightsquigarrow k$.\n\n    :::{#exm-accessible}\n    Consider the Markov chain shown below. \n\n    ![](figures/svg/markov-examples3.svg)\n\n    a. Identify all states that are accessible from state $1$.\n    b. Identify all states from which state $1$ is accessible.\n    :::\n    :::{.callout-note collapse=\"true\"}\n    #### Solution\n    a. States accessible from state $1$ are $\\{1,2,3\\}$.\n    a. States from which state $1$ is accessible are $\\{1,2,3,4,5,6\\}$.\n\n    :::\n\n2.  Two distinct states $i$ and $j$ are said to **communicate**\n    (abbreviated to $i \\leftrightsquigarrow j$) if $i$ is accessible\n    from $j$ (i.e., $j \\rightsquigarrow i$) and $j$ is accessible from\n    $i$ ($i \\rightsquigarrow j$). Alternatively, we say that $i$ and $j$\n    communicate if there exist $m, m' \\in \\integers_{\\ge 0}$ such that\n    $P^{(m)}_{ij} > 0$ and $P^{(m')}_{ji} > 0$.\n\n    For instance, in @exm-accessible, state $1$ communicates with state $2$ but does not communicate with state $5$. \n\n    Communication is an equivalence relationship, i.e., it is reflexive\n    ($i \\leftrightsquigarrow i$), symmetric ($i \\leftrightsquigarrow j$\n    if and only if $j \\leftrightsquigarrow i$), and transitive\n    ($i \\leftrightsquigarrow j$ and $j \\leftrightsquigarrow k$ implies\n    $i \\leftrightsquigarrow k$).\n\n3.  The states in a finite-state Markov chain can be partitioned into\n    two sets: **recurrent states** and **transient states**. A state is\n    recurrent if it is accessible from all states that are accessible from it (i.e., $i$ is recurrent if $i \\rightsquigarrow j$ implies that $j \\rightsquigarrow i$). States that are not recurrent are\n    **transient**.\n\n    It can be shown that a state $i$ is recurrent if and only if\n    $$\\sum_{m=1}^{\\infty} P^{(m)}_{ii} = \\infty.$$\n\n\n4.  States $i$ and $j$ are said to belong to the same **communicating\n    class** if $i$ and $j$ communicate. Communicating classes form a\n    partition the state space. Within a communicating class, all states\n    are of the same type, i.e., either all states are recurrent (in\n    which case the class is called a recurrent class) or all states are\n    transient (in which case the class is called a transient class).\n\n    For example, in @exm-accessible, there are two communication classes: $\\{1,2,3\\}$ and $\\{4,5,6\\}$. The communication class $\\{4,5,6\\}$ is transient while the communication class $\\{1,2,3\\}$ is recurrent.\n\n\n5.  A communicating class $C$ is said to be **closed** if \n    $$\n        i \\in C \\text{ and } i \\rightsquigarrow j \\implies j \\in C.\n    $$\n    Thus, there is no escape from a closed class. For finite state spaces, a recurrent class is always closed and a transient class is never closed. But this is not the case for countable state Markov chains. \n\n6.  A state $i$ is called **absorbing** if $\\{i\\}$ is a closed class, i.e., if $P_{ii} = 1$. \n\n7.  A Markov chain with a single communicating class (thus, all states\n    communicate with each other and are, therefore, recurrent) is called\n    **irreducible**.\n\n8.  The **period** of a state $i$, denoted by $d(i)$, is defined as\n    $$d(i) = \\gcd\\{ t \\in \\integers_{\\ge 1} : [P^t]_{ii} > 0 \\}.$$ If\n    the period is $1$, the state is **aperiodic**, and if the period is\n    $2$ or more, the state is **periodic**. It can be shown that all\n    states in the same class have the same period.\n\n9.  A Markov chain is aperiodic, if all states are aperiodic. A simple\n    sufficient (but not necessary) condition for an irreducible Markov\n    chain to be aperiodic is that there exists a state $i$ such that\n    $P_{ii} > 0$. In general, for a finite and aperiodic Markov chain,\n    there exists a positive integer $M$ such that \n    $$ P^{(m)}_{ii} > 0, \n            \\quad \\forall m \\ge M, i \\in \\ALPHABET X.$$\n\n\n## Hitting times\n\n1. We use the following notation:\n\n    - For any event $E$, $P_i(E)$ denotes $\\PR(E \\mid X_0 = i)$ \n    - For any random variable $Y$, $\\EXP_i[Y]$ denotes $\\EXP[Y \\mid X_0 = i]$.\n\n2. Let $A$ be a subset of $\\ALPHABET X$. The **hitting time** $H^A$ of $A$ is a random variable $H^A \\colon \\ALPHABET X \\to \\{0, 1, \\dots \\} \\cup \\{∞\\}$ given by\n   $$\n     H^A(ω) = \\min\\{n \\ge 0 : X_n(ω) \\in A\\}.\n   $$\n   The standard convention is that $H^A$ is taken to be $∞$ if $X_n \\neq A$ for any $n > 0$. For a state $j \\in \\ALPHABET X$, we use the short-hand $H^j$ to denote $H^{\\{j\\}}$. \n\n3. The probability that starting from state $i$ the Markov chain ever hits $A$ is then given by\n   $$\n      h^A_i = P_i(H^A < ∞).\n   $$\n   This is called **hitting probability**. When $A$ is a closed class, $h^A_i$ is called the **absorption probability**. \n\n4. The mean-time taken for the Markov chain to reach $A$ is given by \n   $$\n     m^A_i = \\EXP_i[H^A] = \\sum_{n=0}^{∞} n \\PR_i(H^A = n).\n   $$\n   This is called the **mean hitting time**.\n\n\nA remarkable property of Markov chain is that these quantities can be computed by solving a system of linear equations associated with the transition probability matrix $P$. \n\n:::{#thm-hitting-probabilities}\n#### Hitting probabilities\nThe hitting probabilities $\\{h^A_i\\}_{i \\in \\ALPHABET X}$ satisfies the following system of linear equations:\n$$\n  h^A_i = \\begin{cases}\n    1, & i \\in A \\\\\n    \\sum_{j \\in \\ALPHABET X} P_{ij} h^A_j, & i \\not\\in A\n  \\end{cases}\n$$\nWhen $\\ALPHABET X$ is finite, the above system has a unique solution; when $\\ALPHABET X$ is countable, the above system may have multiple solutions and the hitting probabilities correspond to the *minimal* non-negative solution.\n:::\n\n:::{.callout-note collapse=\"true\"}\n#### Proof\nWhen $X_0 = i \\in A$, the hitting time $H^A = 0$, so $h^A_i = 1$. This proves the first part of the formula.\n\nFor the second part, consider $X_0 = i \\not\\in A$. Then $H^A_i \\ge 1$. By the Markov property, we have\n$$\n\\PR_i(H^A < ∞ \\mid X_1 = j) = \\PR_j(H^A < ∞) = h^A_j.\n$$\nMoreover, by the law of total probability, we have\n\\begin{align*}\nh^A_i &= \\PR_i(H^A < ∞) = \\sum_{j \\in \\ALPHABET X} \\PR_i(H^A < ∞, X_1 = j) \\\\\n&= \\sum_{j \\in \\ALPHABET X} \\PR_i(H^A < ∞ \\mid X_1 = j) \\PR_i(X_1 = j) \\\\\n&= \\sum_{j \\in \\ALPHABET X} P_{ij} h^A_j.\n\\end{align*}\n:::\n\n:::{#exm-gambler-ruin-instance}\nConsider a gambler's ruin problem, where the gambler starts with \\$$1$ and stops either when he is ruined or when his fortune reaches \\$$K$. \n\nFind the probability of ruin (i.e., the fortune gets absorbed in state $0$ rather than state $K$). \n:::\n\n:::{.callout-note collapse=\"true\"}\n### Solution\nFor the ease of notation, we use $h_i$ as a short-form for $h^{\\{0\\}}_i$. The hitting probabilities satisy the linear system of equations:\n\n\\begin{align*}\n  h_0 &= 1 \\\\\n  h_i &= ph_{i+1} + q h_{i-1}, \\quad i \\in \\{1,\\dots,K-1\\} \\\\\n  h_K &= 0,\n\\end{align*}\nwhere $q = 1-p$. \n\nThe characteristic equation associated with the linear recurrence relationship is\n$$\n  λ = p λ^2 + q\n$$\nwhich has two distinct roots, $λ_1 = 1$ and $λ_2 = q/p$ if $p \\neq q$ and a double root at $λ_1 = 1$ if $p = q = \\frac 12$. Therefore, the general solution is of the form\n$$\n  h_i = a λ_1^i + b λ_2^i = a + b\\Bigl(\\tfrac {q}{p} \\Bigr)^i\n$$\nwhere we determine the coefficients $a$ and $b$ from the boundary conditions $h_0 = 1$ and $h_K = 0$. Solving for $a$ and $b$, we get that for $p \\neq q$, we have\n$$\n  h_i = \\frac{1 - \\Bigl(\\frac qp\\Bigr)^i}{1 - \\Bigl(\\frac qp\\Bigr)^K}\n$$\nand for $p = q = \\frac 12$, we have\n$$\n  h_i = \\frac{i}{K}.\n$$\n:::\n\n:::{#thm-mean-hitting-times}\n#### Mean hitting times\nThe mean hitting times $\\{m^A_i\\}_{i \\in \\ALPHABET X}$ satisfies the following system of linear equations:\n$$\n  m^A_i = \\begin{cases}\n    0, & i \\in A \\\\\n    1 + \\sum_{j \\not\\in A} P_{ij} m^A_j, & i \\not\\in A\n  \\end{cases}\n$$\nWhen $\\ALPHABET X$ is finite, the above system has a unique solution; when $\\ALPHABET X$ is countable, the above system may have multiple solutions and the mean hitting times correspond to the *minimal* non-negative solution.\n:::\n\n:::{.callout-note collapse=\"true\"}\n#### Proof\n\nThe proof is similar to the proof of @thm-hitting-probabilities. When $X_0 = i \\in A$, the hitting time $H^A = 0$, so $m^A_i = 0$. This proves the first part of the formula.\n\nFor the second part, consider $X_0 = i \\not\\in A$. Then $H^A_i \\ge 1$. By the Markov property, we have\n$$\n  \\EXP_i[ H^A \\mid X_1 = j] = 1 + \\EXP_j[ H^A ] = 1 + m^A_j.\n$$\nMoreover, by the law of total probability, we have\n\\begin{align*}\n  m^A_i &= \\EXP_i[ H^A ] \n  = \\sum_{j \\in \\ALPHABET X} \\EXP_i[ H^A \\mid X_1 = j ] \\PR_i(X_1 = j) \\\\\n  &= \\sum_{j \\in \\ALPHABET X} \\EXP_i[ H^A \\mid X_1 = j ] \\PR_i(X_1 = j) \\\\\n  &= \\sum_{j \\in \\ALPHABET X} P_{ij} \\bigl[ 1 + m^A_j \\bigr]\n  \\\\\n  &= 1 + \\sum_{j \\not\\in A} P_{ij} m^A_j.\n\\end{align*}\n\n:::\n\n\n## First passage time and strong Markov property\n\n3. Define the **first passage time**\n   $$\n     f^{(n)}_{ij} \\coloneqq P_i(T_j = n) = P_i(X_1 \\neq j, \\dots, X_{n-1} \\neq j, X_n = j).\n   $$\n\n4. $f^{(n)}_{ij}$ satisfies the following recursion.\n\n   - $f^{(1)}_{ij} = P_i(X_1 = j) = P_{ij}$.\n   - And for $n > 1$, $f^{(n+1)}_{ij} = \\sum_{k \\neq j} P_{ik} f^{(n)}_{kj}$.\n\n5. Let\n   $$\n     f_{ij} = \\sum_{n=1}^∞ f^{(n)}_{ij} = P_i(T_j < ∞)\n   $$\n   denotes the probability that a chain starting in $i$ eventually visits $j$. In particular $f_{jj}$ denotes the probability that a chain starting in $j$ will return to $j$. \n\n6. $f_{ij}$ satisfy the following property.  \n   $$\\displaystyle P^{(n)}_{ij} = \\sum_{m=1}^n  f^{(m)}_{ij} P^{(n-m)}_{jj}.$$\n\n      An immediate implication of the above is that if $j$ is an absorbing state then \n      $P^{(n)}_{ij} = \\sum_{m=1}^n f^{(m)}_{ij} = P_i(T_j \\le n).$\n\n\n7. Let $N^{(n)}_j = \\sum_{m=1}^{n} \\IND\\{X_m = j\\}$ denote the number of visits to state $j$ in $n$ steps. Define\n   $$\n     G_{ij}^{(n)} = \\EXP_i[ N^{(n)}_j ] = \\sum_{m=1}^n P^{(m)}_{ij}\n   $$\n   to be the expected number of visits to state $j$ in $n$ steps, starting in $i$.\n\n8. Let $N_j = \\lim_{n \\to ∞} N^{(n)}_j \\sum_{m=1}^∞ \\IND\\{X_m = j \\}$ denote the number of visits to state $j$. Similarly, define\n   $$\n      G_{ij} = \\EXP_{i}[N_j] = \\lim_{n \\to ∞} G_{ij}^{(n)}\n      = \\sum_{m=1}^{∞} P^{(m)}_{ij}\n   $$\n   to denote the expected number of visits to state $j$ for a chain starting in $i$.\n\n8. A state $j$ is **recurrent** if $f_{jj} = 1$ and **transient** if $f_{jj} < 1$. \n\n9. A state is called periodic if $f_{ii}^(n)$ is non-zero only for multiples of some smallest integer $d$, $d > 1$. \n\n9. For every transient state $j$, we have for every $i$, $P_i(N_j < ∞) = 1$ and \n   $$ G_{ij} = \\dfrac{f_{ij}}{1 - f_{jj}}.$$\n   On the other hand, if $j$ is recurrent, then $P_j(N_j = ∞) = 1$ and $G_{jj} = ∞$. Moreover,\n   $$ P_i(N_j = ∞) = P_i(T_j < ∞) = f_{ij}. $$\n   So, if $f_{ij} = 0$, then $G_{ij} = 0$ while if $f_{ij} = ∞$ then $G_{ij} = ∞$. \n\n10. Thus, a state $i$ is recurrent if and only if \n    $$G_{ii} = \\sum_{n=1}^∞ P^{(n)}_{ii} = ∞. $$\n\n11. A state $j$ is said to be **accessible from** $i$ (abbreviated as $i \\rightsquigarrow j$) if there is an ordered string of notes $(i_0, \\dots, i_m)$ such that $i_0 = i$ and $i_m = j$ and $P_{i_k i_{k+1}} > 0$. Equivalently, $i \\rightsquigarrow j$ if there exists a $m$ such that $P^{(m)}_{ij} > 0$. \n\n12. Accessibility is an transitive relationship, i.e., if $i \\rightsquigarrow j$ and $j \\rightsquigarrow k$ implies that $i \\rightsquigarrow k$. \n\n#. If $f_{ij} > 0$ but $f_{ji} < 1$, then $i$ is transient. \n\n13. If $i$ is recurrent and $i \\rightsquigarrow j$. Then, $j$ is also recurrent and $f_{ij} = f_{ji} = 1$. \n\n14. A subset $C$ of $\\ALPHABET X$ is said to be **closed** if no state inside $C$ can lead to any state outside $C$, i.e.,\n    $$\n        f_{ij} = 0, \\quad \\forall i \\in C \\text{ and } j \\not\\in C.\n    $$\n\n15. A closed set $C$ is called **irreducible** if $i \\rightsquigarrow j$ for all $i,j \\in C$. Thus, if $C$ is an irreducible set, then all states in $C$ are either recurrent or transient. \n\n16. Consequently, if $C$ is an irreducible and closed set of recurrent states. Then for all $i,j \\in C$, \n  \n      - $f_{ij} = 1$\n      - $P_i(N_j = ∞) = 1$\n      - $G_{ij} = ∞$\n\n17. If $C$ is a finite irreducible closed set of states. Then every state in $C$ is recurrent.\n\n18. Let $\\ALPHABET X_T$ and $\\ALPHABET X_R$ denote the set of transient and recurrent states. The set $\\ALPHABET X_R$ can be paritioned into a finite or countable number of irreducible closed sets $C_1$, $C_2$, $\\dots$. \n\n## Expected duration of play\n\nLet's start with a simple example. Suppose we toss a coin multiple times and\nstop at a heads. What are the expected number of tosses until stopping?\n\nFrom elementary probability we know that the number of tosses until stopping\nis a geometric random variable. However, we will model this using a Markov\nchain where the state denotes the number of consecutive heads so far. Let $p$\ndenote the probability of heads and $q = 1-p$ denote the probability of tails.\nThen, the Markov chain model is as follows.\n\n![Markov chain for coin tossing until one head](figures/svg/markov-chains1.svg)\n\n\nLet $v_i$ denote the expected number of tosses until stopping when starting at\nstate $i$. Then, we have\n\\begin{align*}\n  v_0 &= 1 + q v_0 + p v_1, \\\\\n  v_1 &= 0.\n\\end{align*}\nSolving this system of equations, we get $v_0 = 1/(1-q) = 1/p$. \n\nNow, let's try a variation of the above model. Suppose we toss a coin multiple\ntimes and stop at two heads. What are the expected number of tosses until\nstopping. \n\nWe can model this in the same manner as the before, where the state denotes\nthe number of consecutive heads so far. The Markov chain is as follows:\n\n![Markov chain for coin tossing until two heads](figures/svg/markov-chains2.svg)\n\nAs before, let $v_i$ denote the expected number of tosses until stopping when\nstarting at state $i$. Then, we have\n\\begin{align*}\n  v_0 &= 1 + q v_0 + p v_1, \\\\\n  v_1 &= 1 + q v_0 + p v_2, \\\\\n  v_2 &= 0.\n\\end{align*}\nSolving this system of equations, we get $v_0 = 1/(1-p)$. \n\nWe can generalize these ideas to find time of hitting a state.\n\n\n## Stationary distribution\n\n1. A distribution $π$ is said the be a **stationary distribution** if \n    $$\n      π = π P.\n    $$\n\n2. Stationary distributions can be computed by solving **balance equations.**\n\n3. Let $C$ be an irreducible class of a Markov chain. Then, there is a unique stationary distribution $π$ that assigns positive probability only to states in $C$. \n\n4. If $π_1$ and $π_2$ are stationary distributions of a Markov chain and $α \\in (0,1)$, then $α π_1 + (1-α) π_2$ is also a stationary distribution. \n\n5. Thus, if a Markov chain has a single irreducible class, then it has a unique stationary distribution; if it has multiple irreducible classes, then it has uncountable number of stationary distributions.\n\n## Limiting distribution\n\n1. Suppose $j$ is a transient state. Then, we know that $N_j < ∞$ and $G_{ij} < ∞$. Therefore,\n   $$\n      \\lim_{n \\to ∞}\n      \\frac{N^{(n)}_{j}}{n} = 0, a.s., \n      \\quad\\text{and}\\quad\n      \\lim_{n \\to ∞}\n      \\frac{G^{(n)}_{ij}}{n} = 0, \\forall i \\in \\ALPHABET X.\n   $$\n\n2. Suppose $j$ is a recurrent state. Then, \n   $$\n      \\lim_{n \\to ∞}\n      \\frac{N^{(n)}_{j}}{n} = μ_j, a.s., \n      \\quad\\text{and}\\quad\n      \\lim_{n \\to ∞}\n      \\frac{G^{(n)}_{ij}}{n} = μ_j, \\forall i \\in \\ALPHABET X\n   $$\n   where $μ_j = \\EXP_{j}[T_j]$ is the mean return time to state $j$. \n\n\nA Markov chain is said to have a _steady state distribution_ if $π_n$\nconverges to a limit as $n \\to ∞$ and the limit does not depend on the initial\ndistribution $π_0$. \n\nA Markov chain has a steady state distribution if it is ergodic. We can find\nthe steady state distribution by solving the _balance equation_: $π = π P$. \n\n## Exercises{-}\n:::{#exr-time-reversal}\n#### Time reversal of Markov chains\n\nLet $\\{X_n\\}_{n \\ge 1}$ is a Markov chain. Show that for any $N > n$,\n$$\n  \\PR(X_n = x_n \\mid X_{n+1:N} = x_{n+1:N})\n  = \\PR(X_n = x_n \\mid X_{n+1} = x_{n+1}).\n$$\nThus, a time reversed Markov chain is also Markov.\n:::\n\n:::{#exr-k-step-Markov}\nSuppose $\\{X_n\\}_{n \\ge 0}$ is a Markov chain with transition matrix $P$. For a fixed positive integer $k$, define $Y_n = X_{kn}$. Show that $\\{Y_n\\}_{n \\ge 0}$ is a Markov chain with transition matrix $P^k$.\n:::\n\n:::{#exr-biased-die}\nSuppose a (6-sided) die is 'fixed' so that two consecutive rolls cannot have the same outcome. In particular, if the outcome of a roll is $i$, then the next roll cannot be $i$; all $5$ other outcomes are equally likely. \n\na. Model the above as a Markov chain.\nb. If the outcome of the first roll is $1$, what is the probability that the outcome of the $n$th roll is also $1$?\n:::\n\n",
    "supporting": [
      "markov-chains_files"
    ],
    "filters": [],
    "includes": {}
  }
}